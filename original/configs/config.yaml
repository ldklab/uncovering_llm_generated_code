seed: 44
mode: offline
tags:
  - bertcls
batch_size: 16
simulate_batch_size: 32
valid_batch_size: 32
gradient_norm_clip_value: 1.0
n_epoch: 5
decay: 3e-4
lr: 1e-4
warmup: 0.1
device: -1

data:
  mini_dataset: false
  max_input_len: 512
  tokenizer_pth: "microsoft/graphcodebert-base"
  input_key: extracted_full_func
  train_frac: 1.0

model:
  plm_name: "microsoft/graphcodebert-base"
  cls_drop: 0.2
  cls_dim: 768
  save_pth: none
  # used for supervised contrastive learning
  temperature: 0.1
  trade_off: 0.1

defaults:
  - setting: apps_full