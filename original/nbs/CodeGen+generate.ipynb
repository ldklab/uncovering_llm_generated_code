{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "73023c464eb844239eacad0bd85d135c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_26d3cc5465594e06bab4a275cc63ff90",
       "IPY_MODEL_98f8706ccc7a4cf991317079f6b5d730",
       "IPY_MODEL_d1f3474010fe44df8337c4a4ab38552f"
      ],
      "layout": "IPY_MODEL_e3f143ec5e0b40c1a511727b37dfcea8"
     }
    },
    "26d3cc5465594e06bab4a275cc63ff90": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3ecac726ca2407199bf32b9e7dd58b4",
      "placeholder": "​",
      "style": "IPY_MODEL_e6ed94af126042d99b69b68f8b0f0915",
      "value": "100%"
     }
    },
    "98f8706ccc7a4cf991317079f6b5d730": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a19d3a1a00304b60bba93f554479ddfb",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac61ad0ddfe54b168b2a184de443839d",
      "value": 3
     }
    },
    "d1f3474010fe44df8337c4a4ab38552f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b242cf7485b4b2d834681ea0ff4374c",
      "placeholder": "​",
      "style": "IPY_MODEL_f27058d74777496da84b7127278db97d",
      "value": " 3/3 [00:00&lt;00:00,  9.64it/s]"
     }
    },
    "e3f143ec5e0b40c1a511727b37dfcea8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3ecac726ca2407199bf32b9e7dd58b4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6ed94af126042d99b69b68f8b0f0915": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a19d3a1a00304b60bba93f554479ddfb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac61ad0ddfe54b168b2a184de443839d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9b242cf7485b4b2d834681ea0ff4374c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f27058d74777496da84b7127278db97d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "human_dataset = []\n",
    "with open(\"dataset/csn/human_code_prompt_extracted.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        human_dataset.append(json.loads(line.strip()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cozBNa8CGyhg",
    "outputId": "1c9ae7f8-c83f-4a89-81ad-3ec3ecb0ecfe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 147,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "import json\n",
    "gen_dataset = []\n",
    "with open(\"dataset/humaneval/codegen-2B-mono-0.2-1.0-256-10.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        gen_dataset.append(json.loads(line.strip()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "1212"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_gen_dataset = []\n",
    "for data in gen_dataset:\n",
    "    unique_set = set(data['full_funcs'])\n",
    "    for func in unique_set:\n",
    "        flat_gen_dataset.append({\"func_str\":func,\"label\":1})\n",
    "len(flat_gen_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "flat_human_dataset = []\n",
    "for data in human_dataset:\n",
    "    flat_human_dataset.append({\"func_str\":data[\"human_code\"],\"label\":0})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(flat_gen_dataset)\n",
    "random.shuffle(flat_human_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "train_in_domain_gen = flat_gen_dataset[:1500]\n",
    "valid_in_domain_gen = flat_gen_dataset[1500:1750]\n",
    "test_in_domain_gen = flat_gen_dataset[1750:2000]\n",
    "\n",
    "train_in_domain_hum = flat_human_dataset[:6000]\n",
    "valid_in_domain_hum = flat_human_dataset[6000:7000]\n",
    "test_in_domain_hum = flat_human_dataset[7000:8000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "train_in_domain = train_in_domain_gen + train_in_domain_hum\n",
    "valid_in_domain = valid_in_domain_gen + valid_in_domain_hum\n",
    "test_in_domain = test_in_domain_gen + test_in_domain_hum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "2212"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out_domain = flat_gen_dataset + flat_human_dataset[7000:8000]\n",
    "len(test_out_domain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "from utils import dump_dataset_json\n",
    "dump_dataset_json(\"detection_data/train_in_domain.jsonl\",train_in_domain)\n",
    "dump_dataset_json(\"detection_data/valid_in_domain.jsonl\",valid_in_domain)\n",
    "dump_dataset_json(\"detection_data/test_in_domain.jsonl\",test_in_domain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "dump_dataset_json(\"detection_data/test_out_domain_varyP.jsonl\",test_out_domain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "import json\n",
    "conf_mat = json.load(open(\"detection_results/in_domain_conf_mat.json\",'r'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def _sort(self, short_list, sorts):\n",
      "        \"\"\"\n",
      "        TAKE SHORTLIST, RETURN IT SORTED\n",
      "        :param short_list:\n",
      "        :param sorts: LIST OF SORTS TO PERFORM\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        if len(sorts) == 0:\n",
      "            return short_list\n",
      "        else:\n",
      "            return self._sort(short_list, sorts[1:]) + self._sort(self._get_sorted_list(short_list, sorts[0]), sorts[1:])\n",
      "\n",
      "    def _get_sorted_list(self, short_list, sort_type):\n",
      "        \"\"\"\n",
      "        TAKE SHORTLIST, RETURN IT SORTED\n",
      "        :param short_list:\n",
      "        :param sort_type:\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        if sort_type == \"ascending\":\n",
      "            return sorted(short_list, key=lambda x: x[0])\n",
      "        else:\n",
      "            return sorted(short_list, key=lambda x: x[0], reverse=True)\n",
      "\n",
      "    def _get_sorted_list_by_value(self, short_list, sort_type):\n",
      "        \"\"\"\n",
      "        TAKE SHORTLIST, RETURN IT SORTED\n",
      "        :param short_list:\n",
      "        :param sort_type:\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        if sort_type == \"asc\n",
      "==========score: 1.0===============\n",
      "def __get_current_datetime(self):\n",
      "        \"\"\"Get current datetime for every file.\"\"\"\n",
      "        return datetime.now()\n",
      "\n",
      "    def __get_current_time(self):\n",
      "        \"\"\"Get current time for every file.\"\"\"\n",
      "        return datetime.now().strftime(\"%H:%M:%S\")\n",
      "\n",
      "    def __get_current_date(self):\n",
      "        \"\"\"Get current date for every file.\"\"\"\n",
      "        return datetime.now().strftime(\"%d/%m/%Y\")\n",
      "\n",
      "    def __get_current_year(self):\n",
      "        \"\"\"Get current year for every file.\"\"\"\n",
      "        return datetime.now().strftime(\"%Y\")\n",
      "\n",
      "    def __get_current_month(self):\n",
      "        \"\"\"Get current month for every file.\"\"\"\n",
      "        return datetime.now().strftime(\"%m\")\n",
      "\n",
      "    def __get_current_day(self):\n",
      "        \"\"\"Get current day for every file.\"\"\"\n",
      "        return datetime.now().strftime(\"%d\")\n",
      "\n",
      "    def __get_current_hour(self):\n",
      "        \"\"\"Get current hour for every file.\"\"\"\n",
      "        return datetime.now().strftime(\"%H\")\n",
      "\n",
      "    def __get_current_minute(self):\n",
      "        \n",
      "==========score: 1.0===============\n",
      "def distinct_permutations(iterable):\n",
      "    \"\"\"Yield successive distinct permutations of the elements in *iterable*.\n",
      "\n",
      "        >>> sorted(distinct_permutations([1, 0, 1]))\n",
      "        [(0, 1, 1), (1, 0, 1), (1, 1, 0)]\n",
      "\n",
      "    Equivalent to ``set(permutations(iterable))``, except duplicates are not\n",
      "    generated and thrown away. For larger input sequences this is much more\n",
      "    efficient.\n",
      "\n",
      "    Duplicate permutations arise when there are duplicated elements in the\n",
      "    input iterable. The number of items returned is\n",
      "    `n! / (x_1! * x_2! * ... * x_n!)`, where `n` is the total number of\n",
      "    items input, and each `x_i` is the count of a distinct item in the input\n",
      "    sequence.\n",
      "\n",
      "    \"\"\"\n",
      "    def make_new_permutations(tokens, current_index):\n",
      "        if current_index == len(tokens) - 1:\n",
      "            yield tuple(tokens)\n",
      "        else:\n",
      "            for permutation in make_new_permutations(tokens, current_index + 1):\n",
      "                for i in range(len(permutation)):\n",
      "                    yield permutation[:i] + (tokens[current_index],) + permutation[i:]\n",
      "\n",
      "    if iterable is None:\n",
      "        raise TypeError('iterable must be callable')\n",
      "\n",
      "    if callable(iterable):\n",
      "        return make_new_permutations([], 0)\n",
      "\n",
      "    try:\n",
      "        tokens = list(iterable)\n",
      "    except TypeError:\n",
      "        raise ValueError('iterable must be callable or an iterable')\n",
      "\n",
      "    distinct_permutations = make_new_permutations(tokens, 0)\n",
      "    for permutation in distinct_permutations:\n",
      "        yield permutation\n",
      "==========score: 1.0===============\n",
      "def _postgresql(self, dbhost, dbport, dbname, dbuser, dbpass, dsn_style=None):  # noqa\n",
      "        \"\"\"PostgreSQL psycopg2 driver  accepts two syntaxes\n",
      "\n",
      "        Plus a string for .pgpass file\n",
      "        \"\"\"\n",
      "        if dsn_style =='string':\n",
      "            dsn = 'host={0} port={1} dbname={2} user={3} password={4}'.format(\n",
      "                dbhost, dbport, dbname, dbuser, dbpass)\n",
      "            return psycopg2.connect(dsn)\n",
      "        elif dsn_style == 'pgpass':\n",
      "            dsn = 'host={0} port={1} dbname={2} user={3} password={4}'.format(\n",
      "                dbhost, dbport, dbname, dbuser, dbpass)\n",
      "            return psycopg2.connect(dsn)\n",
      "        else:\n",
      "            dsn = 'host={0} port={1} dbname={2} user={3} password={4}'.format(\n",
      "                dbhost, dbport, dbname, dbuser, dbpass)\n",
      "            return psycopg2.connect(dsn)\n",
      "\n",
      "    def _mysql(self, dbhost, dbport, dbname, dbuser, dbpass, dsn_style=None):  # noqa\n",
      "        \"\"\"MySQL psycopg2 driver  accepts\n",
      "==========score: 1.0===============\n",
      "def fuzzmatch(self, fuzzkey, multi=False):\n",
      "        \"\"\"\n",
      "        Identify a filter by fuzzy string matching.\n",
      "\n",
      "        Partial ('fuzzy') matching performed by `fuzzywuzzy.fuzzy.ratio`\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fuzzkey : str\n",
      "            A string that partially matches one filter name more than the others.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The name of the most closely matched filter. : str\n",
      "        \"\"\"\n",
      "        if not self.filters:\n",
      "            return None\n",
      "        if multi:\n",
      "            return self.fuzzy_match(fuzzkey, self.filters, self.multi_match)\n",
      "        else:\n",
      "            return self.fuzzy_match(fuzzkey, self.filters, self.single_match)\n",
      "\n",
      "    def fuzzy_match(self, fuzzkey, filters, match_func):\n",
      "        \"\"\"\n",
      "        Identify a filter by fuzzy string matching.\n",
      "\n",
      "        Partial ('fuzzy') matching performed by `fuzzywuzzy.fuzzy.ratio`\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fuzzkey : str\n",
      "            A string that partially matches one filter name more than the others.\n",
      "\n",
      "        filters : list\n",
      "            List of filter names.\n",
      "\n",
      "        match_func : function\n",
      "            Function that takes a string and a list of strings and returns a float.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The name of the most closely matched filter. : str\n",
      "        \"\"\"\n",
      "        if not filters:\n",
      "            return None\n",
      "        if fuzzkey not in filters:\n",
      "            return None\n",
      "        best_match = fuzzkey\n",
      "        best_rat\n",
      "==========score: 1.0===============\n",
      "def dateFromLDAPTimestamp(timestamp):\n",
      "    \"\"\" Takes an LDAP date (In the form YYYYmmdd\n",
      "        with whatever is after that) and returns a\n",
      "        datetime.date object.\n",
      "    \"\"\"\n",
      "    return datetime.date(int(timestamp[:4]), int(timestamp[4:6]), int(timestamp[6:]))\n",
      "==========score: 1.0===============\n",
      "def is_effective(self):\n",
      "        \"\"\"Tests if the current date is within the start end end dates inclusive.\n",
      "\n",
      "        return: (boolean) - ``true`` if this is effective, ``false``\n",
      "                otherwise\n",
      "        *compliance: mandatory -- This method must be implemented.*\n",
      "\n",
      "        \"\"\"\n",
      "        raise errors.Unimplemented()\n",
      "\n",
      "    @utilities.arguments_not_none\n",
      "    def get_start_date(self):\n",
      "        \"\"\"Gets the start date for this effective period.\n",
      "\n",
      "        return: (date) - the start date for this effective period\n",
      "        *compliance: mandatory -- This method must be implemented.*\n",
      "\n",
      "        \"\"\"\n",
      "        raise errors.Unimplemented()\n",
      "\n",
      "    @utilities.arguments_not_none\n",
      "    def get_end_date(self):\n",
      "        \"\"\"Gets the end date for this effective period.\n",
      "\n",
      "        return: (date) - the end date for this effective period\n",
      "        *compliance: mandatory -- This method must be implemented.*\n",
      "\n",
      "        \"\"\"\n",
      "        raise errors.Unimplemented()\n",
      "\n",
      "    @utilities.arguments_not_none\n",
      "    def get_start_date_offset(self, offset):\n",
      "        \"\"\"Gets the start date for this effective period, offset by the specified number of days.\n",
      "\n",
      "        arg:    offset (integer): the number of days to offset the start date\n",
      "        return: (date) - the start date for this effective period\n",
      "        *compliance: mandatory -- This method must be\n",
      "==========score: 1.0===============\n",
      "def groupby_count(i, key=None, force_keys=None):\n",
      "    \"\"\" Aggregate iterator values into buckets based on how frequently the\n",
      "    values appear.\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> list(groupby_count([1, 1, 1, 2, 3]))\n",
      "        [(1, 3), (2, 1), (3, 1)]\n",
      "    \"\"\"\n",
      "    if key is None:\n",
      "        key = lambda x: x\n",
      "    if force_keys is None:\n",
      "        force_keys = []\n",
      "    if not force_keys:\n",
      "        force_keys = [key(x) for x in i]\n",
      "    else:\n",
      "        force_keys = [key(x) for x in force_keys]\n",
      "    counts = defaultdict(int)\n",
      "    for x in i:\n",
      "        k = key(x)\n",
      "        if k in force_keys:\n",
      "            counts[k] += 1\n",
      "        else:\n",
      "            for fk in force_keys:\n",
      "                if fk in counts:\n",
      "                    counts[fk] += 1\n",
      "                else:\n",
      "                    counts[fk] = 1\n",
      "    return counts.items()\n",
      "==========score: 1.0===============\n",
      "def get_binary_path(executable, logging_level='INFO'):\n",
      "    \"\"\"Gets the software name and returns the path of the binary.\"\"\"\n",
      "    logging.basicConfig(level=logging_level)\n",
      "    logger = logging.getLogger(__name__)\n",
      "    logger.info('Getting binary path for %s', executable)\n",
      "    if executable == 'ffmpeg':\n",
      "        binary_path = 'ffmpeg'\n",
      "    elif executable == 'ffprobe':\n",
      "        binary_path = 'ffprobe'\n",
      "    elif executable == 'ffplay':\n",
      "        binary_path = 'ffplay'\n",
      "    elif executable == 'ffprobe':\n",
      "        binary_path = 'ffprobe'\n",
      "    elif executable == 'ffprobe':\n",
      "        binary_path = 'ffprobe'\n",
      "    elif executable == 'ffprobe':\n",
      "        binary_path = 'ffprobe'\n",
      "    elif executable == 'ffprobe':\n",
      "        binary_path = 'ffprobe'\n",
      "    elif executable == 'ffprobe':\n",
      "        binary_path = 'ffprobe'\n",
      "    elif executable == 'ffprobe':\n",
      "        binary_path = 'ffprobe'\n",
      "    elif executable == 'ffprobe':\n",
      "        binary_path =\n",
      "==========score: 1.0===============\n",
      "def normal_distribution(self, pos, sample):\n",
      "        \"\"\"returns the value of normal distribution, given the weight's sample and target position\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        pos: int\n",
      "            the epoch number of the position you want to predict\n",
      "        sample: list\n",
      "            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            the value of normal distribution\n",
      "        \"\"\"\n",
      "        return np.exp(-(sample - self.target_pos[pos]) ** 2 / (2 * self.sigma ** 2)) / (\n",
      "                    np.sqrt(2 * np.pi) * self.sigma)\n",
      "\n",
      "    def get_weight(self, pos):\n",
      "        \"\"\"returns the weight of the position\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        pos: int\n",
      "            the epoch number of the position you want to predict\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list\n",
      "            the weight of the position\n",
      "        \"\"\"\n",
      "        return self.weight[pos]\n",
      "\n",
      "    def get_target_pos(self):\n",
      "        \"\"\"returns the target position\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list\n",
      "            the target position\n",
      "        \"\"\"\n",
      "        return self.target_pos\n",
      "\n",
      "    def get_sigma(self):\n",
      "        \"\"\"returns the sigma\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            the sigma\n",
      "        \"\"\"\n",
      "        return self.sigma\n",
      "\n",
      "    def get_weight_list(self):\n",
      "        \"\"\"returns the weight list\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list\n",
      "            the weight list\n",
      "        \"\"\"\n",
      "        return self.weight\n",
      "\n",
      "    def get\n",
      "==========score: 1.0===============\n",
      "def generate_html():\n",
      "    \"\"\"\n",
      "    Generate an HTML file incorporating the images produced by this script\n",
      "    \"\"\"\n",
      "    # Open the file\n",
      "    with open(OUTPUT_FILE, 'r') as f:\n",
      "        html = f.read()\n",
      "\n",
      "    # Replace the image tag with the image\n",
      "    html = html.replace('<img src=\"', '<img src=\"' + IMAGE_PATH)\n",
      "\n",
      "    # Write the file\n",
      "    with open(OUTPUT_FILE, 'w') as f:\n",
      "        f.write(html)\n",
      "==========score: 1.0===============\n",
      "def from_text_file(file_path):\n",
      "        \"\"\"Load MonsoonData objects from a text file generated by\n",
      "        MonsoonData.save_to_text_file.\n",
      "\n",
      "        Args:\n",
      "            file_path: The full path of the file load from, including the file\n",
      "                name.\n",
      "\n",
      "        Returns:\n",
      "            A list of MonsoonData objects.\n",
      "        \"\"\"\n",
      "        with open(file_path, 'r') as f:\n",
      "            lines = f.readlines()\n",
      "        return [\n",
      "            MonsoonData.from_text_line(line) for line in lines\n",
      "        ]\n",
      "\n",
      "    @staticmethod\n",
      "    def from_text_line(line):\n",
      "        \"\"\"Load MonsoonData object from a line of text.\n",
      "\n",
      "        Args:\n",
      "            line: A string containing the line of text.\n",
      "\n",
      "        Returns:\n",
      "            A MonsoonData object.\n",
      "        \"\"\"\n",
      "        fields = line.split(',')\n",
      "        if len(fields)!= 6:\n",
      "            raise ValueError(\n",
      "                'Expected 6 fields in line, got {}'.format(len(fields)))\n",
      "        return MonsoonData(\n",
      "            fields[0], fields[1], fields[2], fields[3], fields[4], fields[5])\n",
      "\n",
      "    def to_text_line(self):\n",
      "        \"\"\"Convert MonsoonData object to a line of text.\n",
      "\n",
      "        Returns:\n",
      "            A string containing the line of text.\n",
      "        \"\"\"\n",
      "        return ','.join([\n",
      "            self.timestamp, self.voltage, self.current, self\n",
      "==========score: 1.0===============\n",
      "def timer(fn, miniter=3, minwall=3.0):\n",
      "    \"\"\"Runs fn() multiple times and returns the results.\n",
      "\n",
      "    Runs for at least ``miniter`` iterations and ``minwall`` wall time.\n",
      "    \"\"\"\n",
      "    start = time.time()\n",
      "    for i in range(miniter):\n",
      "        fn()\n",
      "    end = time.time()\n",
      "    wall = end - start\n",
      "    if wall < minwall:\n",
      "        raise RuntimeError('Not enough time to run %s' % fn)\n",
      "    return wall\n",
      "==========score: 1.0===============\n",
      "def to_file(self, outputfile=DEFAULT_OUTPUTFILE):\n",
      "        \"\"\"Write the report to a file.\n",
      "        \n",
      "        By default a name is generated.\n",
      "\n",
      "        Parameters:\n",
      "        ----------\n",
      "        outputfile : str\n",
      "            The name or the path of the file to generale including the extension (.html).\n",
      "        \"\"\"\n",
      "        if not outputfile:\n",
      "            outputfile = self.get_outputfile()\n",
      "        with open(outputfile, 'w') as f:\n",
      "            f.write(self.to_html())\n",
      "\n",
      "    def to_html(self):\n",
      "        \"\"\"Return the report as a HTML string.\"\"\"\n",
      "        return self.html\n",
      "\n",
      "    def to_file(self, outputfile=DEFAULT_OUTPUTFILE):\n",
      "        \"\"\"Write the report to a file.\n",
      "        \n",
      "        By default a name is generated.\n",
      "\n",
      "        Parameters:\n",
      "        ----------\n",
      "        outputfile : str\n",
      "            The name or the path of the file to generale including the extension (.html).\n",
      "        \"\"\"\n",
      "        if not outputfile:\n",
      "            outputfile = self.get_outputfile()\n",
      "        with open(outputfile, 'w') as f:\n",
      "            f.write(self.to_html())\n",
      "\n",
      "    def get_outputfile(self):\n",
      "        \"\"\"Return the name of the output file.\"\"\"\n",
      "        return self.outputfile\n",
      "\n",
      "    def get_title(self):\n",
      "        \"\"\"Return the title of the report.\"\"\"\n",
      "        return self.title\n",
      "\n",
      "    def get_description(self\n",
      "==========score: 1.0===============\n",
      "def replace_entities(self, html):\n",
      "        \"\"\"\n",
      "        Replace htmlentities with unicode characters\n",
      "        @Params\n",
      "        html - html source to replace entities in\n",
      "        @Returns\n",
      "        String html with entities replaced\n",
      "        \"\"\"\n",
      "        return html.replace(u\"&#39;\", \"'\")\n",
      "\n",
      "    def replace_urls(self, html):\n",
      "        \"\"\"\n",
      "        Replace urls with unicode characters\n",
      "        @Params\n",
      "        html - html source to replace urls in\n",
      "        @Returns\n",
      "        String html with urls replaced\n",
      "        \"\"\"\n",
      "        return html.replace(u\"http://\", u\"http://www.\")\n",
      "\n",
      "    def replace_emails(self, html):\n",
      "        \"\"\"\n",
      "        Replace emails with unicode characters\n",
      "        @Params\n",
      "        html - html source to replace emails in\n",
      "        @Returns\n",
      "        String html with emails replaced\n",
      "        \"\"\"\n",
      "        return html.replace(u\"mailto:\", u\"mailto@\")\n",
      "\n",
      "    def replace_phone_numbers(self, html):\n",
      "        \"\"\"\n",
      "        Replace phone numbers with unicode characters\n",
      "        @Params\n",
      "        html - html source to replace phone numbers in\n",
      "        @Returns\n",
      "        String html with phone numbers replaced\n",
      "        \"\"\"\n",
      "        return html.replace(u\"tel:\", u\"tel:+\")\n",
      "\n",
      "    def replace_numbers(self, html):\n",
      "        \"\"\"\n",
      "        \n",
      "==========score: 1.0===============\n",
      "def run(args):\n",
      "    \"\"\"\n",
      "    Args:\n",
      "        args (argparse.Namespace)\n",
      "    \"\"\"\n",
      "    if args.verbose:\n",
      "        logging.basicConfig(level=logging.DEBUG)\n",
      "    else:\n",
      "        logging.basicConfig(level=logging.INFO)\n",
      "\n",
      "    if args.mode == 'train':\n",
      "        train(args)\n",
      "    elif args.mode == 'test':\n",
      "        test(args)\n",
      "    else:\n",
      "        raise ValueError('Unknown mode: {}'.format(args.mode))\n",
      "==========score: 1.0===============\n",
      "def fit_model(regressor_type,\n",
      "              regressor_kwargs,\n",
      "              tf_matrix,\n",
      "              target_gene_expression,\n",
      "              early_stop_window_length=EARLY_STOP_WINDOW_LENGTH,\n",
      "              seed=DEMON_SEED):\n",
      "    \"\"\"\n",
      "    :param regressor_type: string. Case insensitive.\n",
      "    :param regressor_kwargs: a dictionary of key-value pairs that configures the regressor.\n",
      "    :param tf_matrix: the predictor matrix (transcription factor matrix) as a numpy array.\n",
      "    :param target_gene_expression: the target (y) gene expression to predict in function of the tf_matrix (X).\n",
      "    :param early_stop_window_length: window length of the early stopping monitor.\n",
      "    :param seed: (optional) random seed for the regressors.\n",
      "    :return: a trained regression model.\n",
      "    \"\"\"\n",
      "    # TODO: implement this function.\n",
      "    # 1. instantiate the regressor.\n",
      "    # 2. fit the regressor to the data.\n",
      "    # 3. return the trained regressor.\n",
      "    # 4. return the trained regressor.\n",
      "    # 5. return the trained regressor.\n",
      "    # 6. return the trained regressor.\n",
      "    # 7. return the trained regressor.\n",
      "    # 8. return the trained regressor.\n",
      "    # 9. return the trained regressor.\n",
      "    # 10. return the trained regressor.\n",
      "    # 11. return the trained regressor.\n",
      "    # 12. return the trained regressor.\n",
      "    # 13. return the trained regressor.\n",
      "    # 14. return the trained regressor.\n",
      "    # 15. return the trained regressor.\n",
      "    # 16. return the trained regressor.\n",
      "    # 17. return the trained regressor.\n",
      "    # 18. return the trained regressor.\n",
      "    # 19. return the trained regressor.\n",
      "    # 20. return the trained regressor.\n",
      "    # 21. return the trained regressor.\n",
      "    # 22. return the trained regressor.\n",
      "    \n",
      "==========score: 1.0===============\n",
      "def aes_encrypt(base64_encryption_key, data):\n",
      "    \"\"\"Encrypt data with AES-CBC and sign it with HMAC-SHA256\n",
      "\n",
      "    Arguments:\n",
      "        base64_encryption_key (str): a base64-encoded string containing an AES encryption key\n",
      "            and HMAC signing key as generated by generate_encryption_key()\n",
      "        data (str): a byte string containing the data to be encrypted\n",
      "\n",
      "    Returns:\n",
      "        str: the encrypted data as a byte string with the HMAC signature appended to the end\n",
      "\n",
      "    \"\"\"\n",
      "    # Decode the base64-encoded encryption key and HMAC signing key\n",
      "    encryption_key, signing_key = base64_encryption_key.split(':')\n",
      "    encryption_key = base64.b64decode(encryption_key)\n",
      "    signing_key = base64.b64decode(signing_key)\n",
      "\n",
      "    # Encrypt the data\n",
      "    iv = os.urandom(16)\n",
      "    cipher = AES.new(encryption_key, AES.MODE_CBC, iv)\n",
      "    encrypted_data = iv + cipher.encrypt(data)\n",
      "\n",
      "    # Sign the data\n",
      "    hmac = HMAC.new(signing_key, msg=encrypted_data, digestmod=SHA256)\n",
      "    signature = hmac.digest()\n",
      "\n",
      "    # Append the signature to the encrypted data\n",
      "    return encrypted_data + signature\n",
      "==========score: 1.0===============\n",
      "def _readline(self, ignore_comments=True):\n",
      "        \"\"\"The next line of the DETX file, optionally ignores comments\"\"\"\n",
      "        line = self._file.readline()\n",
      "        if ignore_comments:\n",
      "            while line.startswith('#'):\n",
      "                line = self._file.readline()\n",
      "        return line\n",
      "\n",
      "    def _read_next_line(self):\n",
      "        \"\"\"Reads the next line of the file, and returns it as a string\"\"\"\n",
      "        line = self._readline()\n",
      "        if not line:\n",
      "            return None\n",
      "        return line.strip()\n",
      "\n",
      "    def _read_next_line_as_int(self):\n",
      "        \"\"\"Reads the next line of the file, and returns it as an int\"\"\"\n",
      "        line = self._read_next_line()\n",
      "        if not line:\n",
      "            return None\n",
      "        return int(line)\n",
      "\n",
      "    def _read_next_line_as_float(self):\n",
      "        \"\"\"Reads the next line of the file, and returns it as a float\"\"\"\n",
      "        line = self._read_next_line()\n",
      "        if not line:\n",
      "            return None\n",
      "        return float(line)\n",
      "\n",
      "    def _read_next_line_as_list(self):\n",
      "        \"\"\"Reads the next line of the\n",
      "==========score: 1.0===============\n",
      "def fuzzmatch(self, fuzzkey, multi=False):\n",
      "        \"\"\"\n",
      "        Identify a filter by fuzzy string matching.\n",
      "\n",
      "        Partial ('fuzzy') matching performed by `fuzzywuzzy.fuzzy.ratio`\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fuzzkey : str\n",
      "            A string that partially matches one filter name more than the others.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The name of the most closely matched filter. : str\n",
      "        \"\"\"\n",
      "        if not self.filters:\n",
      "            return None\n",
      "        if multi:\n",
      "            return self.fuzzy_match(fuzzkey, self.filters, self.filter_names)\n",
      "        else:\n",
      "            return self.fuzzy_match(fuzzkey, self.filters, self.filter_names)[0]\n",
      "\n",
      "    def fuzzy_match(self, fuzzkey, filters, filter_names):\n",
      "        \"\"\"\n",
      "        Identify a filter by fuzzy string matching.\n",
      "\n",
      "        Partial ('fuzzy') matching performed by `fuzzywuzzy.fuzzy.ratio`\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fuzzkey : str\n",
      "            A string that partially matches one filter name more than the others.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The name of the most closely matched filter. : str\n",
      "        \"\"\"\n",
      "        fuzzy_ratios = []\n",
      "        for filter_name in filter_names:\n",
      "            fuzzy_ratios.append(fuzzywuzzy.fuzzy.ratio(fuzzkey, filter_name))\n",
      "        return filter_names[fuzzy_ratios.index(max\n",
      "==========score: 1.0===============\n",
      "def inttoDER(a):\n",
      "    '''\n",
      "    Format an int/long to DER hex format\n",
      "    '''\n",
      "    return hex(a).replace('0x','').zfill(8)\n",
      "\n",
      "def DERtoInt(a):\n",
      "    '''\n",
      "    Format a DER hex format to int\n",
      "    '''\n",
      "    return int(a,16)\n",
      "\n",
      "\n",
      "==========score: 1.0===============\n",
      "def read(self, size):\n",
      "\t\t\"\"\"\n",
      "\t\tRead raw data from the serial connection. This function is not\n",
      "\t\tmeant to be called directly.\n",
      "\n",
      "\t\t:param int size: The number of bytes to read from the serial connection.\n",
      "\t\t\"\"\"\n",
      "\t\treturn self.serial.read(size)\n",
      "\n",
      "\tdef write(self, data):\n",
      "\t\t\"\"\"\n",
      "\t\tWrite raw data to the serial connection. This function is not meant to be called directly.\n",
      "\n",
      "\t\t:param bytes data: The data to write to the serial connection.\n",
      "\t\t\"\"\"\n",
      "\t\tself.serial.write(data)\n",
      "\n",
      "\tdef flush(self):\n",
      "\t\t\"\"\"\n",
      "\t\tFlush the serial connection. This function is not meant to be called directly.\n",
      "\t\t\"\"\"\n",
      "\t\tself.serial.flush()\n",
      "\n",
      "\tdef close(self):\n",
      "\t\t\"\"\"\n",
      "\t\tClose the serial connection. This function is not meant to be called directly.\n",
      "\t\t\"\"\"\n",
      "\t\tself.serial.close()\n",
      "\n",
      "\tdef set_baudrate(self, baudrate):\n",
      "\t\t\"\"\"\n",
      "\t\tSet the baudrate of the serial connection. This function is not meant to be called directly.\n",
      "\n",
      "\t\t:param int baudrate: The baudrate to set the serial connection to.\n",
      "\t\t\"\"\"\n",
      "\t\tself.serial.baudrate = baudrate\n",
      "\n",
      "\tdef set_bytesize(self, bytesize):\n",
      "\t\t\"\"\"\n",
      "\t\tSet the bytesize of the serial connection. This function is not\n",
      "==========score: 1.0===============\n",
      "def binomial_prefactor(s,ia,ib,xpa,xpb):\n",
      "    \"\"\"\n",
      "    The integral prefactor containing the binomial coefficients from Augspurger and Dykstra.\n",
      "    >>> binomial_prefactor(0,0,0,0,0)\n",
      "    1\n",
      "    \"\"\"\n",
      "    if ia == ib:\n",
      "        return 1\n",
      "    elif ia == 0:\n",
      "        return xpa\n",
      "    elif ib == 0:\n",
      "        return xpb\n",
      "    else:\n",
      "        return binomial_coefficient(ia,ib)*(xpa*xpb)**(ia+ib-2)\n",
      "\n",
      "def binomial_coefficient(ia,ib):\n",
      "    \"\"\"\n",
      "    The binomial coefficient.\n",
      "    >>> binomial_coefficient(0,0)\n",
      "    1\n",
      "    \"\"\"\n",
      "    if ia == ib:\n",
      "        return 1\n",
      "    elif ia == 0:\n",
      "        return 0\n",
      "    elif ib == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return binomial_coefficient(ia-1,ib-1) + binomial_coefficient(ia-1,ib)\n",
      "\n",
      "\n",
      "==========score: 1.0===============\n",
      "def encode(self):\n",
      "        \"\"\"\n",
      "        Encodes the current state of the object into a string.\n",
      "\n",
      "        :return: The encoded string\n",
      "        \"\"\"\n",
      "        return json.dumps(self.__dict__)\n",
      "\n",
      "    def __str__(self):\n",
      "        \"\"\"\n",
      "        Returns a string representation of the object.\n",
      "\n",
      "        :return: A string representation of the object\n",
      "        \"\"\"\n",
      "        return self.__repr__()\n",
      "\n",
      "    def __repr__(self):\n",
      "        \"\"\"\n",
      "        Returns a string representation of the object.\n",
      "\n",
      "        :return: A string representation of the object\n",
      "        \"\"\"\n",
      "        return self.__class__.__name__ + \"(\" + self.encode() + \")\"\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        \"\"\"\n",
      "        Compares the current object with another object for equality.\n",
      "\n",
      "        :param other: The object to compare with\n",
      "        :return: True if the objects are equal, False otherwise\n",
      "        \"\"\"\n",
      "        if not isinstance(other, self.__class__):\n",
      "            return False\n",
      "        return self.__dict__ == other.__dict__\n",
      "\n",
      "==========score: 1.0===============\n",
      "def _file_size(file_path, uncompressed=False):\n",
      "    \"\"\"Return size of a single file, compressed or uncompressed\"\"\"\n",
      "    if uncompressed:\n",
      "        return os.stat(file_path).st_size\n",
      "    else:\n",
      "        return os.path.getsize(file_path)\n",
      "==========score: 1.0===============\n",
      "def set_checkbox(self, data, uncheck_other_boxes=True):\n",
      "        \"\"\"Set the *checked*-attribute of input elements of type \"checkbox\"\n",
      "        specified by ``data`` (i.e. check boxes).\n",
      "\n",
      "        :param data: Dict of ``{name: value, ...}``.\n",
      "            In the family of checkboxes whose *name*-attribute is ``name``,\n",
      "            check the box whose *value*-attribute is ``value``. All boxes in\n",
      "            the family can be checked (unchecked) if ``value`` is True (False).\n",
      "            To check multiple specific boxes, let ``value`` be a tuple or list.\n",
      "        :param uncheck_other_boxes: If True (default), before checking any\n",
      "            boxes specified by ``data``, uncheck the entire checkbox family.\n",
      "            Consider setting to False if some boxes are checked by default when\n",
      "            the HTML is served.\n",
      "        \"\"\"\n",
      "        for name, value in data.items():\n",
      "            if isinstance(value, (tuple, list)):\n",
      "                for v in value:\n",
      "                    self.set_checkbox(name, v, uncheck_other_boxes)\n",
      "            else:\n",
      "                self.set_checkbox(name, value, uncheck_other_boxes)\n",
      "\n",
      "    def set_checkbox(self, name, value, uncheck_other_boxes=True):\n",
      "        \"\"\"Set the *checked*-attribute of input elements of type \"checkbox\"\n",
      "        specified by ``name`` and ``value``.\n",
      "\n",
      "        :param name: Name of the checkbox whose *value*-attribute is ``value``.\n",
      "        :param value: True (default) to check the checkbox, False to uncheck.\n",
      "        :param uncheck_other_boxes: If True (default), before checking any\n",
      "            boxes specified by ``data``, uncheck the entire checkbox family.\n",
      "            Consider setting to False if some boxes are checked by default when\n",
      "            the HTML is served.\n",
      "        \"\"\"\n",
      "        if name not in self.inputs:\n",
      "            raise KeyError('No checkbox with name\n",
      "==========score: 1.0===============\n",
      "def initialize(g, app):\n",
      "    \"\"\"\n",
      "    If postgresql url is defined in configuration params a\n",
      "    scoped session will be created\n",
      "    \"\"\"\n",
      "    if app.config.get('POSTGRESQL_URL'):\n",
      "        g.db = scoped_session(\n",
      "            sessionmaker(\n",
      "                autocommit=False,\n",
      "                autoflush=False,\n",
      "                bind=engine,\n",
      "                binds={\n",
      "                    'database': engine,\n",
      "                }\n",
      "            )\n",
      "        )\n",
      "    else:\n",
      "        g.db = scoped_session(sessionmaker(autocommit=False, autoflush=False))\n",
      "==========score: 1.0===============\n",
      "def replace_entities(self, html):\n",
      "        \"\"\"\n",
      "        Replace htmlentities with unicode characters\n",
      "        @Params\n",
      "        html - html source to replace entities in\n",
      "        @Returns\n",
      "        String html with entities replaced\n",
      "        \"\"\"\n",
      "        return html.replace(u\"&#8211;\", \"-\") \\\n",
      "           .replace(u\"&#8217;\", \"'\") \\\n",
      "           .replace(u\"&#8220;\", \"\\\"\") \\\n",
      "           .replace(u\"&#8221;\", \"\\\"\") \\\n",
      "           .replace(u\"&#8212;\", \"--\") \\\n",
      "           .replace(u\"&#8230;\", \"...\") \\\n",
      "           .replace(u\"&#8482;\", \"--\") \\\n",
      "           .replace(u\"&#8230;\", \"...\") \\\n",
      "           .replace(u\"&#8216;\", \"'\") \\\n",
      "           .replace(u\"&#8217;\", \"'\") \\\n",
      "           .replace(u\"&#8220;\", \"\\\"\") \\\n",
      "           .replace(u\"&#8221;\", \"\\\"\") \\\n",
      "           .replace(u\"&#8226;\", \"...\") \\\n",
      "           .replace(u\"&#8222;\", \"...\") \\\n",
      "           .replace(u\"&#8226;\", \"...\") \\\n",
      "           .\n",
      "==========score: 0.9999===============\n",
      "def error_code_to_str(code):\n",
      "    \"\"\"\n",
      "    Converts a given error code (errno) to a useful and human readable string.\n",
      "\n",
      "    :param int code: a possibly invalid/unknown error code\n",
      "    :rtype: str\n",
      "    :returns: a string explaining and containing the given error code, or a string\n",
      "              explaining that the errorcode is unknown if that is the case\n",
      "    \"\"\"\n",
      "    if code == errno.EINVAL:\n",
      "        return \"Invalid argument\"\n",
      "    elif code == errno.ENOSYS:\n",
      "        return \"No such file or directory\"\n",
      "    elif code == errno.EACCES:\n",
      "        return \"Permission denied\"\n",
      "    elif code == errno.ENOENT:\n",
      "        return \"No such file or directory\"\n",
      "    elif code == errno.EEXIST:\n",
      "        return \"File already exists\"\n",
      "    elif code == errno.EISDIR:\n",
      "        return \"Is a directory\"\n",
      "    elif code == errno.EINTR:\n",
      "        return \"Interrupted system call\"\n",
      "    elif code == errno.ENOTDIR:\n",
      "        return \"Not a directory\"\n",
      "    elif code == errno.EOPNOTSUPP:\n",
      "        return \"Operation not supported\"\n",
      "    elif code == errno.EACCES:\n",
      "        return \"Permission denied\"\n",
      "    elif code == errno.EFAULT:\n",
      "        return \"Bad address\"\n",
      "    elif code == errno.ENOMEM:\n",
      "        return \"Out of memory\"\n",
      "\n",
      "==========score: 1.0===============\n",
      "def is_effective(self):\n",
      "        \"\"\"Tests if the current date is within the start end end dates inclusive.\n",
      "\n",
      "        return: (boolean) - ``true`` if this is effective, ``false``\n",
      "                otherwise\n",
      "        *compliance: mandatory -- This method must be implemented.*\n",
      "\n",
      "        \"\"\"\n",
      "        raise errors.Unimplemented()\n",
      "\n",
      "    @utilities.arguments_not_none\n",
      "    def get_start_date(self):\n",
      "        \"\"\"Gets the start date of this effective period.\n",
      "\n",
      "        return: (date) - the start date of this effective period\n",
      "        *compliance: mandatory -- This method must be implemented.*\n",
      "\n",
      "        \"\"\"\n",
      "        raise errors.Unimplemented()\n",
      "\n",
      "    @utilities.arguments_not_none\n",
      "    def get_end_date(self):\n",
      "        \"\"\"Gets the end date of this effective period.\n",
      "\n",
      "        return: (date) - the end date of this effective period\n",
      "        *compliance: mandatory -- This method must be implemented.*\n",
      "\n",
      "        \"\"\"\n",
      "        raise errors.Unimplemented()\n",
      "\n",
      "    @utilities.arguments_not_none\n",
      "    def get_start_date_offset(self, offset):\n",
      "        \"\"\"Gets the start date of this effective period, offset by the specified number of days.\n",
      "\n",
      "        arg:    offset (integer): number of days to offset the start date\n",
      "        return: (date) - the start date of this effective period\n",
      "        *compliance: mandatory -- This method must be implemented\n",
      "==========score: 1.0===============\n",
      "def mxmg(m1, m2, nrow1, ncol1, ncol2):\n",
      "    \"\"\"\n",
      "    Multiply two double precision matrices of arbitrary size.\n",
      "\n",
      "    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/mxmg_c.html\n",
      "\n",
      "    :param m1: nrow1 X ncol1 double precision matrix.\n",
      "    :type m1: NxM-Element Array of floats\n",
      "    :param m2: ncol1 X ncol2 double precision matrix.\n",
      "    :type m2: NxM-Element Array of floats\n",
      "    :param nrow1: Row dimension of m1\n",
      "    :type nrow1: int\n",
      "    :param ncol1: Column dimension of m1 and row dimension of m2.\n",
      "    :type ncol1: int\n",
      "    :param ncol2: Column dimension of m2\n",
      "    :type ncol2: int\n",
      "    :return: nrow1 X ncol2 double precision matrix.\n",
      "    :rtype: NxM-Element Array of floats\n",
      "    \"\"\"\n",
      "    m1 = ctypes.c_double * ncol1 * nrow1(m1)\n",
      "    m2 = ctypes.c_double * ncol2 * nrow1(m2)\n",
      "    m3 = ctypes.c_double * nrow1 * ncol2\n",
      "    libspice.mxmg_c(m1, m2, m3, nrow1, ncol1, ncol2)\n",
      "    return m3\n",
      "==========score: 1.0===============\n",
      "def median(self, **kwargs):\n",
      "        \"\"\"Returns median of each column or row.\n",
      "\n",
      "        Returns:\n",
      "            A new QueryCompiler object containing the median of each column or row.\n",
      "        \"\"\"\n",
      "        return self.__class__(self.__class__.median(self, **kwargs))\n",
      "\n",
      "    def mode(self, **kwargs):\n",
      "        \"\"\"Returns mode of each column or row.\n",
      "\n",
      "        Returns:\n",
      "            A new QueryCompiler object containing the mode of each column or row.\n",
      "        \"\"\"\n",
      "        return self.__class__(self.__class__.mode(self, **kwargs))\n",
      "\n",
      "    def quantile(self, **kwargs):\n",
      "        \"\"\"Returns quantile of each column or row.\n",
      "\n",
      "        Returns:\n",
      "            A new QueryCompiler object containing the quantile of each column or row.\n",
      "        \"\"\"\n",
      "        return self.__class__(self.__class__.quantile(self, **kwargs))\n",
      "\n",
      "    def unique(self, **kwargs):\n",
      "        \"\"\"Returns unique values of each column or row.\n",
      "\n",
      "        Returns:\n",
      "            A new QueryCompiler object containing the unique values of each column or row.\n",
      "        \"\"\"\n",
      "        return self.__class__(self.__class__.unique(self, **kwargs))\n",
      "\n",
      "    def __getitem__(self, item):\n",
      "        \"\"\"Returns a new Query\n",
      "==========score: 1.0===============\n",
      "def deserialize(self):\n",
      "        \"\"\" Invoke the RFC 7159 spec compliant parser\n",
      "\n",
      "        :return:\n",
      "            the parsed & vetted request body\n",
      "        \"\"\"\n",
      "        try:\n",
      "            return self.parser.parse(self.body)\n",
      "        except Exception as e:\n",
      "            raise MalformedRequestBody(e)\n",
      "\n",
      "    def serialize(self):\n",
      "        \"\"\" Serialize the request to a string representation.\n",
      "\n",
      "        :returns: the string representation of the request.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            (code, headers, body) = self._serialize()\n",
      "            return urllib3.HTTPResponse(code, headers, body).to_string()\n",
      "        except Exception as e:\n",
      "            raise SerializationError(e)\n",
      "\n",
      "    def _serialize(self):\n",
      "        \"\"\"Serializes the request.\n",
      "\n",
      "        :returns: A 3-tuple of (code, headers, body)\n",
      "        \"\"\"\n",
      "\n",
      "        uri_parts = self.uri.split('?')\n",
      "        uri_parts[0] = self.method +'' + uri_parts[0]\n",
      "        uri = uri_parts[0]\n",
      "\n",
      "        if len(uri_parts) > 1:\n",
      "            query_string = uri_parts[1]\n",
      "        else:\n",
      "            query_string = ''\n",
      "\n",
      "        # TOD\n",
      "==========score: 1.0===============\n",
      "def zeros(stype, shape, ctx=None, dtype=None, **kwargs):\n",
      "    \"\"\"Return a new array of given shape and type, filled with zeros.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    stype: string\n",
      "        The storage type of the empty array, such as 'row_sparse', 'csr', etc\n",
      "    shape : int or tuple of int\n",
      "        The shape of the empty array\n",
      "    ctx : Context, optional\n",
      "        An optional device context (default is the current default context)\n",
      "    dtype : str or numpy.dtype, optional\n",
      "        An optional value type (default is `float32`)\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    RowSparseNDArray or CSRNDArray\n",
      "        A created array\n",
      "    Examples\n",
      "    --------\n",
      "    >>> mx.nd.sparse.zeros('csr', (1,2))\n",
      "    <CSRNDArray 1x2 @cpu(0)>\n",
      "    >>> mx.nd.sparse.zeros('row_sparse', (1,2), ctx=mx.cpu(), dtype='float16').asnumpy()\n",
      "    array([[ 0.,  0.]], dtype=float16)\n",
      "    \"\"\"\n",
      "    # pylint: disable= unused-argument\n",
      "    if stype == 'default':\n",
      "        return _zeros_ndarray(shape, ctx=ctx, dtype=dtype)\n",
      "    if ctx is None:\n",
      "        ctx = current_context()\n",
      "    dtype = mx_real_t if dtype is None else dtype\n",
      "    # pylint: disable= no-member, protected-access\n",
      "    if stype == 'row_sparse' or stype == 'csr':\n",
      "        return _ndarray_cls(_new_alloc_handle('row_sparse', shape, ctx, False, dtype,\n",
      "                                              kvstore=kvstore,\n",
      "                                              readonly=False, data_init=False))\n",
      "    raise Exception(\"unknown stype : \" + str(stype))\n",
      "    # pylint: enable= no-member, protected-access\n",
      "==========score: 1.0===============\n",
      "def copy_file(src, dst, ignore=None):\n",
      "   \"\"\" this function will simply copy the file from the source path to the dest\n",
      "   path given as input\n",
      "   \"\"\"\n",
      "   if not os.path.exists(src):\n",
      "      print(\"source file does not exist\")\n",
      "      return\n",
      "   if os.path.exists(dst):\n",
      "      print(\"destination file already exists\")\n",
      "      return\n",
      "   if ignore:\n",
      "      ignore = ignore.split(\",\")\n",
      "      for pattern in ignore:\n",
      "         if fnmatch.fnmatch(src, pattern):\n",
      "            print(\"pattern {} matched\".format(pattern))\n",
      "            return\n",
      "   shutil.copyfile(src, dst)\n",
      "   print(\"copied file from {} to {}\".format(src, dst))\n",
      "\n",
      "def copy_dir(src, dst, ignore=None):\n",
      "   \"\"\" this function will simply copy the file from the source path to the dest\n",
      "   path given as input\n",
      "   \"\"\"\n",
      "   if not os.path.exists(src):\n",
      "      print(\"source file does not exist\")\n",
      "      return\n",
      "   if os.path.exists(dst):\n",
      "      print(\"destination file already exists\")\n",
      "      return\n",
      "   if ignore:\n",
      "      ignore = ignore.split(\",\")\n",
      "      for pattern in ignore:\n",
      "         if fnmatch.fnmatch(\n",
      "==========score: 1.0===============\n",
      "def serial_connect(self):\n",
      "\t\t\"\"\"\n",
      "\t\tConnect to the serial device.\n",
      "\t\t\"\"\"\n",
      "\t\tself.serial_connection = serial.Serial(self.serial_port, self.serial_baudrate, timeout=self.serial_timeout)\n",
      "\t\tself.serial_connection.flushInput()\n",
      "\t\tself.serial_connection.flushOutput()\n",
      "\t\tself.serial_connection.write(self.serial_command)\n",
      "\t\tself.serial_connection.flush()\n",
      "\t\tself.serial_connection.flushInput()\n",
      "\t\tself.serial_connection.flushOutput()\n",
      "\t\tself.serial_connection.write(self.serial_command)\n",
      "\t\tself.serial_connection.flush()\n",
      "\t\tself.serial_connection.flushInput()\n",
      "\t\tself.serial_connection.flushOutput()\n",
      "\t\tself.serial_connection.write(self.serial_command)\n",
      "\t\tself.serial_connection.flush()\n",
      "\t\tself.serial_connection.flushInput()\n",
      "\t\tself.serial_connection.flushOutput()\n",
      "\t\tself.serial_connection.write(self.serial_command)\n",
      "\t\tself.serial_connection.flush()\n",
      "\t\tself.serial_connection.flushInput()\n",
      "\t\tself.serial_connection.flushOutput()\n",
      "\t\tself.serial_connection.\n",
      "==========score: 1.0===============\n",
      "def replace_text(filepath, to_replace, replacement):\n",
      "        \"\"\"\n",
      "        Replaces a string in a given file with another string\n",
      "\n",
      "        :param file: the file in which the string has to be replaced\n",
      "        :param to_replace: the string to be replaced in the file\n",
      "        :param replacement: the string which replaces 'to_replace' in the file\n",
      "        \"\"\"\n",
      "        with open(filepath, 'r') as file:\n",
      "            data = file.read()\n",
      "        data = data.replace(to_replace, replacement)\n",
      "        with open(filepath, 'w') as file:\n",
      "            file.write(data)\n",
      "\n",
      "    def get_file_size(filepath):\n",
      "        \"\"\"\n",
      "        Gets the size of a file in bytes\n",
      "\n",
      "        :param filepath: the file whose size is to be obtained\n",
      "        :return: the size of the file in bytes\n",
      "        \"\"\"\n",
      "        return os.path.getsize(filepath)\n",
      "\n",
      "    def get_file_creation_date(filepath):\n",
      "        \"\"\"\n",
      "        Gets the creation date of a file in seconds\n",
      "\n",
      "        :param filepath: the file whose date of creation is to be obtained\n",
      "        :return: the date of creation of the file in seconds\n",
      "        \"\"\"\n",
      "        return os.path.getctime(filepath)\n",
      "\n",
      "    def get_file_last_modified_date(filepath):\n",
      "        \"\"\"\n",
      "        Gets the last modified date of a file in seconds\n",
      "\n",
      "        :param filepath: the file whose date of last modification is to be obtained\n",
      "        \n",
      "==========score: 1.0===============\n",
      "def header_without_lines(header, remove):\n",
      "    \"\"\"Return :py:class:`Header` without lines given in ``remove``\n",
      "\n",
      "    ``remove`` is an iterable of pairs ``key``/``ID`` with the VCF header key\n",
      "    and ``ID`` of entry to remove.  In the case that a line does not have\n",
      "    a ``mapping`` entry, you can give the full value to remove.\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        # header is a vcfpy.Header, e.g., as read earlier from file\n",
      "        new_header = vcfpy.without_header_lines(\n",
      "            header, [('assembly', None), ('FILTER', 'PASS')])\n",
      "        # now, the header lines starting with \"##assembly=\" and the \"PASS\"\n",
      "        # filter line will be missing from new_header\n",
      "    \"\"\"\n",
      "    new_header = header.copy()\n",
      "    for key, ID in remove:\n",
      "        if key not in new_header:\n",
      "            continue\n",
      "        if ID is None:\n",
      "            del new_header[key]\n",
      "        else:\n",
      "            new_header[key] = new_header[key].without_mapping(ID)\n",
      "    return new_header\n",
      "==========score: 1.0===============\n",
      "def load_device(self, serial=None):\n",
      "        \"\"\"Creates an AndroidDevice for the given serial number.\n",
      "\n",
      "        If no serial is given, it will read from the ANDROID_SERIAL\n",
      "        environmental variable. If the environmental variable is not set, then\n",
      "        it will read from 'adb devices' if there is only one.\n",
      "        \"\"\"\n",
      "        if serial is None:\n",
      "            serial = os.environ.get('ANDROID_SERIAL', None)\n",
      "        if serial is None:\n",
      "            if len(self._devices) == 1:\n",
      "                serial = self._devices[0].serial\n",
      "            else:\n",
      "                raise ValueError('No serial specified and multiple devices present')\n",
      "        return self.create_device(serial)\n",
      "\n",
      "    def load_device_by_name(self, name):\n",
      "        \"\"\"Creates an AndroidDevice for the given device name.\n",
      "\n",
      "        This is equivalent to calling `load_device()` with the given name,\n",
      "        except that it will not automatically select the first device.\n",
      "        \"\"\"\n",
      "        return self.create_device(name)\n",
      "\n",
      "    def load_device_by_udid(self, udid):\n",
      "        \"\"\"Creates an AndroidDevice for the given device UDID.\n",
      "\n",
      "        This is equivalent to calling `load_device()` with the given UDID,\n",
      "        except that it will not automatically select the first device.\n",
      "        \"\"\"\n",
      "        return self.create_device(udid)\n",
      "\n",
      "    def load_device_by_index(self, index):\n",
      "        \n",
      "==========score: 1.0===============\n",
      "def _ExtractMetadataFromFileEntry(self, mediator, file_entry, data_stream):\n",
      "    \"\"\"Extracts metadata from a file entry.\n",
      "\n",
      "    Args:\n",
      "      mediator (ParserMediator): mediates the interactions between\n",
      "          parsers and other components, such as storage and abort signals.\n",
      "      file_entry (dfvfs.FileEntry): file entry to extract metadata from.\n",
      "      data_stream (dfvfs.DataStream): data stream or None if the file entry\n",
      "          has no data stream.\n",
      "    \"\"\"\n",
      "    if not file_entry:\n",
      "      return\n",
      "\n",
      "    if data_stream:\n",
      "      self._ExtractMetadataFromDataStream(mediator, file_entry, data_stream)\n",
      "    else:\n",
      "      self._ExtractMetadataFromFileEntry(mediator, file_entry)\n",
      "\n",
      "  def _ExtractMetadataFromFileEntry(self, mediator, file_entry):\n",
      "    \"\"\"Extracts metadata from a file entry.\n",
      "\n",
      "    Args:\n",
      "      mediator (ParserMediator): mediates the interactions between\n",
      "          parsers and other components, such as storage and abort signals.\n",
      "      file_entry (dfvfs.FileEntry): file entry to extract metadata from.\n",
      "    \"\"\"\n",
      "    if file_entry.is_directory:\n",
      "      return\n",
      "\n",
      "    self._SetFileEntry(file_entry)\n",
      "\n",
      "    self._SetEventDataFromFileEntry(mediator, file_entry)\n",
      "\n",
      "    self._SetEventExtractionInfo(mediator, file_entry)\n",
      "\n",
      "    self._SetEventTimestamp(mediator, file_entry)\n",
      "\n",
      "    self._SetEventTimestampFromFileEntry(mediator, file_entry)\n",
      "\n",
      "    self._SetEvent\n",
      "==========score: 1.0===============\n",
      "def sphere_pick_polar(d, n=1, rng=None):\n",
      "    \"\"\"Return vectors uniformly picked on the unit sphere.\n",
      "    Vectors are in a polar representation.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    d: float\n",
      "        The number of dimensions of the space in which the sphere lives.\n",
      "    n: integer\n",
      "        Number of samples to pick.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    r: array, shape (n, d)\n",
      "        Sample vectors.\n",
      "    \"\"\"\n",
      "    if rng is None:\n",
      "        rng = np.random.RandomState()\n",
      "    r = rng.randn(n, d)\n",
      "    r = r / np.linalg.norm(r, axis=1)[:, None]\n",
      "    return r\n",
      "==========score: 1.0===============\n",
      "def __set_html(self, html=None):\n",
      "        \"\"\"\n",
      "        Sets the html content in the View using given body.\n",
      "\n",
      "        :param html: Html content.\n",
      "        :type html: unicode\n",
      "        \"\"\"\n",
      "        self.__html = html\n",
      "\n",
      "    def __set_css(self, css=None):\n",
      "        \"\"\"\n",
      "        Sets the css content in the View using given body.\n",
      "\n",
      "        :param css: Css content.\n",
      "        :type css: unicode\n",
      "        \"\"\"\n",
      "        self.__css = css\n",
      "\n",
      "    def __set_js(self, js=None):\n",
      "        \"\"\"\n",
      "        Sets the js content in the View using given body.\n",
      "\n",
      "        :param js: Js content.\n",
      "        :type js: unicode\n",
      "        \"\"\"\n",
      "        self.__js = js\n",
      "\n",
      "    def __set_title(self, title=None):\n",
      "        \"\"\"\n",
      "        Sets the title in the View using given body.\n",
      "\n",
      "        :param title: Title content.\n",
      "        :type title: unicode\n",
      "        \"\"\"\n",
      "        self.__title = title\n",
      "\n",
      "    def __set_body(self, body=None):\n",
      "        \"\"\"\n",
      "        Sets the body in the View using given body.\n",
      "\n",
      "        :param body: Body content.\n",
      "        :type body: unicode\n",
      "        \"\"\"\n",
      "        self.__body = body\n",
      "\n",
      "    def __set_foot\n",
      "==========score: 1.0===============\n",
      "def group(self):\n",
      "        \"\"\"(re-)group all logevents by the given group.\"\"\"\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.logger.info(\"group: %s\", self.group)\n",
      "        self.\n",
      "==========score: 1.0===============\n",
      "def __convert_num(number):\n",
      "        \"\"\"\n",
      "        All path items are automatically strings. If you think it's an int or float, this attempts to convert it.\n",
      "        :param str number:\n",
      "        :return float or str:\n",
      "        \"\"\"\n",
      "        if isinstance(number, float):\n",
      "            return number\n",
      "        if isinstance(number, int):\n",
      "            return float(number)\n",
      "        return number\n",
      "\n",
      "    def __convert_path(self, path):\n",
      "        \"\"\"\n",
      "        Converts a path to a list of numbers.\n",
      "        :param str path:\n",
      "        :return list:\n",
      "        \"\"\"\n",
      "        return [self.__convert_num(number) for number in path.split(\"/\")]\n",
      "\n",
      "    def __convert_path_to_dict(self, path):\n",
      "        \"\"\"\n",
      "        Converts a path to a dict.\n",
      "        :param str path:\n",
      "        :return dict:\n",
      "        \"\"\"\n",
      "        return {key: self.__convert_num(value) for key, value in self.__convert_path(path).items()}\n",
      "\n",
      "    def __convert_dict_to_path(self, dict):\n",
      "        \"\"\"\n",
      "        Converts a dict to a path.\n",
      "        :param dict dict:\n",
      "        :return str:\n",
      "        \"\"\"\n",
      "        return \"/\".join([\"{}/{}\".format(key, value) for key, value in dict.\n",
      "==========score: 1.0===============\n",
      "def urlEncodeAndJoin(self, seq, sepr=','):\n",
      "        '''sepr.join(urlencode(seq))\n",
      "        Args:\n",
      "            seq: string list to be urlencoded\n",
      "            sepr: join seq with sepr\n",
      "        Returns:\n",
      "            str\n",
      "        '''\n",
      "        return sepr.join(self.urlEncode(seq))\n",
      "\n",
      "    def urlEncode(self, seq):\n",
      "        '''urlencode(seq)\n",
      "        Args:\n",
      "            seq: string list to be urlencoded\n",
      "        Returns:\n",
      "            str\n",
      "        '''\n",
      "        return urlencode(seq)\n",
      "\n",
      "    def urlDecode(self, seq):\n",
      "        '''urlDecode(seq)\n",
      "        Args:\n",
      "            seq: string list to be urldecoded\n",
      "        Returns:\n",
      "            str\n",
      "        '''\n",
      "        return urlparse.parse_qs(seq)\n",
      "\n",
      "    def urlEncodeDict(self, seq):\n",
      "        '''urlEncodeDict(seq)\n",
      "        Args:\n",
      "            seq: dict to be urlencoded\n",
      "        Returns:\n",
      "            str\n",
      "        '''\n",
      "        return urlencode(seq)\n",
      "\n",
      "    def urlDecodeDict(self, seq):\n",
      "        '''urlDecodeDict(seq)\n",
      "        Args:\n",
      "            seq: dict to be urldecoded\n",
      "        Returns:\n",
      "            str\n",
      "        '''\n",
      "        return urlparse.parse_qs(seq)\n",
      "\n",
      "    \n",
      "==========score: 1.0===============\n",
      "def file_unzipper(directory):\n",
      "   \"\"\" This function will unzip all files in the runroot directory and\n",
      "   subdirectories\n",
      "   \"\"\"\n",
      "   for root, dirs, files in os.walk(directory):\n",
      "      for file in files:\n",
      "         if file.endswith(\".zip\"):\n",
      "            print(\"Unzipping file: \" + file)\n",
      "            zip_ref = zipfile.ZipFile(os.path.join(root, file), 'r')\n",
      "            zip_ref.extractall(root)\n",
      "            zip_ref.close()\n",
      "\n",
      "def file_unzipper_with_path(directory):\n",
      "   \"\"\" This function will unzip all files in the runroot directory and\n",
      "   subdirectories\n",
      "   \"\"\"\n",
      "   for root, dirs, files in os.walk(directory):\n",
      "      for file in files:\n",
      "         if file.endswith(\".zip\"):\n",
      "            print(\"Unzipping file: \" + file)\n",
      "            zip_ref = zipfile.ZipFile(os.path.join(root, file), 'r')\n",
      "            zip_ref.extractall(root)\n",
      "            zip_ref.close()\n",
      "            zip_ref.extractall(os.path.join(root, file[:-4]))\n",
      "            zip_ref\n",
      "==========score: 1.0===============\n",
      "def to_json(self, content, pretty_print=False):\n",
      "        \"\"\" Convert a string to a JSON object\n",
      "\n",
      "        ``content`` String content to convert into JSON\n",
      "\n",
      "        ``pretty_print`` If defined, will output JSON is pretty print format\n",
      "        \"\"\"\n",
      "        if pretty_print:\n",
      "            return json.dumps(content, indent=4, sort_keys=True)\n",
      "        else:\n",
      "            return json.dumps(content)\n",
      "\n",
      "    def to_yaml(self, content, pretty_print=False):\n",
      "        \"\"\" Convert a string to a YAML object\n",
      "\n",
      "        ``content`` String content to convert into YAML\n",
      "\n",
      "        ``pretty_print`` If defined, will output YAML is pretty print format\n",
      "        \"\"\"\n",
      "        if pretty_print:\n",
      "            return yaml.safe_dump(content, default_flow_style=False, sort_keys=False)\n",
      "        else:\n",
      "            return yaml.safe_dump(content)\n",
      "\n",
      "    def to_xml(self, content, pretty_print=False):\n",
      "        \"\"\" Convert a string to a XML object\n",
      "\n",
      "        ``content`` String content to convert into XML\n",
      "\n",
      "        ``pretty_print`` If defined, will output XML is pretty print format\n",
      "        \"\"\"\n",
      "        if pretty_print:\n",
      "            return lxml.etree.tostring(\n",
      "                lxml.etree.fromstring(content), pretty_print=True, encoding='utf-\n",
      "==========score: 1.0===============\n",
      "def parse_rc_json():\n",
      "    \"\"\" Reads the json configuration file(.yasirc.json), parses it and returns the\n",
      "    dictionary\n",
      "    \"\"\"\n",
      "    with open(\"./yasirc.json\", \"r\") as f:\n",
      "        rc_json = json.load(f)\n",
      "    return rc_json\n",
      "==========score: 1.0===============\n",
      "def recv_raw(self, timeout, opcodes, **kwargs):\n",
      "        \"\"\"this is very internal, it will return the raw opcode and data if they\n",
      "        match the passed in opcodes\"\"\"\n",
      "        if not self.connected:\n",
      "            raise ConnectionError(\"Connection not open\")\n",
      "        if not isinstance(opcodes, (list, tuple)):\n",
      "            opcodes = (opcodes,)\n",
      "        if not isinstance(timeout, (int, float)):\n",
      "            raise TypeError(\"timeout must be a number\")\n",
      "        if not isinstance(opcodes, (list, tuple)):\n",
      "            raise TypeError(\"opcodes must be a list or tuple\")\n",
      "        if not isinstance(kwargs, dict):\n",
      "            raise TypeError(\"kwargs must be a dictionary\")\n",
      "        if not isinstance(timeout, (int, float)):\n",
      "            raise TypeError(\"timeout must be a number\")\n",
      "        if not isinstance(opcodes, (list, tuple)):\n",
      "            raise TypeError(\"opcodes must be a list or tuple\")\n",
      "        if not isinstance(kwargs, dict):\n",
      "            raise TypeError(\"kwargs must be a dictionary\")\n",
      "        if not isinstance(timeout, (int, float)):\n",
      "            raise TypeError(\"timeout must be a number\")\n",
      "        if not isinstance(opcodes, (list, tuple)):\n",
      "            raise Type\n",
      "==========score: 1.0===============\n",
      "def heatmap(x, y, step=None, min_pt=None, max_pt=None,\n",
      "                 colormap='Blues', alpha=1, grid=False,\n",
      "                 colorbar=True, scale='lin',\n",
      "                 vmax='auto', vmin='auto', crop=True):\n",
      "    \"\"\"\n",
      "    function to take vectors x and y and hist them\n",
      "    \"\"\"\n",
      "    if min_pt is None:\n",
      "        min_pt = np.min(x)\n",
      "    if max_pt is None:\n",
      "        max_pt = np.max(x)\n",
      "    if step is None:\n",
      "        step = (max_pt - min_pt) / 10\n",
      "    if vmax == 'auto':\n",
      "        vmax = max_pt\n",
      "    if vmin == 'auto':\n",
      "        vmin = min_pt\n",
      "    if grid:\n",
      "        plt.grid(True)\n",
      "    if colorbar:\n",
      "        plt.colorbar()\n",
      "    plt.xlim(min_pt, max_pt)\n",
      "    plt.ylim(min_pt, max_pt)\n",
      "    plt.xlabel('x')\n",
      "    plt.ylabel('y')\n",
      "    plt.title('Heatmap')\n",
      "    plt.pcolormesh(x, y, np.zeros_like(x), cmap=colormap, alpha=alpha)\n",
      "    plt.scatter(x, y, c=x, cmap=colormap, alpha=alpha, s=1)\n",
      "    pl\n",
      "==========score: 1.0===============\n",
      "def get_connection(engine, host, user, port, password, database, ssl={}):\n",
      "    \"\"\" Returns a PostgreSQL or MySQL connection \"\"\"\n",
      "    if engine == 'postgres':\n",
      "        return psycopg2.connect(host=host, user=user, password=password, port=port, database=database, sslmode=ssl.get('sslmode', 'prefer'))\n",
      "    elif engine =='mysql':\n",
      "        return mysql.connect(host=host, user=user, password=password, port=port, database=database, ssl={'ssl': {'ca': ssl.get('ca', None), 'cert': ssl.get('cert', None), 'key': ssl.get('key', None)}})\n",
      "    else:\n",
      "        raise Exception('Unknown database engine: %s' % engine)\n",
      "\n",
      "def get_cursor(connection):\n",
      "    \"\"\" Returns a cursor for the given connection \"\"\"\n",
      "    return connection.cursor()\n",
      "\n",
      "\n",
      "==========score: 1.0===============\n",
      "def recv_with_timeout(self, timeout=1):\n",
      "        \"\"\"Receive a complete ISOTP message, blocking until a message is\n",
      "        received or the specified timeout is reached.\n",
      "        If timeout is 0, then this function doesn't block and returns the\n",
      "        first frame in the receive buffer or None if there isn't any.\"\"\"\n",
      "\n",
      "        if timeout == 0:\n",
      "            return self._recv_queue.popleft()\n",
      "\n",
      "        if timeout is None:\n",
      "            timeout = self.timeout\n",
      "\n",
      "        # Check if we have a message in the receive queue\n",
      "        if self._recv_queue:\n",
      "            return self._recv_queue.popleft()\n",
      "\n",
      "        # No message in the queue. Try to read a single frame\n",
      "        frame = self._recv_single()\n",
      "\n",
      "        # Timeout expired\n",
      "        if frame is None:\n",
      "            return None\n",
      "\n",
      "        # Check if the frame is complete\n",
      "        if frame.length == 0:\n",
      "            return frame\n",
      "\n",
      "        # Get the rest of the frame\n",
      "        rest = frame\n",
      "        while rest.length > 0:\n",
      "            frame = self._recv_single()\n",
      "            if frame is None:\n",
      "                return None\n",
      "            # Check if the frame is complete\n",
      "            if frame.length == 0:\n",
      "                return frame\n",
      "            # The frame is not complete, so add the already received part\n",
      "            # and try again\n",
      "            rest.length = rest.length - frame.length\n",
      "            rest.data = rest.data + frame.data\n",
      "\n",
      "        # Should never\n",
      "==========score: 1.0===============\n",
      "def heatmap(z, x=None, y=None, colorscale='Viridis'):\n",
      "    \"\"\"Create a heatmap.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    z : TODO\n",
      "    x : TODO, optional\n",
      "    y : TODO, optional\n",
      "    colorscale : TODO, optional\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    Chart\n",
      "\n",
      "\n",
      "    \"\"\"\n",
      "    if x is None:\n",
      "        x = []\n",
      "    if y is None:\n",
      "        y = []\n",
      "    if colorscale is None:\n",
      "        colorscale = 'Viridis'\n",
      "    return _chart_types['heatmap'](z, x, y, colorscale)\n",
      "==========score: 1.0===============\n",
      "def get_html_source(self):\n",
      "        \"\"\"Gets source page of url\n",
      "        :return: HTML source\n",
      "        \"\"\"\n",
      "        try:\n",
      "            return requests.get(self.url).text\n",
      "        except Exception as e:\n",
      "            print(e)\n",
      "\n",
      "    def get_urls(self):\n",
      "        \"\"\"Gets urls from source page\n",
      "        :return: List of urls\n",
      "        \"\"\"\n",
      "        soup = BeautifulSoup(self.get_html_source(), 'html.parser')\n",
      "        return soup.find('div', class_='listing-container').find_all('a', href=True)\n",
      "\n",
      "    def get_url(self):\n",
      "        \"\"\"Gets url from url list\n",
      "        :return: URL\n",
      "        \"\"\"\n",
      "        urls = self.get_urls()\n",
      "        return urls[0]['href']\n",
      "\n",
      "    def get_title(self):\n",
      "        \"\"\"Gets title from source page\n",
      "        :return: Title\n",
      "        \"\"\"\n",
      "        soup = BeautifulSoup(self.get_html_source(), 'html.parser')\n",
      "        return soup.find('h1', class_='title').text\n",
      "\n",
      "    def get_price(self):\n",
      "        \"\"\"Gets price from source page\n",
      "        :return: Price\n",
      "        \"\"\"\n",
      "        soup\n",
      "==========score: 1.0===============\n",
      "def maps_json():\n",
      "    \"\"\"\n",
      "    Generates a json object which serves as bridge between\n",
      "    the web interface and the map source collection.\n",
      "\n",
      "    All attributes relevant for openlayers are converted into\n",
      "    JSON and served through this route.\n",
      "\n",
      "    Returns:\n",
      "        Response: All map sources as JSON object.\n",
      "    \"\"\"\n",
      "    return jsonify(map_sources)\n",
      "==========score: 1.0===============\n",
      "def median(self, values, axis=0, average=True):\n",
      "        \"\"\"compute the median value over each group.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        values : array_like, [keys, ...]\n",
      "            values to compute the median of per group\n",
      "        axis : int, optional\n",
      "            alternative reduction axis for values\n",
      "        average : bool, optional\n",
      "            when average is true, the average of the two central values is taken for groups with an even key-count\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        unique: ndarray, [groups]\n",
      "            unique keys\n",
      "        reduced : ndarray, [groups, ...]\n",
      "            value array, reduced over groups\n",
      "        \"\"\"\n",
      "        if axis is None:\n",
      "            axis = self.axis\n",
      "        if axis is None:\n",
      "            axis = 0\n",
      "        if axis == 0:\n",
      "            values = values.T\n",
      "        if axis == 1:\n",
      "            values = values.T\n",
      "        if axis == 2:\n",
      "            values = values.T\n",
      "        if axis == 3:\n",
      "            values = values.T\n",
      "        if axis == 4:\n",
      "            values = values.T\n",
      "        if axis == 5:\n",
      "            values = values.T\n",
      "        if axis == 6:\n",
      "            values = values.T\n",
      "        if axis == 7:\n",
      "            values = values.T\n",
      "        if axis == 8:\n",
      "            values = values.T\n",
      "        if axis == 9:\n",
      "            values = values.T\n",
      "        if axis == 10:\n",
      "            values = values.T\n",
      "        if axis == 11:\n",
      "            values = values.T\n",
      "        if axis == 12:\n",
      "            values = values.T\n",
      "        if axis == 13:\n",
      "            values = values.T\n",
      "        if axis == 14:\n",
      "            values = values.T\n",
      "        if axis == 15:\n",
      "            values = values.T\n",
      "        if axis == 16\n",
      "==========score: 1.0===============\n",
      "async def deserialize(data: dict):\n",
      "        \"\"\"\n",
      "        Builds a Proof object with defined attributes.\n",
      "        Attributes are provided by a previous call to the serialize function.\n",
      "        :param data:\n",
      "        Example:\n",
      "        name = \"proof name\"\n",
      "        requested_attrs = [{\"name\": \"age\", \"restrictions\": [{\"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\" } ] }, { \"name\":\"name\", \"restrictions\": [ { \"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\"}]}]\n",
      "        proof = await Proof.create(source_id, name, requested_attrs)\n",
      "        data = proof.serialize()\n",
      "        proof2 = await Proof.deserialize(data)\n",
      "        :return: Proof Object\n",
      "        \"\"\"\n",
      "        proof = await Proof.create(source_id, name, requested_attrs)\n",
      "        data = proof.serialize()\n",
      "        proof2 = await Proof.deserialize(data)\n",
      "        return proof2\n",
      "\n",
      "    @staticmethod\n",
      "    async def create(source_id: str, name: str, requested_attrs: list):\n",
      "        \"\"\"\n",
      "        Creates a Proof object with defined attributes.\n",
      "        Attributes are provided by a previous call to the serialize function.\n",
      "        :param source_id:\n",
      "        :param name:\n",
      "        :param requested_attrs:\n",
      "        Example:\n",
      "        name = \"proof name\"\n",
      "        requested_attrs = [{\"name\": \"age\", \"restrictions\": [{\"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\",\n",
      "==========score: 1.0===============\n",
      "def kmeans_clustering(self, numc, X=None, npcs=15):\n",
      "        \"\"\"Performs k-means clustering.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        numc - int\n",
      "            Number of clusters\n",
      "\n",
      "        npcs - int, optional, default 15\n",
      "            Number of principal components to use as inpute for k-means\n",
      "            clustering.\n",
      "\n",
      "        \"\"\"\n",
      "        if X is None:\n",
      "            X = self.X\n",
      "        if npcs is None:\n",
      "            npcs = self.npcs\n",
      "        if numc is None:\n",
      "            numc = self.numc\n",
      "        if self.n_clusters is None:\n",
      "            self.n_clusters = numc\n",
      "        if self.n_clusters!= numc:\n",
      "            raise ValueError(\"Number of clusters does not match\")\n",
      "        if self.n_clusters == 0:\n",
      "            raise ValueError(\"Number of clusters must be greater than 0\")\n",
      "        if self.n_clusters > self.n_samples:\n",
      "            raise ValueError(\"Number of clusters must be less than or equal to\"\n",
      "                             \" number of samples\")\n",
      "        if self.n_clusters > self.n_features:\n",
      "            raise ValueError(\"Number of clusters must be less than or equal to\"\n",
      "                             \" number of features\")\n",
      "        if self.n_clusters < 2:\n",
      "            raise ValueError(\"Number of clusters must be greater than 1\")\n",
      "        if self.n_clusters == 2:\n",
      "            raise ValueError(\"Number of clusters must be greater\n",
      "==========score: 1.0===============\n",
      "def setcookie(self, key, value, max_age=None, expires=None, path='/', domain=None, secure=None, httponly=False):\n",
      "        \"\"\"\n",
      "        Add a new cookie\n",
      "        \"\"\"\n",
      "        self.cookies[key] = value\n",
      "        if max_age is not None:\n",
      "            self.cookies[key]['max-age'] = max_age\n",
      "        if expires is not None:\n",
      "            self.cookies[key]['expires'] = expires\n",
      "        if path is not None:\n",
      "            self.cookies[key]['path'] = path\n",
      "        if domain is not None:\n",
      "            self.cookies[key]['domain'] = domain\n",
      "        if secure:\n",
      "            self.cookies[key]['secure'] = True\n",
      "        if httponly:\n",
      "            self.cookies[key]['httponly'] = True\n",
      "\n",
      "    def deletecookie(self, key, path='/', domain=None):\n",
      "        \"\"\"\n",
      "        Delete a cookie\n",
      "        \"\"\"\n",
      "        self.setcookie(key, expires=0, max_age=0, path=path, domain=domain)\n",
      "\n",
      "    def get_cookie_value(self, key):\n",
      "        \"\"\"\n",
      "        Get a cookie value\n",
      "        \"\"\"\n",
      "        return self.cookies.get(key, None)\n",
      "\n",
      "    def get_cookie_dict(self):\n",
      "        \"\"\"\n",
      "\n",
      "==========score: 1.0===============\n",
      "def randbelow(num: int) -> int:\n",
      "    \"\"\"Return a random int in the range [0,num).\n",
      "\n",
      "    Raises ValueError if num <= 0, and TypeError if it's not an integer.\n",
      "\n",
      "    >>> randbelow(16)  #doctest:+SKIP\n",
      "    13\n",
      "\n",
      "    \"\"\"\n",
      "    if num <= 0:\n",
      "        raise ValueError(\"randbelow(%d): random number out of range\" % num)\n",
      "    if num == 1:\n",
      "        return 0\n",
      "    if num == 2:\n",
      "        return 1\n",
      "    if num == 3:\n",
      "        return 2\n",
      "    if num == 4:\n",
      "        return 3\n",
      "    if num == 5:\n",
      "        return 4\n",
      "    if num == 6:\n",
      "        return 5\n",
      "    if num == 7:\n",
      "        return 6\n",
      "    if num == 8:\n",
      "        return 7\n",
      "    if num == 9:\n",
      "        return 8\n",
      "    if num == 10:\n",
      "        return 9\n",
      "    if num == 11:\n",
      "        return 10\n",
      "    if num == 12:\n",
      "        return 11\n",
      "    if num == 13:\n",
      "        return 12\n",
      "    if num == 14:\n",
      "        return 13\n",
      "    if num == 15:\n",
      "        return 14\n",
      "    if num == 16:\n",
      "        return 15\n",
      "    if num == 17:\n",
      "        return 16\n",
      "    if num == 18:\n",
      "        return 17\n",
      "    if num == 19:\n",
      "        return 18\n",
      "    if num == 20:\n",
      "        return 19\n",
      "    if num == 21:\n",
      "\n",
      "==========score: 1.0===============\n",
      "def heatmap(z, x=None, y=None, colorscale='Viridis'):\n",
      "    \"\"\"Create a heatmap.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    z : TODO\n",
      "    x : TODO, optional\n",
      "    y : TODO, optional\n",
      "    colorscale : TODO, optional\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    Chart\n",
      "\n",
      "\n",
      "    \"\"\"\n",
      "    if x is None:\n",
      "        x = []\n",
      "    if y is None:\n",
      "        y = []\n",
      "    if colorscale is None:\n",
      "        colorscale = 'Viridis'\n",
      "    return make_chart('heatmap', x, y, z, colorscale)\n",
      "==========score: 1.0===============\n",
      "def write(self, data):\n",
      "        \"\"\"\n",
      "        Writes data to the device.\n",
      "\n",
      "        :param data: data to write\n",
      "        :type data: string\n",
      "\n",
      "        :raises: py:class:`~alarmdecoder.util.CommError`\n",
      "        \"\"\"\n",
      "        self._write(data)\n",
      "\n",
      "    def read(self, size=1):\n",
      "        \"\"\"\n",
      "        Reads data from the device.\n",
      "\n",
      "        :param size: number of bytes to read\n",
      "        :type size: int\n",
      "\n",
      "        :returns: data read\n",
      "        :rtype: string\n",
      "\n",
      "        :raises: py:class:`~alarmdecoder.util.CommError`\n",
      "        \"\"\"\n",
      "        return self._read(size)\n",
      "\n",
      "    def read_until(self, marker):\n",
      "        \"\"\"\n",
      "        Reads data from the device until marker is found.\n",
      "\n",
      "        :param marker: marker to search for\n",
      "        :type marker: string\n",
      "\n",
      "        :returns: data read\n",
      "        :rtype: string\n",
      "\n",
      "        :raises: py:class:`~alarmdecoder.util.CommError`\n",
      "        \"\"\"\n",
      "        return self._read_until(marker)\n",
      "\n",
      "    def read_until_cr(self):\n",
      "        \"\"\"\n",
      "        Reads data from the device until a carriage return is found.\n",
      "\n",
      "        :returns: data read\n",
      "        :rtype: string\n",
      "\n",
      "        :raises: py:class:`~alarmdecoder.util\n",
      "==========score: 1.0===============\n",
      "def randbelow(num: int) -> int:\n",
      "    \"\"\"Return a random int in the range [0,num).\n",
      "\n",
      "    Raises ValueError if num <= 0, and TypeError if it's not an integer.\n",
      "\n",
      "    >>> randbelow(16)  #doctest:+SKIP\n",
      "    13\n",
      "\n",
      "    \"\"\"\n",
      "    if num <= 0:\n",
      "        raise ValueError(\"randbelow() should be used with a positive integer\")\n",
      "    if num == 1:\n",
      "        return 0\n",
      "    min_int = -1\n",
      "    max_int = num\n",
      "    if num % 2 == 0:\n",
      "        min_int = 0\n",
      "        max_int = num // 2\n",
      "    else:\n",
      "        min_int = (num - 1) // 2\n",
      "        max_int = num\n",
      "    return randint(min_int, max_int)\n",
      "\n",
      "==========score: 1.0===============\n",
      "def connect(self, db_uri, debug=False):\n",
      "        \"\"\"Configure connection to a SQL database.\n",
      "\n",
      "        Args:\n",
      "            db_uri (str): path/URI to the database to connect to\n",
      "            debug (Optional[bool]): whether to output logging information\n",
      "        \"\"\"\n",
      "        self.debug = debug\n",
      "        self.db_uri = db_uri\n",
      "        self.engine = create_engine(db_uri)\n",
      "        self.session = scoped_session(sessionmaker(bind=self.engine))\n",
      "\n",
      "    def close(self):\n",
      "        \"\"\"Close the database session.\"\"\"\n",
      "        self.session.remove()\n",
      "\n",
      "    def add(self, model):\n",
      "        \"\"\"Add a model to the database.\n",
      "\n",
      "        Args:\n",
      "            model (Model): model to add\n",
      "        \"\"\"\n",
      "        self.session.add(model)\n",
      "\n",
      "    def commit(self):\n",
      "        \"\"\"Commit the current transaction.\"\"\"\n",
      "        self.session.commit()\n",
      "\n",
      "    def delete(self, model):\n",
      "        \"\"\"Delete a model from the database.\n",
      "\n",
      "        Args:\n",
      "            model (Model): model to delete\n",
      "        \"\"\"\n",
      "        self.session.delete(model)\n",
      "\n",
      "    def get(self, model, id):\n",
      "        \"\"\"Get a model by id.\n",
      "\n",
      "        Args:\n",
      "            model (Model): model to get\n",
      "            id (int): id of the model to get\n",
      "\n",
      "        Returns:\n",
      "            Model: model matching id\n",
      "        \"\"\"\n",
      "        return self.session\n",
      "==========score: 1.0===============\n",
      "def generate_html():\n",
      "    \"\"\"\n",
      "    Generate an HTML file incorporating the images produced by this script\n",
      "    \"\"\"\n",
      "    # Get the images\n",
      "    images = get_images()\n",
      "\n",
      "    # Generate the HTML\n",
      "    html = \"\"\"\n",
      "<html>\n",
      "<head>\n",
      "<style>\n",
      "body {\n",
      "    font-family: sans-serif;\n",
      "    font-size: 14px;\n",
      "}\n",
      "\n",
      "img {\n",
      "    border: 1px solid #000;\n",
      "    border-radius: 50%;\n",
      "    padding: 5px;\n",
      "    margin: 5px;\n",
      "}\n",
      "\n",
      ".image {\n",
      "    display: inline-block;\n",
      "    margin: 5px;\n",
      "    width: 100%;\n",
      "}\n",
      "\n",
      ".image img {\n",
      "    width: 100%;\n",
      "    height: 100%;\n",
      "}\n",
      "\n",
      ".image img {\n",
      "    border: 1px solid #000;\n",
      "    border-radius: 50%;\n",
      "    margin: 5px;\n",
      "    padding: 5px;\n",
      "}\n",
      "\n",
      ".image img {\n",
      "    border-image: url(../images/{0});\n",
      "}\n",
      "\n",
      ".image img {\n",
      "    border-image: url(../images/{0});\n",
      "}\n",
      "\n",
      ".image img {\n",
      "    border-image: url(../images/{0});\n",
      "}\n",
      "\n",
      "==========score: 1.0===============\n",
      "def to_file(self, outputfile=DEFAULT_OUTPUTFILE):\n",
      "        \"\"\"Write the report to a file.\n",
      "        \n",
      "        By default a name is generated.\n",
      "\n",
      "        Parameters:\n",
      "        ----------\n",
      "        outputfile : str\n",
      "            The name or the path of the file to generale including the extension (.html).\n",
      "        \"\"\"\n",
      "        if outputfile is None:\n",
      "            outputfile = self.get_outputfile()\n",
      "        with open(outputfile, 'w') as f:\n",
      "            f.write(self.to_html())\n",
      "\n",
      "    def to_html(self):\n",
      "        \"\"\"Return the report as a string.\n",
      "        \n",
      "        Returns:\n",
      "        -------\n",
      "        str\n",
      "            The report as a string.\n",
      "        \"\"\"\n",
      "        return self.html\n",
      "\n",
      "    def to_dict(self):\n",
      "        \"\"\"Return the report as a dictionary.\n",
      "        \n",
      "        Returns:\n",
      "        -------\n",
      "        dict\n",
      "            The report as a dictionary.\n",
      "        \"\"\"\n",
      "        return self.dict\n",
      "\n",
      "    def to_json(self):\n",
      "        \"\"\"Return the report as a JSON string.\n",
      "        \n",
      "        Returns:\n",
      "        -------\n",
      "        str\n",
      "            The report as a JSON string.\n",
      "        \"\"\"\n",
      "        return json.dumps(self.to_dict())\n",
      "\n",
      "    def to_json_file(self, outputfile=DEFAULT_OUTPUTFILE):\n",
      "        \"\"\"Write the report to a file.\n",
      "        \n",
      "        By default a name is generated.\n",
      "\n",
      "        Parameters:\n",
      "        ----------\n",
      "        outputfile : str\n",
      "==========score: 1.0===============\n",
      "def readline(self, f):\n",
      "        \"\"\"A helper method that only reads uncommented lines\"\"\"\n",
      "        while True:\n",
      "            line = f.readline()\n",
      "            if not line:\n",
      "                break\n",
      "            if not line.startswith('#'):\n",
      "                return line\n",
      "\n",
      "    def read_file(self, filename):\n",
      "        \"\"\"Reads the file and returns a list of lines\"\"\"\n",
      "        with open(filename, 'r') as f:\n",
      "            return self.readline(f)\n",
      "\n",
      "    def read_file_lines(self, filename):\n",
      "        \"\"\"Reads the file and returns a list of lines\"\"\"\n",
      "        with open(filename, 'r') as f:\n",
      "            return f.readlines()\n",
      "\n",
      "    def read_file_lines_to_list(self, filename):\n",
      "        \"\"\"Reads the file and returns a list of lines\"\"\"\n",
      "        with open(filename, 'r') as f:\n",
      "            return f.readlines()\n",
      "\n",
      "    def read_file_lines_to_list_to_dict(self, filename):\n",
      "        \"\"\"Reads the file and returns a list of lines\"\"\"\n",
      "        with open(filename, 'r') as f:\n",
      "            return f.readlines()\n",
      "\n",
      "    def read_file_lines_to_list\n",
      "==========score: 1.0===============\n",
      "def groupby_count(i, key=None, force_keys=None):\n",
      "    \"\"\" Aggregate iterator values into buckets based on how frequently the\n",
      "    values appear.\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> list(groupby_count([1, 1, 1, 2, 3]))\n",
      "        [(1, 3), (2, 1), (3, 1)]\n",
      "    \"\"\"\n",
      "    if key is None:\n",
      "        key = lambda x: x\n",
      "    if force_keys is None:\n",
      "        force_keys = []\n",
      "    return groupby(i, key, lambda k: (k, sum(1 for _ in k)), force_keys)\n",
      "==========score: 1.0===============\n",
      "def get_current_date_time(i):\n",
      "    \"\"\"\n",
      "    Input:  {}\n",
      "\n",
      "    Output: {\n",
      "              return       - return code =  0\n",
      "\n",
      "              array        - array with date and time\n",
      "              iso_datetime - date and time in ISO format\n",
      "            }\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # date and time\n",
      "        iso_datetime = datetime.now().isoformat()\n",
      "        # date\n",
      "        date = datetime.now().date()\n",
      "        # time\n",
      "        time = datetime.now().time()\n",
      "        # array\n",
      "        array = [date, time]\n",
      "        return {'return': 0, 'array': array, 'iso_datetime': iso_datetime}\n",
      "    except Exception as e:\n",
      "        return {'return': -1, 'error': str(e)}\n",
      "==========score: 0.9999===============\n",
      "def copy_dir(self, path):\n",
      "        \"\"\"\n",
      "        Recursively copy directory\n",
      "        \"\"\"\n",
      "        if not os.path.exists(path):\n",
      "            return\n",
      "        if not os.path.isdir(path):\n",
      "            raise ValueError(\"Path must be a directory\")\n",
      "        if not self.exists(path):\n",
      "            raise ValueError(\"Path must exist\")\n",
      "        if not self.is_dir(path):\n",
      "            raise ValueError(\"Path must be a directory\")\n",
      "        if self.is_file(path):\n",
      "            raise ValueError(\"Path must be a directory\")\n",
      "        if self.is_link(path):\n",
      "            raise ValueError(\"Path must be a directory\")\n",
      "        if self.is_file(path):\n",
      "            raise ValueError(\"Path must be a directory\")\n",
      "        if self.is_symlink(path):\n",
      "            raise ValueError(\"Path must be a directory\")\n",
      "        if self.is_socket(path):\n",
      "            raise ValueError(\"Path must be a directory\")\n",
      "        if self.is_fifo(path):\n",
      "            raise ValueError(\"Path must be a directory\")\n",
      "        if self.is_block_device(path):\n",
      "            raise ValueError(\"Path must be a directory\")\n",
      "        if self\n",
      "==========score: 1.0===============\n",
      "def OnCopyResult(self, event):\n",
      "        \"\"\"Clipboard copy results event handler\"\"\"\n",
      "        if self.clipboard is None:\n",
      "            return\n",
      "        self.clipboard.SetData(wx.TextDataObject(self.clipboard.GetValue()))\n",
      "        self.clipboard.Close()\n",
      "        self.clipboard.Flush()\n",
      "\n",
      "    def OnPaste(self, event):\n",
      "        \"\"\"Clipboard paste event handler\"\"\"\n",
      "        if self.clipboard is None:\n",
      "            return\n",
      "        data = self.clipboard.GetData()\n",
      "        if not data.IsText():\n",
      "            return\n",
      "        self.clipboard.SetValue(data.GetText())\n",
      "        self.clipboard.Close()\n",
      "        self.clipboard.Flush()\n",
      "\n",
      "    def OnPasteResult(self, event):\n",
      "        \"\"\"Clipboard paste results event handler\"\"\"\n",
      "        if self.clipboard is None:\n",
      "            return\n",
      "        self.clipboard.SetData(wx.TextDataObject(self.clipboard.GetValue()))\n",
      "        self.clipboard.Close()\n",
      "        self.clipboard.Flush()\n",
      "\n",
      "    def OnClear(self, event):\n",
      "        \"\"\"Clear event handler\"\"\"\n",
      "        self.clipboard.SetData(\n",
      "==========score: 1.0===============\n",
      "def create_cookie(name, value, domain, httponly=None, **kwargs):\n",
      "    \"\"\"Creates `cookielib.Cookie` instance\"\"\"\n",
      "    kwargs.update({\n",
      "        \"version\": 0,\n",
      "        \"name\": name,\n",
      "        \"value\": value,\n",
      "        \"port\": None,\n",
      "        \"port_specified\": False,\n",
      "        \"domain\": domain,\n",
      "        \"domain_specified\": True,\n",
      "        \"domain_initial_dot\": False,\n",
      "        \"path\": \"/\",\n",
      "        \"path_specified\": True,\n",
      "        \"secure\": False,\n",
      "        \"expires\": None,\n",
      "        \"discard\": True,\n",
      "        \"comment\": None,\n",
      "        \"comment_url\": None,\n",
      "        \"rest\": {\"HttpOnly\": httponly},\n",
      "        \"rfc2109\": False,\n",
      "    })\n",
      "    cookie = Cookie(**kwargs)\n",
      "    return cookie\n",
      "==========score: 0.9999===============\n",
      "def linear_regression(self):\n",
      "        \"\"\" Linear Regression.\n",
      "\n",
      "        This function runs linear regression and stores the, \n",
      "        1. Model\n",
      "        2. Model name \n",
      "        3. Mean score of cross validation\n",
      "        4. Metrics\n",
      "\n",
      "        \"\"\"\n",
      "        self.model = LinearRegression()\n",
      "        self.model_name = 'Linear Regression'\n",
      "        self.mean_score = self.cross_validation(self.model)\n",
      "        self.metrics = self.model.get_params()\n",
      "        self.metrics.update({'model_name':self.model_name})\n",
      "        self.metrics.update({'mean_score':self.mean_score})\n",
      "        self.metrics.update({'model':self.model})\n",
      "        return self.metrics\n",
      "\n",
      "    def logistic_regression(self):\n",
      "        \"\"\" Logistic Regression.\n",
      "\n",
      "        This function runs logistic regression and stores the, \n",
      "        1. Model\n",
      "        2. Model name \n",
      "        3. Mean score of cross validation\n",
      "        4. Metrics\n",
      "\n",
      "        \"\"\"\n",
      "        self.model = LogisticRegression()\n",
      "        self.model_name = 'Logistic Regression'\n",
      "        self.mean_score = self.cross_validation(self.model)\n",
      "        self.metrics = self.model.get_params()\n",
      "        self.metrics.update({'model_name':\n",
      "==========score: 1.0===============\n",
      "def confusion_matrix(expected: np.ndarray, predicted: np.ndarray, num_classes: int) -> np.ndarray:\n",
      "    \"\"\"\n",
      "    Calculate and return confusion matrix for the predicted and expected labels\n",
      "\n",
      "    :param expected: array of expected classes (integers) with shape `[num_of_data]`\n",
      "    :param predicted: array of predicted classes (integers) with shape `[num_of_data]`\n",
      "    :param num_classes: number of classification classes\n",
      "    :return: confusion matrix (cm) with absolute values\n",
      "    \"\"\"\n",
      "    cm = np.zeros((num_classes, num_classes))\n",
      "    for i in range(num_classes):\n",
      "        for j in range(num_classes):\n",
      "            cm[i, j] = np.sum(np.logical_and(expected == i, predicted == j))\n",
      "    return cm\n",
      "==========score: 1.0===============\n",
      "def __convert_num(number):\n",
      "        \"\"\"\n",
      "        All path items are automatically strings. If you think it's an int or float, this attempts to convert it.\n",
      "        :param str number:\n",
      "        :return float or str:\n",
      "        \"\"\"\n",
      "        if isinstance(number, str):\n",
      "            if number.isdigit():\n",
      "                return float(number)\n",
      "            else:\n",
      "                return number\n",
      "        else:\n",
      "            return number\n",
      "\n",
      "    def __convert_path(self, path):\n",
      "        \"\"\"\n",
      "        Converts a path to a list of numbers.\n",
      "        :param str path:\n",
      "        :return list:\n",
      "        \"\"\"\n",
      "        return [self.__convert_num(x) for x in path.split('/')]\n",
      "\n",
      "    def __convert_path_list(self, path_list):\n",
      "        \"\"\"\n",
      "        Converts a list of paths to a list of numbers.\n",
      "        :param list path_list:\n",
      "        :return list:\n",
      "        \"\"\"\n",
      "        return [self.__convert_path(x) for x in path_list]\n",
      "\n",
      "    def __get_path_list(self, path):\n",
      "        \"\"\"\n",
      "        Gets a list of paths from a path.\n",
      "        :param str path:\n",
      "        :return list:\n",
      "        \"\"\"\n",
      "        if path.startswith('/'):\n",
      "            return self.__convert_path_list(path.split\n",
      "==========score: 1.0===============\n",
      "def get_conn(self):\n",
      "        \"\"\"\n",
      "        Returns a mssql connection object\n",
      "        \"\"\"\n",
      "        conn = self.get_connection(self.conn_id)\n",
      "        conn = self.get_mssql_connection(conn)\n",
      "\n",
      "        self.log.info(\n",
      "            \"Establishing a connection to database using \"\n",
      "            \"connection settings '{}'\".format(\n",
      "                json.dumps(conn.extra_dejson, indent=4)\n",
      "            )\n",
      "        )\n",
      "        return conn\n",
      "\n",
      "    def set_autocommit(self, conn, autocommit):\n",
      "        \"\"\"\n",
      "        Sets autocommit on the connection\n",
      "\n",
      "        :param conn: connection to set autocommit setting to True or False\n",
      "        :type conn: connection object\n",
      "        :param autocommit: The autocommit setting to set.\n",
      "        :type autocommit: bool\n",
      "        \"\"\"\n",
      "        conn.autocommit = autocommit\n",
      "\n",
      "==========score: 1.0===============\n",
      "def unique_by_index(sequence):\n",
      "    \"\"\" unique elements in `sequence` in the order in which they occur\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    sequence : iterable\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    uniques : list\n",
      "        unique elements of sequence, ordered by the order in which the element\n",
      "        occurs in `sequence`\n",
      "    \"\"\"\n",
      "    seen = set()\n",
      "    uniques = []\n",
      "    for item in sequence:\n",
      "        if item in seen:\n",
      "            continue\n",
      "        seen.add(item)\n",
      "        uniques.append(item)\n",
      "    return uniques\n",
      "==========score: 1.0===============\n",
      "def make_diagonal_povm(pi_basis, confusion_rate_matrix):\n",
      "    \"\"\"\n",
      "    Create a DiagonalPOVM from a ``pi_basis`` and the ``confusion_rate_matrix`` associated with a\n",
      "    readout.\n",
      "\n",
      "    See also the grove documentation.\n",
      "\n",
      "    :param OperatorBasis pi_basis: An operator basis of rank-1 projection operators.\n",
      "    :param numpy.ndarray confusion_rate_matrix: The matrix of detection probabilities conditional\n",
      "    on a prepared qubit state.\n",
      "    :return: The POVM corresponding to confusion_rate_matrix.\n",
      "    :rtype: DiagonalPOVM\n",
      "    \"\"\"\n",
      "    # pylint: disable=invalid-name\n",
      "    # pylint: disable=unused-variable\n",
      "    # pylint: disable=unused-argument\n",
      "    # pylint: disable=no-member\n",
      "    # pylint: disable=too-many-locals\n",
      "    # pylint: disable=too-many-branches\n",
      "    # pylint: disable=too-many-statements\n",
      "\n",
      "    # Check the input\n",
      "    if not isinstance(pi_basis, OperatorBasis):\n",
      "        raise TypeError('pi_basis must be an OperatorBasis object.')\n",
      "\n",
      "    if not isinstance(confusion_rate_matrix, numpy.ndarray):\n",
      "        raise TypeError('confusion_rate_matrix must be a numpy.ndarray object.')\n",
      "\n",
      "    if confusion_rate_matrix.shape!= (2, 2):\n",
      "        raise ValueError('confusion_rate_matrix must be of shape (2, 2)')\n",
      "\n",
      "    # Check the input\n",
      "    if not is_hermitian_matrix(confusion_rate_matrix):\n",
      "        raise ValueError('conf\n",
      "==========score: 1.0===============\n",
      "def memoize(cache=None):\n",
      "    \"\"\"\n",
      "    | Implements method / definition memoization.\n",
      "    | Any method / definition decorated will get its return value cached and restored whenever called with the same arguments.\n",
      "\n",
      "    :param cache: Alternate cache.\n",
      "    :type cache: dict\n",
      "    :return: Object.\n",
      "    :rtype: object\n",
      "    \"\"\"\n",
      "    def memoize_decorator(func):\n",
      "        \"\"\"\n",
      "        | Decorator.\n",
      "        | Any method / definition decorated will get its return value cached and restored whenever called with the same arguments.\n",
      "\n",
      "        :param func: Object.\n",
      "        :type func: function\n",
      "        :return: Object.\n",
      "        :rtype: function\n",
      "        \"\"\"\n",
      "        if cache is None:\n",
      "            cache = {}\n",
      "\n",
      "        def wrapper(*args, **kwargs):\n",
      "            \"\"\"\n",
      "            | Wrapper.\n",
      "            | Any method / definition decorated will get its return value cached and restored whenever called with the same arguments.\n",
      "\n",
      "            :param args: Object.\n",
      "            :type args: tuple\n",
      "            :param kwargs: Object.\n",
      "            :type kwargs: dict\n",
      "            :return: Object.\n",
      "            :rtype: object\n",
      "            \"\"\"\n",
      "            key = str(args) + str(kwargs)\n",
      "            if key in cache:\n",
      "                return cache[key]\n",
      "            else:\n",
      "                value = func(*args, **kwargs)\n",
      "                cache[key] = value\n",
      "                return value\n",
      "        return wrapper\n",
      "    return memoize_decorator\n",
      "\n",
      "==========score: 1.0===============\n",
      "def convert(self, content, conversion):\n",
      "        \"\"\"Convert content to Python data structures.\"\"\"\n",
      "        if conversion == 'json':\n",
      "            return json.loads(content)\n",
      "        elif conversion == 'yaml':\n",
      "            return yaml.safe_load(content)\n",
      "        else:\n",
      "            raise ValueError('Unknown conversion: {}'.format(conversion))\n",
      "\n",
      "    def get_data(self, path, conversion='json'):\n",
      "        \"\"\"Get data from the path.\"\"\"\n",
      "        if not path:\n",
      "            return None\n",
      "        if not path.startswith('/'):\n",
      "            path = '/' + path\n",
      "        url = self.url + path\n",
      "        headers = {'Accept': 'application/json'}\n",
      "        response = requests.get(url, headers=headers)\n",
      "        if response.status_code == 200:\n",
      "            return self.convert(response.text, conversion)\n",
      "        else:\n",
      "            raise RuntimeError('Error getting data: {}'.format(response.text))\n",
      "\n",
      "    def get_data_file(self, path, conversion='json'):\n",
      "        \"\"\"Get data from the path.\"\"\"\n",
      "        if not path:\n",
      "            return None\n",
      "        if not path.startswith('/'):\n",
      "            path = '/\n",
      "==========score: 1.0===============\n",
      "def fuzzmatch(self, fuzzkey, multi=False):\n",
      "        \"\"\"\n",
      "        Identify a filter by fuzzy string matching.\n",
      "\n",
      "        Partial ('fuzzy') matching performed by `fuzzywuzzy.fuzzy.ratio`\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fuzzkey : str\n",
      "            A string that partially matches one filter name more than the others.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The name of the most closely matched filter. : str\n",
      "        \"\"\"\n",
      "        if not self.filters:\n",
      "            return None\n",
      "        if multi:\n",
      "            return self._fuzzy_multi(fuzzkey)\n",
      "        else:\n",
      "            return self._fuzzy_single(fuzzkey)\n",
      "\n",
      "    def _fuzzy_single(self, fuzzkey):\n",
      "        \"\"\"\n",
      "        Identify a filter by fuzzy string matching.\n",
      "\n",
      "        Partial ('fuzzy') matching performed by `fuzzywuzzy.fuzzy.ratio`\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fuzzkey : str\n",
      "            A string that partially matches one filter name more than the others.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The name of the most closely matched filter. : str\n",
      "        \"\"\"\n",
      "        if not self.filters:\n",
      "            return None\n",
      "        fuzzy = fuzzywuzzy.fuzzy.ratio(fuzzkey, self.filters[0])\n",
      "        if fuzzy > self.threshold:\n",
      "            return self.filters[0]\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def _fuzzy_multi(self, fuzzkey):\n",
      "        \"\"\"\n",
      "        Identify a filter by fuzzy string matching\n",
      "==========score: 1.0===============\n",
      "def _dt_to_epoch(self, dt):\n",
      "        \"\"\"\n",
      "        Convert a offset-aware datetime to POSIX time.\n",
      "        \"\"\"\n",
      "        return (dt - self.EPOCH).total_seconds()\n",
      "\n",
      "    def _dt_to_timestamp(self, dt):\n",
      "        \"\"\"\n",
      "        Convert a UTC datetime to POSIX timestamp.\n",
      "        \"\"\"\n",
      "        return (dt - self.EPOCH).total_seconds()\n",
      "\n",
      "    def _timestamp_to_dt(self, timestamp):\n",
      "        \"\"\"\n",
      "        Convert a POSIX timestamp to a UTC datetime.\n",
      "        \"\"\"\n",
      "        return self.EPOCH + datetime.timedelta(seconds=timestamp)\n",
      "\n",
      "    def _timestamp_to_micros(self, timestamp):\n",
      "        \"\"\"\n",
      "        Convert a POSIX timestamp to microseconds since the UNIX epoch.\n",
      "        \"\"\"\n",
      "        return int(timestamp * 1e6)\n",
      "\n",
      "    def _dt_to_micros(self, dt):\n",
      "        \"\"\"\n",
      "        Convert a UTC datetime to microseconds since the UNIX epoch.\n",
      "        \"\"\"\n",
      "        return self._timestamp_to_micros(self._dt_to_timestamp(dt))\n",
      "\n",
      "    def _micros_to_dt(self, micros):\n",
      "        \"\"\"\n",
      "==========score: 1.0===============\n",
      "def get_table_location(self, database_name, table_name):\n",
      "        \"\"\"\n",
      "        Get the physical location of the table\n",
      "\n",
      "        :param database_name: Name of hive database (schema) @table belongs to\n",
      "        :type database_name: str\n",
      "        :param table_name: Name of hive table\n",
      "        :type table_name: str\n",
      "        :return: str\n",
      "        \"\"\"\n",
      "        resp = self.get_conn().get_table(dbname=database_name, tbl_name=table_name)\n",
      "        return resp.location\n",
      "\n",
      "    def get_table(self, database, table):\n",
      "        \"\"\"\n",
      "        Get the details of the table/view from the database.\n",
      "\n",
      "        :param database: Name of hive database (schema) @table belongs to\n",
      "        :type database: str\n",
      "        :param table: Name of hive table\n",
      "        :type table: str\n",
      "        :return: table schema\n",
      "        \"\"\"\n",
      "        self.get_conn().get_table(dbname=database, tbl_name=table)\n",
      "        # pylint: disable=no-member\n",
      "        return self.get_conn().get_table(dbname=database, tbl_name=table)\n",
      "\n",
      "    def get_tables(self, database, pattern='*'):\n",
      "        \"\"\"\n",
      "        Get list of tables from the database\n",
      "\n",
      "        :param database: Name of hive database @table belongs to\n",
      "        :type database: str\n",
      "        :param pattern: Get only tables matching the pattern (optional)\n",
      "        :type pattern: str\n",
      "        :return:\n",
      "==========score: 1.0===============\n",
      "def diff(s1, s2):\n",
      "    \"\"\"\n",
      "    Return a normalised Levenshtein distance between two strings.\n",
      "\n",
      "    Distance is normalised by dividing the Levenshtein distance of the\n",
      "    two strings by the max(len(s1), len(s2)).\n",
      "\n",
      "    Examples:\n",
      "\n",
      "        >>> text.diff(\"foo\", \"foo\")\n",
      "        0\n",
      "\n",
      "        >>> text.diff(\"foo\", \"fooo\")\n",
      "        1\n",
      "\n",
      "        >>> text.diff(\"foo\", \"\")\n",
      "        1\n",
      "\n",
      "        >>> text.diff(\"1234\", \"1 34\")\n",
      "        1\n",
      "\n",
      "    Arguments:\n",
      "\n",
      "        s1 (str): Argument A.\n",
      "        s2 (str): Argument B.\n",
      "\n",
      "    Returns:\n",
      "\n",
      "        float: Normalised distance between the two strings.\n",
      "    \"\"\"\n",
      "    return 1.0 - (levenshtein(s1, s2) / max(len(s1), len(s2)))\n",
      "==========score: 1.0===============\n",
      "def encode(self):\n",
      "        \"\"\"\n",
      "        Encodes the current state of the object into a string.\n",
      "\n",
      "        :return: The encoded string\n",
      "        \"\"\"\n",
      "        return json.dumps(self.__dict__)\n",
      "\n",
      "    def __str__(self):\n",
      "        \"\"\"\n",
      "        Returns the string representation of the object.\n",
      "\n",
      "        :return: The string representation of the object\n",
      "        \"\"\"\n",
      "        return self.__repr__()\n",
      "\n",
      "    def __repr__(self):\n",
      "        \"\"\"\n",
      "        Returns the string representation of the object.\n",
      "\n",
      "        :return: The string representation of the object\n",
      "        \"\"\"\n",
      "        return self.__class__.__name__ + '(' + self.encode() + ')'\n",
      "==========score: 1.0===============\n",
      "def int_to_hex(i):\n",
      "    \"\"\"Create a hex-representation of the given serial.\n",
      "\n",
      "    >>> int_to_hex(12345678)\n",
      "    'BC:61:4E'\n",
      "    \"\"\"\n",
      "    return ':'.join('%02x' % x for x in i.to_bytes(4, 'big'))\n",
      "==========score: 1.0===============\n",
      "def deserialize(self):\n",
      "        \"\"\" Invoke the RFC 7159 spec compliant parser\n",
      "\n",
      "        :return:\n",
      "            the parsed & vetted request body\n",
      "        \"\"\"\n",
      "        try:\n",
      "            return self.parser.parse(self.raw_data)\n",
      "        except Exception as e:\n",
      "            raise ParseError(e)\n",
      "\n",
      "    def serialize(self):\n",
      "        \"\"\" Serialize the request body\n",
      "\n",
      "        :return:\n",
      "            the serialized request body\n",
      "        \"\"\"\n",
      "        try:\n",
      "            return self.parser.serialize(self.raw_data)\n",
      "        except Exception as e:\n",
      "            raise SerializeError(e)\n",
      "\n",
      "    def validate(self):\n",
      "        \"\"\" Validate the request body\n",
      "\n",
      "        :return:\n",
      "            the validated request body\n",
      "        \"\"\"\n",
      "        try:\n",
      "            return self.parser.validate(self.raw_data)\n",
      "        except Exception as e:\n",
      "            raise ValidateError(e)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return '<{0}({1})>'.format(self.__class__.__name__, self.raw_data)\n",
      "\n",
      "    def __str__(self):\n",
      "        return self.__repr__()\n",
      "\n",
      "==========score: 1.0===============\n",
      "def runningMedian(seq, M):\n",
      "    \"\"\"\n",
      "     Purpose: Find the median for the points in a sliding window (odd number in size)\n",
      "              as it is moved from left to right by one point at a time.\n",
      "      Inputs:\n",
      "            seq -- list containing items for which a running median (in a sliding window)\n",
      "                   is to be calculated\n",
      "              M -- number of items in window (window size) -- must be an integer > 1\n",
      "      Otputs:\n",
      "         medians -- list of medians with size N - M + 1\n",
      "       Note:\n",
      "         1. The median of a finite list of numbers is the \"center\" value when this list\n",
      "            is sorted in ascending order.\n",
      "         2. If M is an even number the two elements in the window that\n",
      "            are close to the center are averaged to give the median (this\n",
      "            is not by definition)\n",
      "    \"\"\"\n",
      "    # Check if M is an even number\n",
      "    if M % 2 == 0:\n",
      "        print(\"M must be an odd number\")\n",
      "        return\n",
      "    # Check if M is an integer\n",
      "    if not isinstance(M, int):\n",
      "        print(\"M must be an integer\")\n",
      "        return\n",
      "    # Check if seq is a list\n",
      "    if not isinstance(seq, list):\n",
      "        print(\"seq must be a list\")\n",
      "        return\n",
      "    # Check if seq is not empty\n",
      "    if len(seq) == 0:\n",
      "        print(\"seq must not be empty\")\n",
      "        return\n",
      "    # Check if seq is not empty\n",
      "    if M < 1:\n",
      "        print(\"M must be greater than 0\")\n",
      "        return\n",
      "    # Check if seq is not empty\n",
      "    if len(seq) < M:\n",
      "        print(\"seq must be greater than M\")\n",
      "        return\n",
      "    # Check if seq is not empty\n",
      "    if M > len(seq):\n",
      "        print(\"M must be less than or equal to the length of seq\")\n",
      "        return\n",
      "    # Check if seq is not empty\n",
      "    if M == 1:\n",
      "        print(\"M must be greater\n",
      "==========score: 1.0===============\n",
      "def get_table_location(self, database_name, table_name):\n",
      "        \"\"\"\n",
      "        Get the physical location of the table\n",
      "\n",
      "        :param database_name: Name of hive database (schema) @table belongs to\n",
      "        :type database_name: str\n",
      "        :param table_name: Name of hive table\n",
      "        :type table_name: str\n",
      "        :return: str\n",
      "        \"\"\"\n",
      "        resp = self.get_conn().get_table(\n",
      "            dbname=database_name,\n",
      "            tbl_name=table_name)\n",
      "        return resp['tableLocation'] if 'tableLocation' in resp else None\n",
      "\n",
      "    def get_tables(self, database, pattern='*'):\n",
      "        \"\"\"\n",
      "        Get a metastore table.\n",
      "\n",
      "        >>> hh = HiveMetastoreHook()\n",
      "        >>> tables = hh.get_tables(database='airflow', pattern='airflow.*_temp_')\n",
      "        >>> tables\n",
      "        [{'comment': None, 'db': 'airflow', 'dragnet': 'airflow_temp_partitioned_tst', 'is_view': False, 'owners': ['airflow'], 'table': 'airflow_temp_partitioned_tst'}]\n",
      "\n",
      "        :param database: The name of the database, table belongs to\n",
      "        :type database: str\n",
      "        :param pattern: The pattern of the table name. Defaults to consuming\n",
      "            everything\n",
      "        :type pattern: str\n",
      "        :return: a list of dicts describing the tables, see the\n",
      "==========score: 1.0===============\n",
      "def _encrypt(data):\n",
      "    \"\"\"Equivalent to OpenSSL using 256 bit AES in CBC mode\"\"\"\n",
      "    iv = Random.new().read(AES.block_size)\n",
      "    cipher = AES.new(key, AES.MODE_CBC, iv)\n",
      "    return base64.b64encode(iv + cipher.encrypt(data))\n",
      "==========score: 0.9999===============\n",
      "def export_xlsx(wb, output, fn):\n",
      "    \"\"\"\n",
      "    export as excel\n",
      "    wb:\n",
      "    output:\n",
      "    fn: file name\n",
      "    \"\"\"\n",
      "    wb.save(output)\n",
      "    print('excel file saved as {}'.format(fn))\n",
      "==========score: 1.0===============\n",
      "def optimize(self, x0, f=None, df=None, f_df=None):\n",
      "        \"\"\"\n",
      "        :param x0: initial point for a local optimizer.\n",
      "        :param f: function to optimize.\n",
      "        :param df: gradient of the function to optimize.\n",
      "        :param f_df: returns both the function to optimize and its gradient.\n",
      "        \"\"\"\n",
      "        if f is None:\n",
      "            f = self.f\n",
      "        if df is None:\n",
      "            df = self.df\n",
      "        if f_df is None:\n",
      "            f_df = self.f_df\n",
      "\n",
      "        if self.verbose:\n",
      "            print(\"Optimizing...\")\n",
      "\n",
      "        # Initialize\n",
      "        x = x0\n",
      "        f_x = f(x)\n",
      "        df_x = df(x)\n",
      "        f_df_x = f_df(x)\n",
      "\n",
      "        # Iterate\n",
      "        for i in range(self.max_iter):\n",
      "            if self.verbose:\n",
      "                print(\"Iteration:\", i)\n",
      "            # Compute gradient\n",
      "            df_x = df(x)\n",
      "            # Compute Hessian\n",
      "            H = self.hessian(x)\n",
      "            # Compute new point\n",
      "            x_new = self.newton_step(x, df_x, H, f_df_x)\n",
      "            # Check if new point is better\n",
      "            f_x_new = f(x_new)\n",
      "            if f_x_new < f_x:\n",
      "                x = x_\n",
      "==========score: 1.0===============\n",
      "def initialize(g, app):\n",
      "    \"\"\"\n",
      "    If postgresql url is defined in configuration params a\n",
      "    scoped session will be created\n",
      "    \"\"\"\n",
      "    if app.config.get('POSTGRESQL_URL'):\n",
      "        g.db = scoped_session(\n",
      "            sessionmaker(\n",
      "                autocommit=False,\n",
      "                autoflush=False,\n",
      "                bind=engine\n",
      "            )\n",
      "        )\n",
      "    else:\n",
      "        g.db = scoped_session(sessionmaker(autocommit=False))\n",
      "==========score: 1.0===============\n",
      "def _ExtractMetadataFromFileEntry(self, mediator, file_entry, data_stream):\n",
      "    \"\"\"Extracts metadata from a file entry.\n",
      "\n",
      "    Args:\n",
      "      mediator (ParserMediator): mediates the interactions between\n",
      "          parsers and other components, such as storage and abort signals.\n",
      "      file_entry (dfvfs.FileEntry): file entry to extract metadata from.\n",
      "      data_stream (dfvfs.DataStream): data stream or None if the file entry\n",
      "          has no data stream.\n",
      "    \"\"\"\n",
      "    if not file_entry.HasParent():\n",
      "      return\n",
      "\n",
      "    file_object = file_entry.GetFileObject()\n",
      "    if data_stream:\n",
      "      data_size = data_stream.get_size()\n",
      "    else:\n",
      "      data_size = file_object.get_size()\n",
      "\n",
      "    # Note that the file entry is not closed.\n",
      "    file_object.seek(0, os.SEEK_SET)\n",
      "\n",
      "    # Note that the file entry is not closed.\n",
      "    file_object.seek(0, os.SEEK_SET)\n",
      "\n",
      "    # Note that the file entry is not closed.\n",
      "    file_object.seek(0, os.SEEK_SET)\n",
      "\n",
      "    # Note that the file entry is not closed.\n",
      "    file_object.seek(0, os.SEEK_SET)\n",
      "\n",
      "    # Note that the file entry is not closed.\n",
      "    file_object.seek(0, os.SEEK_SET)\n",
      "\n",
      "    # Note that the file entry is not closed.\n",
      "    file_object.seek(0, os.SEEK_SET)\n",
      "\n",
      "    # Note that the file entry is not closed.\n",
      "    file\n",
      "==========score: 1.0===============\n",
      "def _string_substr(self, start, length=None):\n",
      "    \"\"\"\n",
      "    Pull substrings out of each string value by position and maximum\n",
      "    length.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    start : int\n",
      "      First character to start splitting, indices starting at 0 (like\n",
      "      Python)\n",
      "    length : int, optional\n",
      "      Maximum length of each substring. If not supplied, splits each string\n",
      "      to the end\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    substrings : type of caller\n",
      "    \"\"\"\n",
      "    if length is None:\n",
      "        length = len(self) - start\n",
      "    return self.values.ravel().astype(str)[start:start+length]\n",
      "==========score: 1.0===============\n",
      "def multiply(self, matrix):\n",
      "        \"\"\"\n",
      "        Multiply this matrix by a local dense matrix on the right.\n",
      "\n",
      "        :param matrix: a local dense matrix whose number of rows must match the number of columns\n",
      "                       of this matrix\n",
      "        :returns: :py:class:`RowMatrix`\n",
      "\n",
      "        >>> rm = RowMatrix(sc.parallelize([[0, 1], [2, 3]]))\n",
      "        >>> rm.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()\n",
      "        [DenseVector([2.0, 3.0]), DenseVector([6.0, 11.0])]\n",
      "        \"\"\"\n",
      "        if not isinstance(matrix, DenseMatrix):\n",
      "            raise ValueError(\"Only multiplication with DenseMatrix \"\n",
      "                             \"is supported.\")\n",
      "        matrix = matrix.toArray()\n",
      "        return self._java_matrix_wrapper.call(\"multiply\", matrix)\n",
      "\n",
      "    @since('2.0.0')\n",
      "    def toLocalMatrix(self):\n",
      "        \"\"\"\n",
      "        Convert this matrix into a local dense matrix.\n",
      "\n",
      "        :returns: a local dense matrix\n",
      "        \"\"\"\n",
      "        return self._java_matrix_wrapper.call(\"toLocalMatrix\")\n",
      "\n",
      "    @since('2.0.0')\n",
      "    def toLocalMatrixAndDrop(self):\n",
      "        \"\"\"\n",
      "        Convert this matrix into a local dense matrix and drop the duplicate rows.\n",
      "\n",
      "        :returns: a local dense matrix that drops duplicate rows\n",
      "        \"\"\"\n",
      "        return self._java_matrix_wrapper.call(\"toLocalMatrixAndDrop\")\n",
      "\n",
      "    @since('2.0.0')\n",
      "    def toLocalMatrixAndDropUselessRows(self):\n",
      "        \"\"\"\n",
      "        Convert this matrix into a local dense matrix and drop the rows with duplicate entries.\n",
      "==========score: 1.0===============\n",
      "def findAllSubstrings(string, substring):\n",
      "    \"\"\" Returns a list of all substring starting positions in string or an empty\n",
      "    list if substring is not present in string.\n",
      "\n",
      "    :param string: a template string\n",
      "    :param substring: a string, which is looked for in the ``string`` parameter.\n",
      "\n",
      "    :returns: a list of substring starting positions in the template string\n",
      "    \"\"\"\n",
      "    if not substring:\n",
      "        return []\n",
      "    if not string:\n",
      "        return []\n",
      "    if len(string) < len(substring):\n",
      "        return []\n",
      "    if len(string) == len(substring):\n",
      "        if string == substring:\n",
      "            return [0]\n",
      "        return []\n",
      "    if len(string) == len(substring) + 1:\n",
      "        if string[0] == substring[0]:\n",
      "            return [0]\n",
      "        return []\n",
      "    if len(string) == len(substring) + 2:\n",
      "        if string[0] == substring[0] and string[1] == substring[1]:\n",
      "            return [1]\n",
      "        return []\n",
      "    if len(string) == len(substring) + 3:\n",
      "        if string[0] == substring[0] and string[1] == substring[1] and \\\n",
      "                string[2] == substring[2]:\n",
      "            return [2]\n",
      "        return []\n",
      "    if len(string) == len(substring) + 4:\n",
      "        if string[0] == substring[0] and string[1]\n",
      "==========score: 1.0===============\n",
      "def levenshtein(left, right):\n",
      "    \"\"\"Computes the Levenshtein distance of the two given strings.\n",
      "\n",
      "    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "    [Row(d=3)]\n",
      "    \"\"\"\n",
      "    if len(left) < len(right):\n",
      "        return levenshtein(right, left)\n",
      "\n",
      "    if len(right) == 0:\n",
      "        return len(left)\n",
      "\n",
      "    previous_row = range(len(right) + 1)\n",
      "\n",
      "    for i, c1 in enumerate(left):\n",
      "        current_row = [i + 1]\n",
      "        for j, c2 in enumerate(right):\n",
      "            insertions = previous_row[j + 1] + 1\n",
      "            deletions = current_row[j] + 1\n",
      "            substitutions = previous_row[j] + (c1!= c2)\n",
      "            current_row.append(min(insertions, deletions, substitutions))\n",
      "        previous_row = current_row\n",
      "\n",
      "    return previous_row[-1]\n",
      "==========score: 1.0===============\n",
      "def recv_with_timeout(self, timeout=1):\n",
      "        \"\"\"Receive a complete ISOTP message, blocking until a message is\n",
      "        received or the specified timeout is reached.\n",
      "        If timeout is 0, then this function doesn't block and returns the\n",
      "        first frame in the receive buffer or None if there isn't any.\"\"\"\n",
      "\n",
      "        if timeout == 0:\n",
      "            return self._recv_buffer[0]\n",
      "\n",
      "        # Check if we have messages in the receive buffer\n",
      "        if len(self._recv_buffer) > 0:\n",
      "            # Return the first message in the receive buffer\n",
      "            return self._recv_buffer.popleft()\n",
      "\n",
      "        # No message in the receive buffer, try to read a new message\n",
      "        self._recv_buffer.extend(self._recv_cb())\n",
      "        while len(self._recv_buffer) > 0:\n",
      "            # Return the first message in the receive buffer\n",
      "            return self._recv_buffer.popleft()\n",
      "\n",
      "        # No message in the receive buffer and no new messages in the\n",
      "        # buffer from the network, we timed out!\n",
      "        return None\n",
      "\n",
      "    def send(self, data, timeout=1):\n",
      "        \"\"\"Send an ISOTP frame and block until the message has been sent or\n",
      "        the specified timeout is reached.\n",
      "        If timeout is 0, then this function doesn't block and returns the number\n",
      "        of bytes that were sent.\"\"\"\n",
      "\n",
      "        # Send the frame\n",
      "        self._send_cb(\n",
      "==========score: 1.0===============\n",
      "def get_search_regex(query, ignore_case=True):\n",
      "    \"\"\"Returns a compiled regex pattern to search for query letters in order.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    query : str\n",
      "        String to search in another string (in order of character occurrence).\n",
      "    ignore_case : True\n",
      "        Optional value perform a case insensitive search (True by default).\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    pattern : SRE_Pattern\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    This function adds '.*' between the query characters and compiles the\n",
      "    resulting regular expression.\n",
      "    \"\"\"\n",
      "    if ignore_case:\n",
      "        query = query.upper()\n",
      "    return re.compile(r'(.*)' + re.escape(query) + r'(.*)', re.IGNORECASE)\n",
      "==========score: 1.0===============\n",
      "def group(self):\n",
      "        \"\"\"(re-)group all logevents by the given group.\"\"\"\n",
      "        self._group = group\n",
      "        self._group_cache = {}\n",
      "        self._group_cache_hits = {}\n",
      "\n",
      "    def _group_key(self, event):\n",
      "        return (self._group, event.group)\n",
      "\n",
      "    def _group_key_hits(self, event):\n",
      "        return self._group_cache_hits.get(self._group_key(event), 0) + 1\n",
      "\n",
      "    def _group_key_hit_limit(self, event):\n",
      "        if self._group_key_hits(event) > self._group_hit_limit:\n",
      "            raise RuntimeError(\"Hit group key limit\")\n",
      "\n",
      "    def _group_key_hit_limit_reached(self, event):\n",
      "        if self._group_key_hits(event) > self._group_hit_limit_reached:\n",
      "            raise RuntimeError(\"Hit group key limit\")\n",
      "\n",
      "    def _group_key_hit_limit_reached_revert(self, event):\n",
      "        if self._group_key_hits(event) > self._group_hit_limit_reached:\n",
      "            self._group_cache_h\n",
      "==========score: 1.0===============\n",
      "def from_text_file(file_path):\n",
      "        \"\"\"Load MonsoonData objects from a text file generated by\n",
      "        MonsoonData.save_to_text_file.\n",
      "\n",
      "        Args:\n",
      "            file_path: The full path of the file load from, including the file\n",
      "                name.\n",
      "\n",
      "        Returns:\n",
      "            A list of MonsoonData objects.\n",
      "        \"\"\"\n",
      "        monsoon_data_list = []\n",
      "        with open(file_path, 'r') as f:\n",
      "            for line in f:\n",
      "                monsoon_data_list.append(MonsoonData.from_text_line(line))\n",
      "        return monsoon_data_list\n",
      "\n",
      "    @staticmethod\n",
      "    def from_text_line(line):\n",
      "        \"\"\"Load MonsoonData objects from a line of text.\n",
      "\n",
      "        Args:\n",
      "            line: A line of text.\n",
      "\n",
      "        Returns:\n",
      "            A MonsoonData object.\n",
      "        \"\"\"\n",
      "        line = line.strip()\n",
      "        if not line:\n",
      "            return None\n",
      "        if line.startswith('#'):\n",
      "            return None\n",
      "        try:\n",
      "            timestamp = float(line.split()[0])\n",
      "            voltage = float(line.split()[1])\n",
      "            current = float(line.split()[2])\n",
      "            power = float(line.split()[3])\n",
      "            return MonsoonData(timestamp, voltage, current, power)\n",
      "        except ValueError:\n",
      "            return None\n",
      "\n",
      "    def to_text_line(self):\n",
      "        \"\"\"Return a\n",
      "==========score: 1.0===============\n",
      "def set_workdir(self, workdir, chroot=False):\n",
      "        \"\"\"\n",
      "        Set the working directory. Cannot be set more than once unless chroot is True\n",
      "        \"\"\"\n",
      "        if self.chroot and chroot:\n",
      "            self.chroot = False\n",
      "        if self.chroot:\n",
      "            self.workdir = workdir\n",
      "        else:\n",
      "            self.workdir = os.path.join(self.workdir, workdir)\n",
      "\n",
      "    def set_chroot(self, chroot):\n",
      "        \"\"\"\n",
      "        Set the chroot\n",
      "        \"\"\"\n",
      "        self.chroot = chroot\n",
      "\n",
      "    def set_env(self, env):\n",
      "        \"\"\"\n",
      "        Set the environment\n",
      "        \"\"\"\n",
      "        self.env = env\n",
      "\n",
      "    def set_env_var(self, env_var, value):\n",
      "        \"\"\"\n",
      "        Set an environment variable\n",
      "        \"\"\"\n",
      "        self.env[env_var] = value\n",
      "\n",
      "    def set_user(self, user):\n",
      "        \"\"\"\n",
      "        Set the user\n",
      "        \"\"\"\n",
      "        self.user = user\n",
      "\n",
      "    def set_group(self, group):\n",
      "        \"\"\"\n",
      "        Set the group\n",
      "        \"\"\"\n",
      "        self.group = group\n",
      "\n",
      "    def set_umask(self, umask):\n",
      "        \"\"\"\n",
      "        Set the umask\n",
      "        \"\"\"\n",
      "        self.umask = um\n",
      "==========score: 1.0===============\n",
      "def cluster_kmeans(data, n_clusters, **kwargs):\n",
      "    \"\"\"\n",
      "    Identify clusters using K - Means algorithm.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    data : array_like\n",
      "        array of size [n_samples, n_features].\n",
      "    n_clusters : int\n",
      "        The number of clusters expected in the data.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    dict\n",
      "        boolean array for each identified cluster.\n",
      "    \"\"\"\n",
      "    # Initialize K - Means algorithm\n",
      "    kmeans = KMeans(n_clusters=n_clusters, **kwargs)\n",
      "    # Fit K - Means algorithm\n",
      "    kmeans.fit(data)\n",
      "    # Get cluster labels\n",
      "    labels = kmeans.labels_\n",
      "    # Return cluster labels\n",
      "    return labels\n",
      "==========score: 1.0===============\n",
      "def _kl_normal_normal(n_a, n_b, name=None):\n",
      "  \"\"\"Calculate the batched KL divergence KL(n_a || n_b) with n_a and n_b Normal.\n",
      "\n",
      "  Args:\n",
      "    n_a: instance of a Normal distribution object.\n",
      "    n_b: instance of a Normal distribution object.\n",
      "    name: (optional) Name to use for created operations.\n",
      "      default is \"kl_normal_normal\".\n",
      "\n",
      "  Returns:\n",
      "    Batchwise KL(n_a || n_b)\n",
      "  \"\"\"\n",
      "  with tf.name_scope(name, \"kl_normal_normal\", [n_a.loc, n_b.loc]):\n",
      "    one = tf.ones_like(n_a.loc)\n",
      "    two = tf.constant(2, dtype=n_a.dtype)\n",
      "    three = tf.constant(3, dtype=n_a.dtype)\n",
      "    s_a_squared_minus_mu_squared = (\n",
      "        tf.square(n_a.loc) - tf.square(n_a.loc) + tf.square(n_a.scale) -\n",
      "        tf.square(n_b.scale))\n",
      "    term1 = tf.square(n_a.scale / n_b.scale) + tf.square(n_a.loc / n_b.loc)\n",
      "    term2 = (two * tf.square(n_a.loc - n_b.loc)) / s_a_squared_minus_mu_squared\n",
      "    return tf.reshape(\n",
      "        tf.reduce_sum(\n",
      "            tf.log(two * tf\n",
      "==========score: 1.0===============\n",
      "def fromEpoch(cls, epoch_time):\r\n",
      "\r\n",
      "        ''' a method for constructing a labDT object from epoch timestamp\r\n",
      "\r\n",
      "        :param epoch_time: number with epoch timestamp info\r\n",
      "        :return: labDT object\r\n",
      "        '''\r\n",
      "\r\n",
      "        return cls(datetime.datetime.fromtimestamp(epoch_time))\r\n",
      "\r\n",
      "    def toEpoch(self):\r\n",
      "\r\n",
      "        ''' a method for constructing a epoch timestamp from a labDT object\r\n",
      "\r\n",
      "        :return: epoch timestamp\r\n",
      "        '''\r\n",
      "\r\n",
      "        return int(time.mktime(self.timetuple()))\r\n",
      "\r\n",
      "    def toEpochString(self):\r\n",
      "\r\n",
      "        ''' a method for constructing a epoch timestamp string from a labDT object\r\n",
      "\r\n",
      "        :return: epoch timestamp string\r\n",
      "        '''\r\n",
      "\r\n",
      "        return str(self.toEpoch())\r\n",
      "\r\n",
      "    def toEpochStringISO(self):\r\n",
      "\r\n",
      "        ''' a method for constructing a epoch timestamp string from a labDT object\r\n",
      "\r\n",
      "        :return: epoch timestamp string\r\n",
      "        '''\r\n",
      "\r\n",
      "        return self.toEpochString() + 'Z'\r\n",
      "\r\n",
      "    def toEpochStringISO8601(self):\r\n",
      "\r\n",
      "        ''' a method for constructing a epoch timestamp string from a labDT object\n",
      "==========score: 1.0===============\n",
      "def file_unzipper(directory):\n",
      "   \"\"\" This function will unzip all files in the runroot directory and\n",
      "   subdirectories\n",
      "   \"\"\"\n",
      "   for root, dirs, files in os.walk(directory):\n",
      "      for file in files:\n",
      "         if file.endswith(\".zip\"):\n",
      "            print(\"Unzipping file: \" + file)\n",
      "            zip_ref = zipfile.ZipFile(os.path.join(root, file), 'r')\n",
      "            zip_ref.extractall(root)\n",
      "            zip_ref.close()\n",
      "\n",
      "def file_unzipper_recursive(directory):\n",
      "   \"\"\" This function will unzip all files in the runroot directory and\n",
      "   subdirectories\n",
      "   \"\"\"\n",
      "   for root, dirs, files in os.walk(directory):\n",
      "      for file in files:\n",
      "         if file.endswith(\".zip\"):\n",
      "            print(\"Unzipping file: \" + file)\n",
      "            zip_ref = zipfile.ZipFile(os.path.join(root, file), 'r')\n",
      "            zip_ref.extractall(root)\n",
      "            zip_ref.close()\n",
      "\n",
      "\n",
      "==========score: 1.0===============\n",
      "def get_html_source(self):\n",
      "        \"\"\"Gets source page of url\n",
      "        :return: HTML source\n",
      "        \"\"\"\n",
      "        return self.driver.page_source\n",
      "\n",
      "    def get_url(self):\n",
      "        \"\"\"Gets url of current page\n",
      "        :return: URL\n",
      "        \"\"\"\n",
      "        return self.driver.current_url\n",
      "\n",
      "    def get_title(self):\n",
      "        \"\"\"Gets title of current page\n",
      "        :return: Title\n",
      "        \"\"\"\n",
      "        return self.driver.title\n",
      "\n",
      "    def get_current_url(self):\n",
      "        \"\"\"Gets current url\n",
      "        :return: URL\n",
      "        \"\"\"\n",
      "        return self.driver.current_url\n",
      "\n",
      "    def get_page_source(self):\n",
      "        \"\"\"Gets source of current page\n",
      "        :return: Page source\n",
      "        \"\"\"\n",
      "        return self.driver.page_source\n",
      "\n",
      "    def get_page_source_as_text(self):\n",
      "        \"\"\"Gets source of current page as text\n",
      "        :return: Page source as text\n",
      "        \"\"\"\n",
      "        return self.driver.page_source\n",
      "\n",
      "    def get_page_source_as_html(self):\n",
      "        \"\"\"Gets source of current page as html\n",
      "        :return: Page source as html\n",
      "        \"\"\"\n",
      "        return self.driver.page_source\n",
      "\n",
      "\n",
      "==========score: 1.0===============\n",
      "def file_unzipper(directory):\n",
      "   \"\"\" This function will unzip all files in the runroot directory and\n",
      "   subdirectories\n",
      "   \"\"\"\n",
      "   import os\n",
      "   import zipfile\n",
      "   import shutil\n",
      "   import glob\n",
      "   import re\n",
      "   import time\n",
      "   import datetime\n",
      "   import sys\n",
      "   import subprocess\n",
      "   import glob\n",
      "   import re\n",
      "   import shutil\n",
      "   import zipfile\n",
      "   import os\n",
      "   import sys\n",
      "   import subprocess\n",
      "   import glob\n",
      "   import re\n",
      "   import shutil\n",
      "   import zipfile\n",
      "   import os\n",
      "   import sys\n",
      "   import subprocess\n",
      "   import glob\n",
      "   import re\n",
      "   import shutil\n",
      "   import zipfile\n",
      "   import os\n",
      "   import sys\n",
      "   import subprocess\n",
      "   import glob\n",
      "   import re\n",
      "   import shutil\n",
      "   import zipfile\n",
      "   import os\n",
      "   import sys\n",
      "   import subprocess\n",
      "   import glob\n",
      "   import re\n",
      "   import shutil\n",
      "   import zipfile\n",
      "   import os\n",
      "   import sys\n",
      "   import subprocess\n",
      "   import glob\n",
      "   import re\n",
      "   import shutil\n",
      "   import zipfile\n",
      "   import os\n",
      "   import sys\n",
      "   import subprocess\n",
      "   import glob\n",
      "   import re\n",
      "   import shutil\n",
      "   import zipfile\n",
      "   import os\n",
      "   import sys\n",
      "   import sub\n",
      "==========score: 1.0===============\n",
      "def export_analytics_data_to_excel(data, output_file_name, result_info_key, identifier_keys):\n",
      "    \"\"\"Creates an Excel file containing data returned by the Analytics API\n",
      "\n",
      "    Args:\n",
      "        data: Analytics API data as a list of dicts\n",
      "        output_file_name: File name for output Excel file (use .xlsx extension).\n",
      "\n",
      "    \"\"\"\n",
      "    # Create a Pandas dataframe from the data\n",
      "    df = pd.DataFrame(data)\n",
      "\n",
      "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
      "    writer = pd.ExcelWriter(output_file_name, engine='xlsxwriter')\n",
      "\n",
      "    # Convert the dataframe to an XlsxWriter Excel object.\n",
      "    df.to_excel(writer, sheet_name='Sheet1')\n",
      "\n",
      "    # Get the xlsxwriter objects from the dataframe writer object.\n",
      "    workbook = writer.book\n",
      "    worksheet = writer.sheets['Sheet1']\n",
      "\n",
      "    # Add a header format and set it to bold.\n",
      "    header_format = workbook.add_format({'bold': True})\n",
      "    worksheet.set_column(0, 0, None, header_format)\n",
      "\n",
      "    # Add a header format and set it to bold.\n",
      "    header_format = workbook.add_format({'bold': True})\n",
      "    worksheet.set_column(0, 0, None, header_format)\n",
      "\n",
      "    # Add a header format and set it to bold.\n",
      "    header_\n",
      "==========score: 1.0===============\n",
      "def fuzzmatch(self, fuzzkey, multi=False):\n",
      "        \"\"\"\n",
      "        Identify a filter by fuzzy string matching.\n",
      "\n",
      "        Partial ('fuzzy') matching performed by `fuzzywuzzy.fuzzy.ratio`\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fuzzkey : str\n",
      "            A string that partially matches one filter name more than the others.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The name of the most closely matched filter. : str\n",
      "        \"\"\"\n",
      "        if not self.filters:\n",
      "            return None\n",
      "\n",
      "        if multi:\n",
      "            return self._fuzzy_match_multi(fuzzkey)\n",
      "        else:\n",
      "            return self._fuzzy_match_single(fuzzkey)\n",
      "\n",
      "    def _fuzzy_match_single(self, fuzzkey):\n",
      "        \"\"\"\n",
      "        Identify a filter by fuzzy string matching.\n",
      "\n",
      "        Partial ('fuzzy') matching performed by `fuzzywuzzy.fuzzy.ratio`\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fuzzkey : str\n",
      "            A string that partially matches one filter name more than the others.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The name of the most closely matched filter. : str\n",
      "        \"\"\"\n",
      "        if not self.filters:\n",
      "            return None\n",
      "\n",
      "        ratios = []\n",
      "        for name in self.filters:\n",
      "            ratios.append(fuzzywuzzy.fuzzy.ratio(fuzzkey, name))\n",
      "\n",
      "        return self.filters[np.argmax(ratios)]\n",
      "\n",
      "    def _fuzzy_match_multi(self, fuzzkey):\n",
      "        \"\"\"\n",
      "==========score: 1.0===============\n",
      "def plot(self, sizescale=10, color=None, alpha=0.5, label=None, edgecolor='none', **kw):\n",
      "        '''\n",
      "        Plot the ra and dec of the coordinates,\n",
      "        at a given epoch, scaled by their magnitude.\n",
      "\n",
      "        (This does *not* create a new empty figure.)\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        sizescale : (optional) float\n",
      "            The marker size for scatter for a star at the magnitudelimit.\n",
      "        color : (optional) any valid color\n",
      "            The color to plot (but there is a default for this catalog.)\n",
      "        **kw : dict\n",
      "            Additional keywords will be passed on to plt.scatter.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "\n",
      "        plotted : outputs from the plots\n",
      "        '''\n",
      "        if color is None:\n",
      "            color = self.color\n",
      "        if label is None:\n",
      "            label = self.name\n",
      "        if self.ra is None:\n",
      "            raise ValueError('ra is None')\n",
      "        if self.dec is None:\n",
      "            raise ValueError('dec is None')\n",
      "        if self.magnitude is None:\n",
      "            raise ValueError('magnitude is None')\n",
      "        if self.epoch is None:\n",
      "            raise ValueError('epoch is None')\n",
      "\n",
      "        plotted = plt.scatter(self.ra, self.dec, s=sizescale*self.magnitude, color=color, alpha=alpha, label=label, edgecolor=edgecolor, **kw)\n",
      "        return plotted\n",
      "\n",
      "    def plot_with_error(self, sizescale=10, color=None, alpha=0.5, label=None, edgecolor='none', **kw):\n",
      "        '''\n",
      "        Plot the ra and dec of the coordinates,\n",
      "        at a given epoch, scaled by their magnitude.\n",
      "\n",
      "        (This does *not* create a new empty figure.)\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "==========score: 1.0===============\n",
      "def fromEpoch(cls, epoch_time):\r\n",
      "\r\n",
      "        ''' a method for constructing a labDT object from epoch timestamp\r\n",
      "\r\n",
      "        :param epoch_time: number with epoch timestamp info\r\n",
      "        :return: labDT object\r\n",
      "        '''\r\n",
      "\r\n",
      "        return cls(datetime.datetime.fromtimestamp(epoch_time))\r\n",
      "\r\n",
      "    @classmethod\r\n",
      "    def fromDate(cls, date_time):\r\n",
      "\r\n",
      "        ''' a method for constructing a labDT object from date timestamp\r\n",
      "\r\n",
      "        :param date_time: date with date timestamp info\r\n",
      "        :return: labDT object\r\n",
      "        '''\r\n",
      "\r\n",
      "        return cls(datetime.datetime.fromtimestamp(date_time))\r\n",
      "\r\n",
      "    @classmethod\r\n",
      "    def fromTime(cls, time_stamp):\r\n",
      "\r\n",
      "        ''' a method for constructing a labDT object from time stamp\r\n",
      "\r\n",
      "        :param time_stamp: time with time stamp info\r\n",
      "        :return: labDT object\r\n",
      "        '''\r\n",
      "\r\n",
      "        return cls(datetime.datetime.fromtimestamp(time_stamp))\r\n",
      "\r\n",
      "    @classmethod\r\n",
      "    def fromDateTime(cls, date_time):\r\n",
      "\r\n",
      "        ''' a method for constructing a labDT object from date time\r\n",
      "\n",
      "==========score: 1.0===============\n",
      "def permutations(x):\n",
      "    '''Given a listlike, x, return all permutations of x\n",
      "\n",
      "    Returns the permutations of x in the lexical order of their indices:\n",
      "    e.g.\n",
      "    >>> x = [ 1, 2, 3, 4 ]\n",
      "    >>> for p in permutations(x):\n",
      "    >>>   print p\n",
      "    [ 1, 2, 3, 4 ]\n",
      "    [ 1, 2, 4, 3 ]\n",
      "    [ 1, 3, 2, 4 ]\n",
      "    [ 1, 3, 4, 2 ]\n",
      "    [ 1, 4, 2, 3 ]\n",
      "    [ 1, 4, 3, 2 ]\n",
      "    [ 2, 1, 3, 4 ]\n",
      "    ...\n",
      "    [ 4, 3, 2, 1 ]\n",
      "    '''\n",
      "    if len(x) <= 1:\n",
      "        yield x\n",
      "    else:\n",
      "        for i in range(len(x)):\n",
      "            for p in permutations(x[:i] + x[i+1:]):\n",
      "                yield [x[i]] + p\n",
      "\n",
      "def permutations_with_replacement(x):\n",
      "    '''Given a listlike, x, return all permutations of x, with replacement\n",
      "\n",
      "    Returns the permutations of x in the lexical order of their indices:\n",
      "    e.g.\n",
      "    >>> x = [ 1, 2, 3, 4 ]\n",
      "    >>> for p in permutations_with_replacement(x):\n",
      "    >>>   print p\n",
      "    [ 1, 2, 3, 4 ]\n",
      "    [ 1, 2, 3, 1 ]\n",
      "    [ 1, 2, 3, 2 ]\n",
      "    [ 1, 2, 3, 4 ]\n",
      "    [ 1, 2, 4, 3 ]\n",
      "    [ 1, 2, 4, 1 ]\n",
      "    [ 1, 3, 2, 4 ]\n",
      "   ...\n",
      "    [ 4, 3, 2, 1 ]\n",
      "    '''\n",
      "    if len(x\n",
      "==========score: 1.0===============\n",
      "def get_binary_path(executable, logging_level='INFO'):\n",
      "    \"\"\"Gets the software name and returns the path of the binary.\"\"\"\n",
      "    if executable == 'java':\n",
      "        return os.path.join(os.path.dirname(os.path.realpath(__file__)), 'java')\n",
      "    elif executable == 'python':\n",
      "        return os.path.join(os.path.dirname(os.path.realpath(__file__)), 'python')\n",
      "    else:\n",
      "        logging.error('{} is not a valid executable.'.format(executable))\n",
      "        raise Exception('{} is not a valid executable.'.format(executable))\n",
      "==========score: 1.0===============\n",
      "def sort_numpy(array, col=0, order_back=False):\r\n",
      "    \"\"\"\r\n",
      "    Sorts the columns for an entire ``ndarrray`` according to sorting\r\n",
      "    one of them.\r\n",
      "    \r\n",
      "    :param array: Array to sort.\r\n",
      "    :type array: ndarray\r\n",
      "    :param col: Master column to sort.\r\n",
      "    :type col: int\r\n",
      "    :param order_back: If True, also returns the index to undo the\r\n",
      "        new order.\r\n",
      "    :type order_back: bool\r\n",
      "    :returns: sorted_array or [sorted_array, order_back]\r\n",
      "    :rtype: ndarray, list\r\n",
      "    \"\"\"\r\n",
      "    \r\n",
      "    if order_back:\r\n",
      "        return np.sort(array, order=col), np.argsort(array, order=col)\r\n",
      "    else:\r\n",
      "        return np.sort(array, order=col)\r\n",
      "\r\n",
      "\r\n",
      "def sort_numpy_by_column(array, col=0, order_back=False):\r\n",
      "    \"\"\"\r\n",
      "    Sorts the columns for an entire ``ndarrray`` according to sorting\r\n",
      "    one of them.\r\n",
      "    \r\n",
      "    :param array: Array to sort.\r\n",
      "    :type array: ndarray\r\n",
      "    :param col: Master column to sort.\r\n",
      "    :type col: int\r\n",
      "    :param order_back: If True, also returns the index to undo the\r\n",
      "        new order.\r\n",
      "    :type order_back: bool\r\n",
      "    :returns: sorted_array or [sorted_array, order_back]\r\n",
      "    :rtype: ndarray, list\r\n",
      "    \"\"\"\r\n",
      "    \r\n",
      "    if order_back:\r\n",
      "        return np.sort(array, order=col), np.args\n",
      "==========score: 1.0===============\n",
      "def concatenate_aiml(path='aiml-en-us-foundation-alice.v1-9.zip', outfile='aiml-en-us-foundation-alice.v1-9.aiml'):\n",
      "    \"\"\"Strip trailing </aiml> tag and concatenate all valid AIML files found in the ZIP.\"\"\"\n",
      "    aiml_files = []\n",
      "    for filename in os.listdir(path):\n",
      "        if filename.endswith('.aiml'):\n",
      "            aiml_files.append(filename)\n",
      "    aiml_files.sort()\n",
      "    aiml_files.reverse()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml_files.pop()\n",
      "    aiml\n",
      "==========score: 1.0===============\n",
      "def render_pdf(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Render the PDF and returns as bytes.\n",
      "\n",
      "        :rtype: bytes\n",
      "        \"\"\"\n",
      "        options = self.get_rendering_options()\n",
      "        return self.renderer.render(self, **options)\n",
      "\n",
      "    def get_rendering_options(self):\n",
      "        \"\"\"\n",
      "        Return the rendering options to be used for rendering this report.\n",
      "\n",
      "        :rtype: dict\n",
      "        \"\"\"\n",
      "        return {\n",
      "            'landscape': self.landscape,\n",
      "            'page_size': self.page_size,\n",
      "            'encoding': self.encoding,\n",
      "            'header_footer': self.header_footer,\n",
      "            'custom_header': self.custom_header,\n",
      "            'custom_footer': self.custom_footer,\n",
      "            'with_bottom_border': self.with_bottom_border,\n",
      "            'add_page_number': self.add_page_number,\n",
      "            'page_range': self.page_range,\n",
      "            'add_custom_header': self.add_custom_header,\n",
      "            'add_custom_footer': self.add_custom_footer,\n",
      "            'paper_bg_color': self.paper_bg_color,\n",
      "            'orientation': self\n",
      "==========score: 1.0===============\n",
      "def _flush_queue(self, q, ignore_priority=False):\n",
      "        \"\"\"\n",
      "        :param q: PriorityQueue instance holding GarbageCollector entries\n",
      "        :param ignore_priority: If True - all GarbageCollector entries should be resubmitted\n",
      "                If False - only those entries whose waiting time has expired will be resubmitted\n",
      "        \"\"\"\n",
      "        while not q.empty():\n",
      "            gc = q.get()\n",
      "            if not ignore_priority or gc.priority == 0:\n",
      "                gc.submit()\n",
      "            q.task_done()\n",
      "\n",
      "    def _garbage_collector_loop(self):\n",
      "        \"\"\"\n",
      "        Main loop of the garbage collector.\n",
      "        \"\"\"\n",
      "        while True:\n",
      "            self._flush_queue(self._gc_queue, ignore_priority=True)\n",
      "            self._gc_queue.put(GarbageCollector(self._gc_queue.maxsize))\n",
      "            self._gc_queue.task_done()\n",
      "            time.sleep(self._gc_interval)\n",
      "\n",
      "    def _start_garbage_collector(self):\n",
      "        \"\"\"\n",
      "        Starts the garbage collector thread.\n",
      "        \"\"\"\n",
      "        self._gc_thread = threading.Thread(target=self._garbage_collector_loop)\n",
      "        self._gc_thread.daemon = True\n",
      "        self._gc_thread.start()\n",
      "\n",
      "    def _stop_garbage_collector(self):\n",
      "        \"\"\"\n",
      "        Stops the garbage collector thread.\n",
      "        \"\"\"\n",
      "        self\n",
      "==========score: 1.0===============\n",
      "def correct_word(word_string):\n",
      "    '''\n",
      "    Finds all valid one and two letter corrections for word_string, returning the word\n",
      "    with the highest relative probability as type str.\n",
      "    '''\n",
      "    # TODO: Implement this function.\n",
      "    # HINT: You can use the helper function correct_word_helper() to help you.\n",
      "    # HINT: You can use the helper function get_word_probability_helper() to help you.\n",
      "    # HINT: You can use the helper function get_word_probability_helper() to help you.\n",
      "    # HINT: You can use the helper function get_word_probability_helper() to help you.\n",
      "    # HINT: You can use the helper function get_word_probability_helper() to help you.\n",
      "    # HINT: You can use the helper function get_word_probability_helper() to help you.\n",
      "    # HINT: You can use the helper function get_word_probability_helper() to help you.\n",
      "    # HINT: You can use the helper function get_word_probability_helper() to help you.\n",
      "    # HINT: You can use the helper function get_word_probability_helper() to help you.\n",
      "    # HINT: You\n",
      "==========score: 1.0===============\n",
      "def runningMedian(seq, M):\n",
      "    \"\"\"\n",
      "     Purpose: Find the median for the points in a sliding window (odd number in size)\n",
      "              as it is moved from left to right by one point at a time.\n",
      "      Inputs:\n",
      "            seq -- list containing items for which a running median (in a sliding window)\n",
      "                   is to be calculated\n",
      "              M -- number of items in window (window size) -- must be an integer > 1\n",
      "      Otputs:\n",
      "         medians -- list of medians with size N - M + 1\n",
      "       Note:\n",
      "         1. The median of a finite list of numbers is the \"center\" value when this list\n",
      "            is sorted in ascending order.\n",
      "         2. If M is an even number the two elements in the window that\n",
      "            are close to the center are averaged to give the median (this\n",
      "            is not by definition)\n",
      "    \"\"\"\n",
      "    # Check if M is an even number\n",
      "    if M % 2 == 0:\n",
      "        print(\"Error: M must be an odd number\")\n",
      "        return\n",
      "    # Check if M is an integer\n",
      "    if not isinstance(M, int):\n",
      "        print(\"Error: M must be an integer\")\n",
      "        return\n",
      "    # Check if seq is a list\n",
      "    if not isinstance(seq, list):\n",
      "        print(\"Error: seq must be a list\")\n",
      "        return\n",
      "    # Check if seq is not empty\n",
      "    if len(seq) == 0:\n",
      "        print(\"Error: seq must not be empty\")\n",
      "        return\n",
      "    # Check if M is not negative\n",
      "    if M < 0:\n",
      "        print(\"Error: M must be a positive integer\")\n",
      "        return\n",
      "    # Check if M is greater than the length of seq\n",
      "    if M > len(seq):\n",
      "        print(\"Error: M must be less than the length of seq\")\n",
      "        return\n",
      "    # Check if M is greater than 0\n",
      "    if M <= 0:\n",
      "        print(\"Error: M must be greater than 0\")\n",
      "        return\n",
      "    # Check if seq is not empty\n",
      "    \n",
      "==========score: 1.0===============\n",
      "def push(self, item, priority=None):\n",
      "        \"\"\"Push the item in the priority queue.\n",
      "        if priority is not given, priority is set to the value of item.\n",
      "        \"\"\"\n",
      "        if priority is None:\n",
      "            priority = item\n",
      "        self.heap.append((priority, item))\n",
      "        self._siftup(len(self.heap) - 1)\n",
      "\n",
      "    def pop(self):\n",
      "        \"\"\"Pop and return the item at the top of the priority queue.\n",
      "        \"\"\"\n",
      "        return self._pop()\n",
      "\n",
      "    def top(self):\n",
      "        \"\"\"Return the item at the top of the priority queue without removing it.\n",
      "        \"\"\"\n",
      "        return self._top()\n",
      "\n",
      "    def is_empty(self):\n",
      "        \"\"\"Return True if the priority queue is empty.\n",
      "        \"\"\"\n",
      "        return len(self.heap) == 0\n",
      "\n",
      "    def _parent(self, j):\n",
      "        return (j - 1) // 2\n",
      "\n",
      "    def _left(self, j):\n",
      "        return 2 * j + 1\n",
      "\n",
      "    def _right(self, j):\n",
      "        return 2 * j + 2\n",
      "\n",
      "    def _siftup(self, j):\n",
      "        if j > 0:\n",
      "            if self.heap[j][0] < self.heap[self._parent(j)][0]:\n",
      "                self._swap(j\n",
      "==========score: 1.0===============\n",
      "def get_process_info(self, pid=None):\n",
      "        '''\n",
      "        get_process_info(self, pid=None)\n",
      "\n",
      "        Get process general information.\n",
      "\n",
      "        :Parameters:\n",
      "        * *pid* (`string`) -- Identifier of an existing process\n",
      "\n",
      "        '''\n",
      "        if pid is None:\n",
      "            pid = self.pid\n",
      "        return self.execute_command('get_process_info', pid)\n",
      "\n",
      "    def get_process_memory_info(self, pid=None):\n",
      "        '''\n",
      "        get_process_memory_info(self, pid=None)\n",
      "\n",
      "        Get process memory information.\n",
      "\n",
      "        :Parameters:\n",
      "        * *pid* (`string`) -- Identifier of an existing process\n",
      "\n",
      "        '''\n",
      "        if pid is None:\n",
      "            pid = self.pid\n",
      "        return self.execute_command('get_process_memory_info', pid)\n",
      "\n",
      "    def get_process_environment(self, pid=None):\n",
      "        '''\n",
      "        get_process_environment(self, pid=None)\n",
      "\n",
      "        Get process environment variables.\n",
      "\n",
      "        :Parameters:\n",
      "        * *pid* (`string`) -- Identifier of an existing process\n",
      "\n",
      "        '''\n",
      "        if pid is None:\n",
      "            pid = self.pid\n",
      "        return self.execute_command('get_process_environment', pid)\n",
      "\n",
      "    def get_process_command_line(self, pid=None):\n",
      "        '''\n",
      "        \n",
      "==========score: 1.0===============\n",
      "def encrypt(self, key):\n",
      "        \"\"\"This method encrypts and signs the state to make it unreadable by\n",
      "        the server, since it contains information that would allow faking\n",
      "        proof of storage.\n",
      "\n",
      "        :param key: the key to encrypt and sign with\n",
      "        \"\"\"\n",
      "        self.state = self.state.encrypt(key)\n",
      "        self.signature = self.signature.encrypt(key)\n",
      "\n",
      "    def decrypt(self, key):\n",
      "        \"\"\"This method decrypts and verifies the state to make it readable\n",
      "        by the server, since it contains information that would allow\n",
      "        faking proof of storage.\n",
      "\n",
      "        :param key: the key to decrypt and verify with\n",
      "        \"\"\"\n",
      "        self.state = self.state.decrypt(key)\n",
      "        self.signature = self.signature.decrypt(key)\n",
      "        self.verify(key)\n",
      "\n",
      "    def verify(self, key):\n",
      "        \"\"\"This method verifies the signature of the state to make it\n",
      "        readable by the server, since it contains information that would\n",
      "        allow faking proof of storage.\n",
      "\n",
      "        :param key: the key to verify with\n",
      "        \"\"\"\n",
      "        self.state = self.state.verify(key)\n",
      "        self.signature = self.signature.verify(key)\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        \"\"\"This method checks if two states are equal.\n",
      "\n",
      "        :param other:\n",
      "==========score: 1.0===============\n",
      "def levenshtein(left, right):\n",
      "    \"\"\"Computes the Levenshtein distance of the two given strings.\n",
      "\n",
      "    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "    [Row(d=3)]\n",
      "    \"\"\"\n",
      "    return _compute(F.levenshtein, left, right)\n",
      "==========score: 1.0===============\n",
      "def linear_regression(self):\n",
      "        \"\"\" Linear Regression.\n",
      "\n",
      "        This function runs linear regression and stores the, \n",
      "        1. Model\n",
      "        2. Model name \n",
      "        3. Mean score of cross validation\n",
      "        4. Metrics\n",
      "\n",
      "        \"\"\"\n",
      "        self.model = LinearRegression()\n",
      "        self.model_name = \"Linear Regression\"\n",
      "        self.mean_score = self.cross_validation(self.model)\n",
      "        self.metrics = self.metrics_calculation(self.model, self.mean_score)\n",
      "\n",
      "    def decision_tree(self):\n",
      "        \"\"\" Decision Tree.\n",
      "\n",
      "        This function runs decision tree and stores the, \n",
      "        1. Model\n",
      "        2. Model name \n",
      "        3. Mean score of cross validation\n",
      "        4. Metrics\n",
      "\n",
      "        \"\"\"\n",
      "        self.model = DecisionTreeClassifier()\n",
      "        self.model_name = \"Decision Tree\"\n",
      "        self.mean_score = self.cross_validation(self.model)\n",
      "        self.metrics = self.metrics_calculation(self.model, self.mean_score)\n",
      "\n",
      "    def random_forest(self):\n",
      "        \"\"\" Random Forest.\n",
      "\n",
      "        This function runs random forest and stores the, \n",
      "        1. Model\n",
      "        2. Model name \n",
      "        3. Mean score of cross validation\n",
      "        4. Metrics\n",
      "\n",
      "        \"\"\"\n",
      "        self.model =\n",
      "==========score: 1.0===============\n",
      "def __get_current_datetime(self):\n",
      "        \"\"\"Get current datetime for every file.\"\"\"\n",
      "        return datetime.now()\n",
      "\n",
      "    def __get_current_date(self):\n",
      "        \"\"\"Get current date for every file.\"\"\"\n",
      "        return self.__get_current_datetime().date()\n",
      "\n",
      "    def __get_current_time(self):\n",
      "        \"\"\"Get current time for every file.\"\"\"\n",
      "        return self.__get_current_datetime().time()\n",
      "\n",
      "    def __get_current_datetime_string(self):\n",
      "        \"\"\"Get current datetime in string format.\"\"\"\n",
      "        return self.__get_current_datetime().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
      "\n",
      "    def __get_current_date_string(self):\n",
      "        \"\"\"Get current date in string format.\"\"\"\n",
      "        return self.__get_current_date().strftime(\"%Y-%m-%d\")\n",
      "\n",
      "    def __get_current_time_string(self):\n",
      "        \"\"\"Get current time in string format.\"\"\"\n",
      "        return self.__get_current_time().strftime(\"%H:%M:%S\")\n",
      "\n",
      "    def __get_current_datetime_string_with\n",
      "==========score: 1.0===============\n",
      "def memoize(cache=None):\n",
      "    \"\"\"\n",
      "    | Implements method / definition memoization.\n",
      "    | Any method / definition decorated will get its return value cached and restored whenever called with the same arguments.\n",
      "\n",
      "    :param cache: Alternate cache.\n",
      "    :type cache: dict\n",
      "    :return: Object.\n",
      "    :rtype: object\n",
      "    \"\"\"\n",
      "\n",
      "    def memoize_decorator(func):\n",
      "        if cache is None:\n",
      "            cache = {}\n",
      "\n",
      "        def memoized_func(*args, **kwargs):\n",
      "            if args in cache:\n",
      "                return cache[args]\n",
      "            else:\n",
      "                value = func(*args, **kwargs)\n",
      "                cache[args] = value\n",
      "                return value\n",
      "\n",
      "        return memoized_func\n",
      "\n",
      "    return memoize_decorator\n",
      "==========score: 1.0===============\n",
      "def get(self):\n",
      "        \"\"\"\n",
      "        Return the HTTP code status.\n",
      "\n",
      "        :return: The matched and formatted status code.\n",
      "        :rtype: str|int|None\n",
      "        \"\"\"\n",
      "        return self.__code\n",
      "\n",
      "    def get_status(self):\n",
      "        \"\"\"\n",
      "        Return the HTTP code status.\n",
      "\n",
      "        :return: The matched and formatted status code.\n",
      "        :rtype: str|int|None\n",
      "        \"\"\"\n",
      "        return self.__code\n",
      "\n",
      "    def get_status_code(self):\n",
      "        \"\"\"\n",
      "        Return the HTTP code status.\n",
      "\n",
      "        :return: The matched and formatted status code.\n",
      "        :rtype: str|int|None\n",
      "        \"\"\"\n",
      "        return self.__code\n",
      "\n",
      "    def get_status_code_int(self):\n",
      "        \"\"\"\n",
      "        Return the HTTP code status.\n",
      "\n",
      "        :return: The matched and formatted status code.\n",
      "        :rtype: int|None\n",
      "        \"\"\"\n",
      "        return self.__code\n",
      "\n",
      "    def get_status_int(self):\n",
      "        \"\"\"\n",
      "        Return the HTTP code status.\n",
      "\n",
      "        :return: The matched and formatted status code.\n",
      "        :rtype: int|None\n",
      "        \"\"\"\n",
      "        return self.__code\n",
      "\n",
      "    def get_status_int_or_none(self):\n",
      "        \"\"\"\n",
      "        Return the HTTP code status.\n",
      "\n",
      "        :return: The matched\n",
      "==========score: 1.0===============\n",
      "def get_http_status_string(v):\n",
      "    \"\"\"Return HTTP response string, e.g. 204 -> ('204 No Content').\n",
      "    The return string always includes descriptive text, to satisfy Apache mod_dav.\n",
      "\n",
      "    `v`: status code or DAVError\n",
      "    \"\"\"\n",
      "    if isinstance(v, DAVError):\n",
      "        return v.get_status_string()\n",
      "    else:\n",
      "        return \"%d %s\" % (v, HTTP_CODES[v])\n",
      "==========score: 1.0===============\n",
      "def image_from(from_value):\n",
      "    \"\"\"\n",
      "    :param from_value: string like \"image:tag\" or \"image:tag AS name\"\n",
      "    :return: tuple of the image and stage name, e.g. (\"image:tag\", None)\n",
      "    \"\"\"\n",
      "    from_value = from_value.strip()\n",
      "    if from_value.startswith(\"image:\"):\n",
      "        return from_value[len(\"image:\"):], None\n",
      "    elif from_value.startswith(\"image:\"):\n",
      "        return from_value[len(\"image:\"):], from_value[len(\"image:\"):].strip()\n",
      "    else:\n",
      "        raise ValueError(\"Invalid image reference: %s\" % from_value)\n",
      "==========score: 0.9999===============\n",
      "def csv_writer(molecules, options, prefix):\n",
      "    \"\"\"\n",
      "\tWrite a csv file.\n",
      "\t\"\"\"\n",
      "    if options.csv_file == None:\n",
      "        return\n",
      "    with open(options.csv_file, 'w') as f:\n",
      "        f.write('#' + prefix + '\\n')\n",
      "        f.write('#' + prefix + '_molecule_id,'+ prefix + '_atom_id,'+ prefix + '_atom_type,'+ prefix + '_atom_charge,'+ prefix + '_atom_x,'+ prefix + '_atom_y,'+ prefix + '_atom_z,'+ prefix + '_atom_segment,'+ prefix + '_atom_segment_id,'+ prefix + '_atom_residue_id,'+ prefix + '_atom_residue_name,'+ prefix + '_atom_residue_number,'+ prefix + '_atom_residue_insertion,'+ prefix + '_atom_chain_id,'+ prefix + '_atom_chain_name,'+ prefix + '_atom_chain_number,'+ prefix + '_atom_chain_insertion, '\n",
      "==========score: 1.0===============\n",
      "def reverse(self):\n",
      "        \"\"\"\n",
      "        Reverses the items of this collection \"in place\" (only two values are\n",
      "        retrieved from Redis at a time).\n",
      "        \"\"\"\n",
      "        self.client.transaction(self.write_commands, self.read_commands)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return \"%s(%r)\" % (self.__class__.__name__, list(self))\n",
      "\n",
      "==========score: 1.0===============\n",
      "def _unzip_file(self, src_path, dest_path, filename):\n",
      "        \"\"\"unzips file located at src_path into destination_path\"\"\"\n",
      "        with zipfile.ZipFile(src_path, 'r') as zip_ref:\n",
      "            zip_ref.extractall(dest_path)\n",
      "            zip_ref.close()\n",
      "\n",
      "    def _get_file_name(self, file_path):\n",
      "        \"\"\"returns file name from file path\"\"\"\n",
      "        return os.path.basename(file_path)\n",
      "\n",
      "    def _get_file_path(self, file_name):\n",
      "        \"\"\"returns file path from file name\"\"\"\n",
      "        return os.path.join(self.file_path, file_name)\n",
      "\n",
      "    def _get_file_size(self, file_path):\n",
      "        \"\"\"returns file size in bytes\"\"\"\n",
      "        return os.path.getsize(file_path)\n",
      "\n",
      "    def _get_file_modified_time(self, file_path):\n",
      "        \"\"\"returns file modified time in seconds\"\"\"\n",
      "        return os.path.getmtime(file_path)\n",
      "\n",
      "    def _get_file_created_time(self, file_path):\n",
      "        \"\"\"returns file created time in seconds\"\"\"\n",
      "        return os.path.getctime(file_\n",
      "==========score: 1.0===============\n",
      "def deserialize(self):\n",
      "        \"\"\" Invoke the RFC 7159 spec compliant parser\n",
      "\n",
      "        :return:\n",
      "            the parsed & vetted request body\n",
      "        \"\"\"\n",
      "        try:\n",
      "            return self.parser.parse(self.body)\n",
      "        except Exception as e:\n",
      "            raise HTTPBadRequest(body=self.body,\n",
      "                                 message=\"Could not parse request body: %s\" % e)\n",
      "\n",
      "    def serialize(self):\n",
      "        \"\"\" Invoke the RFC 7159 spec compliant serializer\n",
      "\n",
      "        :return:\n",
      "            the serialized & encoded request body\n",
      "        \"\"\"\n",
      "        try:\n",
      "            return self.serializer.serialize(self.body)\n",
      "        except Exception as e:\n",
      "            raise HTTPBadRequest(body=self.body,\n",
      "                                 message=\"Could not serialize request body: %s\" % e)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return '<%s(%r)>' % (self.__class__.__name__, self.body)\n",
      "==========score: 1.0===============\n",
      "def pdf_from_post(self):\n",
      "        \"\"\"Returns a pdf stream with the stickers\n",
      "        \"\"\"\n",
      "        pdf = FPDF(unit=\"pt\", format=[self.width, self.height])\n",
      "        pdf.add_page()\n",
      "        pdf.set_font(\"Arial\", size=12)\n",
      "        for i in range(len(self.stickers)):\n",
      "            pdf.image(self.stickers[i], x=0, y=0, w=self.width, h=self.height)\n",
      "            pdf.ln(self.height)\n",
      "        pdf.output(self.pdf_file, \"F\")\n",
      "\n",
      "    def pdf_from_sticker(self, sticker):\n",
      "        \"\"\"Returns a pdf stream with the sticker\n",
      "        \"\"\"\n",
      "        pdf = FPDF(unit=\"pt\", format=[self.width, self.height])\n",
      "        pdf.add_page()\n",
      "        pdf.set_font(\"Arial\", size=12)\n",
      "        pdf.image(sticker, x=0, y=0, w=self.width, h=self.height)\n",
      "        pdf.ln(self.height)\n",
      "        pdf.output(self.pdf_file, \"F\")\n",
      "\n",
      "    def pdf_from_stickers(self, stickers):\n",
      "==========score: 1.0===============\n",
      "def to_uint8(self):\n",
      "        \"\"\"\n",
      "        Convert this heatmaps object to a 0-to-255 array.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        arr_uint8 : (H,W,C) ndarray\n",
      "            Heatmap as a 0-to-255 array (dtype is uint8).\n",
      "\n",
      "        \"\"\"\n",
      "        return self.__arr.astype(np.uint8)\n",
      "\n",
      "    def to_float(self):\n",
      "        \"\"\"\n",
      "        Convert this heatmaps object to a 0.0-to-1.0 array.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        arr_float : (H,W,C) ndarray\n",
      "            Heatmap as a 0.0-to-1.0 array (dtype is float).\n",
      "\n",
      "        \"\"\"\n",
      "        return self.__arr / self.__arr.max()\n",
      "\n",
      "    def to_abs(self):\n",
      "        \"\"\"\n",
      "        Convert this heatmaps object to an absolute value array.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        arr_abs : (H,W,C) ndarray\n",
      "            Heatmap as an absolute value array (dtype is float).\n",
      "\n",
      "        \"\"\"\n",
      "        return np.abs(self.__arr)\n",
      "\n",
      "    def to_rel(self):\n",
      "        \"\"\"\n",
      "        Convert this heatmaps object to a 0.0-to-1.0 array.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        arr_rel : (H,W,C) ndarray\n",
      "            Heatmap as a 0.0-to-\n",
      "==========score: 1.0===============\n",
      "def _hexvalue_to_rgb(hexvalue):\n",
      "        \"\"\"\n",
      "        Converts the hexvalue used by tuya for colour representation into\n",
      "        an RGB value.\n",
      "        \n",
      "        Args:\n",
      "            hexvalue(string): The hex representation generated by BulbDevice._rgb_to_hexvalue()\n",
      "        \"\"\"\n",
      "        hexvalue = hexvalue.lstrip('#')\n",
      "        if len(hexvalue) == 3:\n",
      "            hexvalue = ''.join(['0', hexvalue])\n",
      "        rgb = tuple(int(hexvalue[i:i+2], 16) for i in (0, 2, 4))\n",
      "        return rgb\n",
      "\n",
      "    def _rgb_to_hexvalue(self, rgb):\n",
      "        \"\"\"\n",
      "        Converts an RGB value into a hex representation used by tuya for\n",
      "        colour representation.\n",
      "        \n",
      "        Args:\n",
      "            rgb(tuple): The RGB value to convert\n",
      "        \"\"\"\n",
      "        hexvalue = ''.join(['%02x' % i for i in rgb])\n",
      "        return '#' + hexvalue\n",
      "\n",
      "    def _rgb_to_hsl(self, rgb):\n",
      "        \"\"\"\n",
      "        Converts an RGB value into an HSL representation used by tuya for\n",
      "        colour representation.\n",
      "        \n",
      "        Args:\n",
      "            rgb(tuple): The RGB value to convert\n",
      "        \"\"\"\n",
      "        rgb = rgb[0] / 255.0, rgb[1] / 255.0, rgb[2] /\n",
      "==========score: 1.0===============\n",
      "def getPathOfExecutable(executable):\n",
      "    \"\"\"\n",
      "    Returns the full path of the executable, or None if the executable\n",
      "    can not be found.\n",
      "    \"\"\"\n",
      "    for path in os.environ[\"PATH\"].split(os.pathsep):\n",
      "        path = path.strip('\"')\n",
      "        exe_file = os.path.join(path, executable)\n",
      "        if os.path.isfile(exe_file) and os.access(exe_file, os.X_OK):\n",
      "            return exe_file\n",
      "    return None\n",
      "==========score: 1.0===============\n",
      "def levenshtein(left, right):\n",
      "    \"\"\"Computes the Levenshtein distance of the two given strings.\n",
      "\n",
      "    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "    [Row(d=3)]\n",
      "    \"\"\"\n",
      "    if len(left) < len(right):\n",
      "        return levenshtein(right, left)\n",
      "\n",
      "    if len(right) == 0:\n",
      "        return len(left)\n",
      "\n",
      "    previous_row = range(len(right) + 1)\n",
      "    for i, c1 in enumerate(left):\n",
      "        current_row = [i + 1]\n",
      "        for j, c2 in enumerate(right):\n",
      "            insertions = previous_row[j + 1] + 1\n",
      "            deletions = current_row[j] + 1\n",
      "            substitutions = previous_row[j] + (c1!= c2)\n",
      "            current_row.append(min(insertions, deletions, substitutions))\n",
      "        previous_row = current_row\n",
      "\n",
      "    return previous_row[-1]\n",
      "==========score: 1.0===============\n",
      "def sort(self, ids):\n",
      "        \"\"\"\n",
      "        Sort the given list of identifiers,\n",
      "        returning a new (sorted) list.\n",
      "\n",
      "        :param list ids: the list of identifiers to be sorted\n",
      "        :rtype: list\n",
      "        \"\"\"\n",
      "        if not ids:\n",
      "            return []\n",
      "\n",
      "        ids.sort()\n",
      "        return ids\n",
      "\n",
      "    def get_last_modified(self, uri):\n",
      "        \"\"\"\n",
      "        Return the last modified date of the given resource.\n",
      "\n",
      "        :param str uri: the URI of the resource\n",
      "        :return: the last modified date as a datetime object\n",
      "        \"\"\"\n",
      "        self._check_access(uri,'read')\n",
      "        return self.__get_resource_last_modified(uri)\n",
      "\n",
      "    def get_resource_last_modified(self, uri):\n",
      "        \"\"\"\n",
      "        Return the last modified date of the given resource.\n",
      "\n",
      "        :param str uri: the URI of the resource\n",
      "        :return: the last modified date as a datetime object\n",
      "        \"\"\"\n",
      "        self._check_access(uri,'read')\n",
      "        return self.__get_resource_last_modified(uri)\n",
      "\n",
      "    def get_resource_mtime(self, uri):\n",
      "        \"\"\"\n",
      "        Return the last modified date of the given resource.\n",
      "\n",
      "        :param str uri: the URI of the resource\n",
      "        :return: the last modified date as a datetime\n",
      "==========score: 1.0===============\n",
      "def fromHTML(html, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Creates abstraction using HTML\n",
      "\n",
      "        :param str html: HTML\n",
      "        :return: TreeOfContents object\n",
      "        \"\"\"\n",
      "        return TreeOfContents.fromHTML(html, *args, **kwargs)\n",
      "\n",
      "    def toHTML(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Creates HTML representation of the tree\n",
      "\n",
      "        :return: HTML\n",
      "        \"\"\"\n",
      "        return self.__html__()\n",
      "\n",
      "    def __html__(self):\n",
      "        \"\"\"\n",
      "        Creates HTML representation of the tree\n",
      "\n",
      "        :return: HTML\n",
      "        \"\"\"\n",
      "        return self.__html__.__func__(self)\n",
      "\n",
      "    def __str__(self):\n",
      "        \"\"\"\n",
      "        Creates string representation of the tree\n",
      "\n",
      "        :return: str\n",
      "        \"\"\"\n",
      "        return self.__str__.__func__(self)\n",
      "\n",
      "    def __str__(self):\n",
      "        \"\"\"\n",
      "        Creates string representation of the tree\n",
      "\n",
      "        :return: str\n",
      "        \"\"\"\n",
      "        return self.__str__.__func__(self)\n",
      "\n",
      "    def __repr__(self):\n",
      "        \"\"\"\n",
      "        Creates string representation of the tree\n",
      "\n",
      "        :return: str\n",
      "        \"\"\"\n",
      "        return self.__repr__.__func__(self)\n",
      "\n",
      "    def __repr__(self):\n",
      "==========score: 1.0===============\n",
      "def cluster_mini_batch_kmeans(data=None, k=100, max_iter=10, batch_size=0.2, metric='euclidean',\n",
      "                              init_strategy='kmeans++', n_jobs=None, chunksize=None, skip=0, clustercenters=None, **kwargs):\n",
      "    r\"\"\"k-means clustering with mini-batch strategy\n",
      "\n",
      "    Mini-batch k-means is an approximation to k-means which picks a randomly\n",
      "    selected subset of data points to be updated in each iteration. Usually\n",
      "    much faster than k-means but will likely deliver a less optimal result.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    kmeans_mini : a :class:`MiniBatchKmeansClustering <pyemma.coordinates.clustering.MiniBatchKmeansClustering>` clustering object\n",
      "        Object for mini-batch kmeans clustering.\n",
      "        It holds discrete trajectories and cluster center information.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    :func:`kmeans <pyemma.coordinates.kmeans>` : for full k-means clustering\n",
      "\n",
      "\n",
      "    .. autoclass:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n",
      "        :members:\n",
      "        :undoc-members:\n",
      "\n",
      "        .. rubric:: Methods\n",
      "\n",
      "        .. autoautosummary:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n",
      "           :methods:\n",
      "\n",
      "        .. rubric:: Attributes\n",
      "\n",
      "        .. autoautosummary:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n",
      "            :attributes:\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n",
      "\n",
      "    \"\"\"\n",
      "    if data is None:\n",
      "        raise ValueError('data must be provided')\n",
      "\n",
      "    if k is None:\n",
      "        raise ValueError('k must be provided')\n",
      "\n",
      "    if max_iter is None:\n",
      "        raise ValueError('max_iter must be provided')\n",
      "\n",
      "    if batch_size is None:\n",
      "        raise ValueError('batch_size must be provided')\n",
      "\n",
      "    if metric is None:\n",
      "        raise ValueError('metric must be provided')\n",
      "\n",
      "    if init_strategy is None:\n",
      "        raise ValueError('init_strategy must be provided')\n",
      "\n",
      "    if n_jobs is None:\n",
      "        raise ValueError('n_jobs must be provided')\n",
      "\n",
      "    if chunksize is None:\n",
      "        raise ValueError('chunksize must be provided')\n",
      "\n",
      "    if skip is None:\n",
      "        raise ValueError('skip must be provided')\n",
      "\n",
      "    if clustercenters is None:\n",
      "        raise ValueError('clustercenters must be provided')\n",
      "\n",
      "    if kwargs is None:\n",
      "        kwargs = {}\n",
      "\n",
      "    # create the clustering object\n",
      "    kmeans_mini = MiniBatchKmeansClustering(data, k, max_iter,\n",
      "==========score: 1.0===============\n",
      "def fromEpoch(cls, epoch_time):\r\n",
      "\r\n",
      "        ''' a method for constructing a labDT object from epoch timestamp\r\n",
      "\r\n",
      "        :param epoch_time: number with epoch timestamp info\r\n",
      "        :return: labDT object\r\n",
      "        '''\r\n",
      "\r\n",
      "        return cls(epoch_time)\r\n",
      "\r\n",
      "    def toEpoch(self):\r\n",
      "\r\n",
      "        ''' a method for constructing a epoch timestamp from labDT object\r\n",
      "\r\n",
      "        :return: epoch timestamp\r\n",
      "        '''\r\n",
      "\r\n",
      "        return self.epoch_time\r\n",
      "\r\n",
      "    def toDate(self):\r\n",
      "\r\n",
      "        ''' a method for constructing a date from labDT object\r\n",
      "\r\n",
      "        :return: date\r\n",
      "        '''\r\n",
      "\r\n",
      "        return self.date\r\n",
      "\r\n",
      "    def toTime(self):\r\n",
      "\r\n",
      "        ''' a method for constructing a time from labDT object\r\n",
      "\r\n",
      "        :return: time\r\n",
      "        '''\r\n",
      "\r\n",
      "        return self.time\r\n",
      "\r\n",
      "    def toTimeStamp(self):\r\n",
      "\r\n",
      "        ''' a method for constructing a time stamp from labDT object\r\n",
      "\r\n",
      "        :return: time stamp\r\n",
      "        '''\r\n",
      "\r\n",
      "        return self.time_stamp\r\n",
      "\r\n",
      "    def toDateTime(self):\r\n",
      "\r\n",
      "        ''' a method for constructing a dat\n",
      "==========score: 1.0===============\n",
      "def cluster_mini_batch_kmeans(data=None, k=100, max_iter=10, batch_size=0.2, metric='euclidean',\n",
      "                              init_strategy='kmeans++', n_jobs=None, chunksize=None, skip=0, clustercenters=None, **kwargs):\n",
      "    r\"\"\"k-means clustering with mini-batch strategy\n",
      "\n",
      "    Mini-batch k-means is an approximation to k-means which picks a randomly\n",
      "    selected subset of data points to be updated in each iteration. Usually\n",
      "    much faster than k-means but will likely deliver a less optimal result.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    kmeans_mini : a :class:`MiniBatchKmeansClustering <pyemma.coordinates.clustering.MiniBatchKmeansClustering>` clustering object\n",
      "        Object for mini-batch kmeans clustering.\n",
      "        It holds discrete trajectories and cluster center information.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    :func:`kmeans <pyemma.coordinates.kmeans>` : for full k-means clustering\n",
      "\n",
      "\n",
      "    .. autoclass:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n",
      "        :members:\n",
      "        :undoc-members:\n",
      "\n",
      "        .. rubric:: Methods\n",
      "\n",
      "        .. autoautosummary:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n",
      "           :methods:\n",
      "\n",
      "        .. rubric:: Attributes\n",
      "\n",
      "        .. autoautosummary:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n",
      "            :attributes:\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n",
      "\n",
      "    \"\"\"\n",
      "    if data is None:\n",
      "        raise ValueError('data must be provided')\n",
      "\n",
      "    if k is None:\n",
      "        raise ValueError('k must be provided')\n",
      "\n",
      "    if max_iter is None:\n",
      "        raise ValueError('max_iter must be provided')\n",
      "\n",
      "    if batch_size is None:\n",
      "        raise ValueError('batch_size must be provided')\n",
      "\n",
      "    if metric is None:\n",
      "        raise ValueError('metric must be provided')\n",
      "\n",
      "    if init_strategy is None:\n",
      "        raise ValueError('init_strategy must be provided')\n",
      "\n",
      "    if n_jobs is None:\n",
      "        raise ValueError('n_jobs must be provided')\n",
      "\n",
      "    if chunksize is None:\n",
      "        raise ValueError('chunksize must be provided')\n",
      "\n",
      "    if skip is None:\n",
      "        raise ValueError('skip must be provided')\n",
      "\n",
      "    if clustercenters is None:\n",
      "        raise ValueError('clustercenters must be provided')\n",
      "\n",
      "    if kwargs is None:\n",
      "        kwargs = {}\n",
      "\n",
      "    if not isinstance(data, np.ndarray):\n",
      "        data = np.asarray(data)\n",
      "\n",
      "    if not isinstance(cluster\n",
      "==========score: 1.0===============\n",
      "def correct_word(word_string):\n",
      "    '''\n",
      "    Finds all valid one and two letter corrections for word_string, returning the word\n",
      "    with the highest relative probability as type str.\n",
      "    '''\n",
      "    # TODO: Implement this function.\n",
      "    # HINT: You will need to use the helper functions you implemented above.\n",
      "    # HINT: You will need to use the probability_word_string function to find the\n",
      "    # probability of word_string.\n",
      "    # HINT: You will need to use the helper functions you implemented above.\n",
      "    # HINT: You will need to use the probability_word_string function to find the\n",
      "    # probability of word_string.\n",
      "    # HINT: You will need to use the helper functions you implemented above.\n",
      "    # HINT: You will need to use the probability_word_string function to find the\n",
      "    # probability of word_string.\n",
      "    # HINT: You will need to use the helper functions you implemented above.\n",
      "    # HINT: You will need to use the probability_word_string function to find the\n",
      "    # probability of word_string.\n",
      "    # HINT: You will need to use the helper functions you implemented above.\n",
      "    # HINT: You will need to use the probability_word_string function to find the\n",
      "    # probability of word_string.\n",
      "    # HINT:\n",
      "==========score: 1.0===============\n",
      "def from_header(self, binary):\n",
      "        \"\"\"Generate a SpanContext object using the trace context header.\n",
      "        The value of enabled parsed from header is int. Need to convert to\n",
      "        bool.\n",
      "\n",
      "        :type binary: bytes\n",
      "        :param binary: Trace context header which was extracted from the\n",
      "                       request headers.\n",
      "\n",
      "        :rtype: :class:`~opencensus.trace.span_context.SpanContext`\n",
      "        :returns: SpanContext generated from the trace context header.\n",
      "        \"\"\"\n",
      "        trace_id, span_id = unpack_tracestate(binary)\n",
      "        return SpanContext(trace_id=trace_id, span_id=span_id,\n",
      "                           is_remote=False)\n",
      "\n",
      "    def to_header(self):\n",
      "        \"\"\"Generate the trace context header.\n",
      "\n",
      "        :rtype: bytes\n",
      "        :returns: The trace context header.\n",
      "        \"\"\"\n",
      "        return pack_tracestate(self.trace_id, self.span_id)\n",
      "\n",
      "    def to_binary(self):\n",
      "        \"\"\"Generate the binary representation of the span context.\n",
      "\n",
      "        :rtype: bytes\n",
      "        :returns: The binary representation of the span context.\n",
      "        \"\"\"\n",
      "        return self.to_header()\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        if not isinstance(other, SpanContext):\n",
      "            return False\n",
      "        return (self.trace_id == other.trace_id and\n",
      "                self.span_id == other.span_id and\n",
      "                self.is_remote == other.is_remote)\n",
      "\n",
      "    def __ne__(self, other):\n",
      "        return not self.__eq__\n",
      "==========score: 1.0===============\n",
      "def post(self, json=None):\n",
      "        \"\"\"Send a POST request and return the JSON decoded result.\n",
      "\n",
      "        Args:\n",
      "            json (dict, optional): Object to encode and send in request.\n",
      "\n",
      "        Returns:\n",
      "            mixed: JSON decoded response data.\n",
      "        \"\"\"\n",
      "        return self._request('POST', json=json)\n",
      "\n",
      "    def put(self, json=None):\n",
      "        \"\"\"Send a PUT request and return the JSON decoded result.\n",
      "\n",
      "        Args:\n",
      "            json (dict, optional): Object to encode and send in request.\n",
      "\n",
      "        Returns:\n",
      "            mixed: JSON decoded response data.\n",
      "        \"\"\"\n",
      "        return self._request('PUT', json=json)\n",
      "\n",
      "    def delete(self, json=None):\n",
      "        \"\"\"Send a DELETE request and return the JSON decoded result.\n",
      "\n",
      "        Args:\n",
      "            json (dict, optional): Object to encode and send in request.\n",
      "\n",
      "        Returns:\n",
      "            mixed: JSON decoded response data.\n",
      "        \"\"\"\n",
      "        return self._request('DELETE', json=json)\n",
      "\n",
      "    def options(self, json=None):\n",
      "        \"\"\"Send a OPTIONS request and return the JSON decoded result.\n",
      "\n",
      "        Args:\n",
      "            json (dict, optional): Object to encode and send in request.\n",
      "\n",
      "        Returns:\n",
      "            mixed: JSON decoded response data.\n",
      "        \"\"\"\n",
      "        return self._request('OPTIONS', json=json)\n",
      "\n",
      "    def head\n",
      "==========score: 1.0===============\n",
      "def read(self, size=-1):\n",
      "        \"\"\"\n",
      "        Mimics the read call to a filehandle object.\n",
      "        \"\"\"\n",
      "        if size == -1:\n",
      "            size = self.size - self.pos\n",
      "        return self.fileobj.read(size)\n",
      "\n",
      "    def seek(self, offset, whence=0):\n",
      "        \"\"\"\n",
      "        Mimics the seek call to a filehandle object.\n",
      "        \"\"\"\n",
      "        seek_amount = offset\n",
      "        if whence == os.SEEK_SET:\n",
      "            seek_amount = min(offset, self.size)\n",
      "        elif whence == os.SEEK_CUR:\n",
      "            seek_amount = min(self.size - self.pos, offset)\n",
      "        elif whence == os.SEEK_END:\n",
      "            seek_amount = min(self.size, self.size + offset)\n",
      "        self.fileobj.seek(self.pos + seek_amount)\n",
      "        self.pos = self.pos + seek_amount\n",
      "\n",
      "    def write(self, data):\n",
      "        \"\"\"\n",
      "        Mimics the write call to a filehandle object.\n",
      "        \"\"\"\n",
      "        self.fileobj.write(data)\n",
      "        self.pos = self.pos + len(data)\n",
      "\n",
      "    def get_offset_from_rva(\n",
      "==========score: 1.0===============\n",
      "def Binomial(n, p, tag=None):\n",
      "    \"\"\"\n",
      "    A Binomial random variate\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n : int\n",
      "        The number of trials\n",
      "    p : scalar\n",
      "        The probability of success\n",
      "    \"\"\"\n",
      "    if n < 0 or p < 0 or p > 1:\n",
      "        raise ValueError(\"n and p must be non-negative and less than 1\")\n",
      "    if n == 0 or p == 1:\n",
      "        return 0\n",
      "    return scipy.stats.binom.rvs(n, p, size=1, random_state=None)\n",
      "\n",
      "def Binomial_2(n, p, tag=None):\n",
      "    \"\"\"\n",
      "    A Binomial random variate\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n : int\n",
      "        The number of trials\n",
      "    p : scalar\n",
      "        The probability of success\n",
      "    \"\"\"\n",
      "    if n < 0 or p < 0 or p > 1:\n",
      "        raise ValueError(\"n and p must be non-negative and less than 1\")\n",
      "    if n == 0 or p == 1:\n",
      "        return 0\n",
      "    return scipy.stats.binom.rvs(n, p, size=1, random_state=None)\n",
      "\n",
      "\n",
      "==========score: 1.0===============\n",
      "def get_binary_path(executable, logging_level='INFO'):\n",
      "    \"\"\"Gets the software name and returns the path of the binary.\"\"\"\n",
      "    if executable.endswith('.exe'):\n",
      "        return executable\n",
      "    else:\n",
      "        return executable + '.exe'\n",
      "==========score: 1.0===============\n",
      "def generate_html():\n",
      "    \"\"\"\n",
      "    Generate an HTML file incorporating the images produced by this script\n",
      "    \"\"\"\n",
      "    # Open the HTML file\n",
      "    with open(HTML_FILE, 'w') as html_file:\n",
      "        # Write the HTML header\n",
      "        html_file.write(HTML_HEADER)\n",
      "\n",
      "        # Write the body\n",
      "        html_file.write(HTML_BODY)\n",
      "==========score: 1.0===============\n",
      "def scatter_plot(self, ax, topic_dims, t=None, ms_limits=True, **kwargs_plot):\n",
      "        \"\"\" 2D or 3D scatter plot.\n",
      "\n",
      "            :param axes ax: matplotlib axes (use Axes3D if 3D data)\n",
      "\n",
      "            :param tuple topic_dims: list of (topic, dims) tuples, where topic is a string and dims is a list of dimensions to be plotted for that topic.\n",
      "\n",
      "            :param int t: time indexes to be plotted\n",
      "\n",
      "            :param dict kwargs_plot: argument to be passed to matplotlib's plot function, e.g. the style of the plotted points 'or'\n",
      "\n",
      "            :param bool ms_limits: if set to True, automatically set axes boundaries to the sensorimotor boundaries (default: True)\n",
      "        \"\"\"\n",
      "\n",
      "        # check if topic_dims is a list of tuples\n",
      "        if not isinstance(topic_dims, list):\n",
      "            raise TypeError('topic_dims must be a list of tuples')\n",
      "\n",
      "        # check if topic_dims is empty\n",
      "        if len(topic_dims) == 0:\n",
      "            raise ValueError('topic_dims must not be empty')\n",
      "\n",
      "        # check if topic_dims is a list of tuples\n",
      "        for topic_dim in topic_dims:\n",
      "            if not isinstance(topic_dim, tuple):\n",
      "                raise TypeError('topic_dims must be a list of tuples')\n",
      "\n",
      "            # check if topic_dim is a tuple\n",
      "            if len(topic_dim)!= 2:\n",
      "                raise ValueError('topic_dims must be a list of tuples with exactly 2 elements')\n",
      "\n",
      "            # check if topic_dim[0] is a string\n",
      "            if not isinstance(topic_dim[0], str):\n",
      "                raise TypeError('topic_dims must be a list of tuples with exactly 2 elements')\n",
      "\n",
      "            # check if topic_dim[1] is a list\n",
      "            \n",
      "==========score: 1.0===============\n",
      "def multiply(self, matrix):\n",
      "        \"\"\"\n",
      "        Multiply this matrix by a local dense matrix on the right.\n",
      "\n",
      "        :param matrix: a local dense matrix whose number of rows must match the number of columns\n",
      "                       of this matrix\n",
      "        :returns: :py:class:`IndexedRowMatrix`\n",
      "\n",
      "        >>> mat = IndexedRowMatrix(sc.parallelize([(0, (0, 1)), (1, (2, 3))]))\n",
      "        >>> mat.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()\n",
      "        [IndexedRow(0, [2.0,3.0]), IndexedRow(1, [6.0,11.0])]\n",
      "        \"\"\"\n",
      "        if not isinstance(matrix, DenseMatrix):\n",
      "            raise ValueError(\"Only multiplication with DenseMatrix \"\n",
      "                             \"is supported.\")\n",
      "        if self.numRows!= matrix.numRows:\n",
      "            raise ValueError(\"Number of rows in matrix must match this \"\n",
      "                             \"matrix: expected %d, got %d.\" %\n",
      "                             (self.numRows, matrix.numRows))\n",
      "        if self.numCols!= matrix.numCols:\n",
      "            raise ValueError(\"Number of columns in matrix must match this \"\n",
      "                             \"matrix: expected %d, got %d.\" %\n",
      "                             (self.numCols, matrix.numCols))\n",
      "        rows = self.rows\n",
      "        newRows = []\n",
      "        for row, colMatrix in zip(rows, matrix.cols):\n",
      "            cols = colMatrix.toArray()\n",
      "            newRow = [matrix._elemwise(row[i], cols[i]) for i in range(len(cols))]\n",
      "            newRows.append(newRow)\n",
      "        return IndexedRowMatrix(self.indices, newRows)\n",
      "==========score: 1.0===============\n",
      "def extract_zip(zip_file_path):\n",
      "    \"\"\"\n",
      "    Returns:\n",
      "        dict: Dict[str, DataFrame]\n",
      "    \"\"\"\n",
      "    with ZipFile(zip_file_path, 'r') as zip_file:\n",
      "        file_names = zip_file.namelist()\n",
      "        if len(file_names)!= 1:\n",
      "            raise ValueError(f'Zip file {zip_file_path} contains {len(file_names)} files, expected 1')\n",
      "        file_name = file_names[0]\n",
      "        if file_name.endswith('.csv'):\n",
      "            return {file_name: pd.read_csv(zip_file.open(file_name))}\n",
      "        elif file_name.endswith('.json'):\n",
      "            return {file_name: pd.read_json(zip_file.open(file_name))}\n",
      "        else:\n",
      "            raise ValueError(f'Unknown file type {file_name}')\n",
      "==========score: 1.0===============\n",
      "def readcsv(fn):\n",
      "    \"\"\"\n",
      "    Wrapper to read arbitrary csv, check for header\n",
      "\n",
      "    Needs some work to be more robust, quickly added for demcoreg sampling\n",
      "    \"\"\"\n",
      "    with open(fn) as f:\n",
      "        reader = csv.reader(f)\n",
      "        header = next(reader)\n",
      "        if header!= ['x', 'y', 'z']:\n",
      "            raise ValueError('CSV file does not have correct header')\n",
      "        data = []\n",
      "        for row in reader:\n",
      "            data.append(row)\n",
      "    return np.array(data)\n",
      "==========score: 1.0===============\n",
      "def get_process_info(self, pid=None):\n",
      "        '''\n",
      "        get_process_info(self, pid=None)\n",
      "\n",
      "        Get process general information.\n",
      "\n",
      "        :Parameters:\n",
      "        * *pid* (`string`) -- Identifier of an existing process\n",
      "\n",
      "        '''\n",
      "        if pid is None:\n",
      "            pid = self.pid\n",
      "        return self.execute_command('get_process_info', pid=pid)\n",
      "\n",
      "    def get_process_memory_info(self, pid=None):\n",
      "        '''\n",
      "        get_process_memory_info(self, pid=None)\n",
      "\n",
      "        Get process memory information.\n",
      "\n",
      "        :Parameters:\n",
      "        * *pid* (`string`) -- Identifier of an existing process\n",
      "\n",
      "        '''\n",
      "        if pid is None:\n",
      "            pid = self.pid\n",
      "        return self.execute_command('get_process_memory_info', pid=pid)\n",
      "\n",
      "    def get_process_io_counters(self, pid=None):\n",
      "        '''\n",
      "        get_process_io_counters(self, pid=None)\n",
      "\n",
      "        Get process I/O statistics.\n",
      "\n",
      "        :Parameters:\n",
      "        * *pid* (`string`) -- Identifier of an existing process\n",
      "\n",
      "        '''\n",
      "        if pid is None:\n",
      "            pid = self.pid\n",
      "        return self.execute_command('get_process_io_counters', pid=pid)\n",
      "\n",
      "    def get_process\n",
      "==========score: 1.0===============\n",
      "def trigger(self, identifier, force=True):\n",
      "        \"\"\"Trigger an upgrade task.\"\"\"\n",
      "        if identifier in self.tasks:\n",
      "            self.tasks[identifier].trigger(force)\n",
      "        else:\n",
      "            raise ValueError(\"Unknown task identifier: %s\" % identifier)\n",
      "\n",
      "    def get_task(self, identifier):\n",
      "        \"\"\"Return the task with the given identifier.\"\"\"\n",
      "        if identifier in self.tasks:\n",
      "            return self.tasks[identifier]\n",
      "        else:\n",
      "            raise ValueError(\"Unknown task identifier: %s\" % identifier)\n",
      "\n",
      "    def get_tasks(self):\n",
      "        \"\"\"Return a list of all tasks.\"\"\"\n",
      "        return list(self.tasks.values())\n",
      "\n",
      "    def get_task_by_name(self, name):\n",
      "        \"\"\"Return the task with the given name.\"\"\"\n",
      "        for task in self.tasks.values():\n",
      "            if task.name == name:\n",
      "                return task\n",
      "        return None\n",
      "\n",
      "    def get_task_by_identifier(self, identifier):\n",
      "        \"\"\"Return the task with the given identifier.\"\"\"\n",
      "        for task in self.tasks.values():\n",
      "            if task.identifier == identifier:\n",
      "                return task\n",
      "        return None\n",
      "\n",
      "\n",
      "==========score: 1.0===============\n",
      "def append_cookie(self, cookie, name, payload, typ, domain=None, path=None,\n",
      "                      timestamp=\"\", max_age=0):\n",
      "        \"\"\"\n",
      "        Adds a cookie to a SimpleCookie instance\n",
      "\n",
      "        :param cookie:\n",
      "        :param name:\n",
      "        :param payload:\n",
      "        :param typ:\n",
      "        :param domain:\n",
      "        :param path:\n",
      "        :param timestamp:\n",
      "        :param max_age:\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        if isinstance(cookie, dict):\n",
      "            cookie = SimpleCookie(cookie)\n",
      "        if isinstance(cookie, SimpleCookie):\n",
      "            cookie[name] = payload\n",
      "            cookie[name][\"path\"] = path\n",
      "            cookie[name][\"domain\"] = domain\n",
      "            cookie[name][\"max-age\"] = max_age\n",
      "            cookie[name][\"expires\"] = timestamp\n",
      "            cookie[name][\"version\"] = typ\n",
      "            cookie[name][\"httponly\"] = True\n",
      "            cookie[name][\"secure\"] = False\n",
      "        else:\n",
      "            raise TypeError(\"Cookie must be a dict or SimpleCookie instance\")\n",
      "\n",
      "    def delete_cookie(self, name, path='/', domain=None):\n",
      "        \"\"\"\n",
      "        Deletes a cookie from the client.\n",
      "\n",
      "        :param name:\n",
      "        :param path:\n",
      "        :param domain:\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        self.set_cookie(name, '', path=path, domain=domain)\n",
      "\n",
      "    def set_secure_cookie(self, name, value, expires=None, path='/', domain=None):\n",
      "        \"\"\"\n",
      "        S\n",
      "==========score: 1.0===============\n",
      "def request(self, url,\n",
      "                json=\"\",\n",
      "                data=\"\",\n",
      "                username=\"\",\n",
      "                password=\"\",\n",
      "                headers=None,\n",
      "                timout=30):\n",
      "        \"\"\"This is overridden on module initialization. This function will make\n",
      "        an HTTP POST to a given url. Either json/data will be what is posted to\n",
      "        the end point. he HTTP request needs to be basicAuth when username and\n",
      "        password are provided. a headers dict maybe provided,\n",
      "        whatever the values are should be applied.\n",
      "\n",
      "        Args:\n",
      "            url (str):                  url to send the POST\n",
      "            json (dict, optional):      Dict of the JSON to POST\n",
      "            data (dict, optional):      Dict, presumed flat structure of\n",
      "                                        key/value of request to place as\n",
      "                                        www-form\n",
      "            username (str, optional):    Username for basic auth. Must be\n",
      "                                        uncluded as part of password.\n",
      "            password (str, optional):   Password for basic auth. Must be\n",
      "                                        included as part of username.\n",
      "            headers (dict, optional):   Key/Value pairs of headers to include\n",
      "        Returns:\n",
      "            str:    Raw request placed\n",
      "            str:    Raw response received\n",
      "            int:    HTTP status code, eg 200,404,401\n",
      "            dict:   Key/Value pairs of the headers received.\n",
      "            :param timout:\n",
      "        \"\"\"\n",
      "        if headers is None:\n",
      "            headers = {}\n",
      "        if username and password:\n",
      "            headers[\"Authorization\"] = \"Basic {}\".format(\n",
      "                base64.b64encode(\n",
      "                    \"{}:{}\".format(username, password).encode()).decode())\n",
      "        if json:\n",
      "            data = json\n",
      "        if not data:\n",
      "            data = {}\n",
      "        if not url.startswith(\"http\"):\n",
      "            url = \"https://{}\".format(url)\n",
      "        if not data:\n",
      "            data = {}\n",
      "        if not headers:\n",
      "            headers = {}\n",
      "        if not headers.get(\"Content-Type\"):\n",
      "            headers[\"Content-Type\"] = \"application/json\"\n",
      "        if not headers.get(\"Accept\"):\n",
      "            headers[\"Accept\"] = \"application/json\"\n",
      "        if not headers.get(\"User-Agent\"):\n",
      "            headers[\"User-Agent\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.\n",
      "==========score: 0.9999===============\n",
      "def cluster_mini_batch_kmeans(data=None, k=100, max_iter=10, batch_size=0.2, metric='euclidean',\n",
      "                              init_strategy='kmeans++', n_jobs=None, chunksize=None, skip=0, clustercenters=None, **kwargs):\n",
      "    r\"\"\"k-means clustering with mini-batch strategy\n",
      "\n",
      "    Mini-batch k-means is an approximation to k-means which picks a randomly\n",
      "    selected subset of data points to be updated in each iteration. Usually\n",
      "    much faster than k-means but will likely deliver a less optimal result.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    kmeans_mini : a :class:`MiniBatchKmeansClustering <pyemma.coordinates.clustering.MiniBatchKmeansClustering>` clustering object\n",
      "        Object for mini-batch kmeans clustering.\n",
      "        It holds discrete trajectories and cluster center information.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    :func:`kmeans <pyemma.coordinates.kmeans>` : for full k-means clustering\n",
      "\n",
      "\n",
      "    .. autoclass:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n",
      "        :members:\n",
      "        :undoc-members:\n",
      "\n",
      "        .. rubric:: Methods\n",
      "\n",
      "        .. autoautosummary:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n",
      "           :methods:\n",
      "\n",
      "        .. rubric:: Attributes\n",
      "\n",
      "        .. autoautosummary:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n",
      "            :attributes:\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n",
      "\n",
      "    \"\"\"\n",
      "    if data is None:\n",
      "        raise ValueError(\"data must be provided\")\n",
      "\n",
      "    if k is None:\n",
      "        raise ValueError(\"k must be provided\")\n",
      "\n",
      "    if max_iter is None:\n",
      "        raise ValueError(\"max_iter must be provided\")\n",
      "\n",
      "    if batch_size is None:\n",
      "        raise ValueError(\"batch_size must be provided\")\n",
      "\n",
      "    if metric is None:\n",
      "        raise ValueError(\"metric must be provided\")\n",
      "\n",
      "    if init_strategy is None:\n",
      "        raise ValueError(\"init_strategy must be provided\")\n",
      "\n",
      "    if n_jobs is None:\n",
      "        raise ValueError(\"n_jobs must be provided\")\n",
      "\n",
      "    if chunksize is None:\n",
      "        raise ValueError(\"chunksize must be provided\")\n",
      "\n",
      "    if skip is None:\n",
      "        raise ValueError(\"skip must be provided\")\n",
      "\n",
      "    if clustercenters is None:\n",
      "        raise ValueError(\"clustercenters must be provided\")\n",
      "\n",
      "    if kwargs is None:\n",
      "        kwargs = {}\n",
      "\n",
      "    if data.shape[0] < k:\n",
      "        raise ValueError(\"data.shape[0] must be >= k\")\n",
      "\n",
      "    if data.\n",
      "==========score: 1.0===============\n",
      "def __get_current_datetime(self):\n",
      "        \"\"\"Get current datetime for every file.\"\"\"\n",
      "        return datetime.now()\n",
      "\n",
      "    def __get_current_date(self):\n",
      "        \"\"\"Get current date for every file.\"\"\"\n",
      "        return self.__get_current_datetime().date()\n",
      "\n",
      "    def __get_current_time(self):\n",
      "        \"\"\"Get current time for every file.\"\"\"\n",
      "        return self.__get_current_datetime().time()\n",
      "\n",
      "    def __get_current_timestamp(self):\n",
      "        \"\"\"Get current timestamp for every file.\"\"\"\n",
      "        return self.__get_current_datetime().timestamp()\n",
      "\n",
      "    def __get_current_timestamp_ms(self):\n",
      "        \"\"\"Get current timestamp for every file in milliseconds.\"\"\"\n",
      "        return self.__get_current_timestamp() * 1000\n",
      "\n",
      "    def __get_current_timestamp_ms_str(self):\n",
      "        \"\"\"Get current timestamp for every file in milliseconds as string.\"\"\"\n",
      "        return str(self.__get_current_timestamp_ms())\n",
      "\n",
      "    def __get_current_timestamp_ms_str_short(self):\n",
      "        \"\"\"Get current timestamp for every file in milliseconds as string.\"\"\"\n",
      "        return str(self\n",
      "==========score: 1.0===============\n",
      "def _tmp_html_file(\n",
      "            self,\n",
      "            content):\n",
      "        \"\"\"*create a tmp html file with some content used for the header or footer of the ebook*\n",
      "\n",
      "        **Key Arguments:**\n",
      "            - ``content`` -- the content to include in the HTML file.\n",
      "        \"\"\"\n",
      "        tmp_html_file = NamedTemporaryFile(suffix='.html', delete=False)\n",
      "        tmp_html_file.write(content.encode('utf-8'))\n",
      "        tmp_html_file.close()\n",
      "        return tmp_html_file\n",
      "\n",
      "    def _tmp_css_file(self, css_file):\n",
      "        \"\"\"*create a tmp css file used for the header or footer of the ebook*\n",
      "\n",
      "        **Key Arguments:**\n",
      "            - ``css_file`` -- the css file to include in the HTML file.\n",
      "        \"\"\"\n",
      "        tmp_css_file = NamedTemporaryFile(suffix='.css', delete=False)\n",
      "        tmp_css_file.write(css_file.encode('utf-8'))\n",
      "        tmp_css_file.close()\n",
      "        return tmp_css_file\n",
      "\n",
      "    def _tmp_html_file_for_css(self, css_file):\n",
      "        \"\"\"*create a tmp html file used for the header or footer of the ebook*\n",
      "\n",
      "        **Key Arguments:**\n",
      "            - ``css_file`` -- the css file to\n",
      "==========score: 1.0===============\n",
      "def heatmap(x, y, step=None, min_pt=None, max_pt=None,\n",
      "                 colormap='Blues', alpha=1, grid=False,\n",
      "                 colorbar=True, scale='lin',\n",
      "                 vmax='auto', vmin='auto', crop=True):\n",
      "    \"\"\"\n",
      "    function to take vectors x and y and hist them\n",
      "    \"\"\"\n",
      "    if min_pt is None:\n",
      "        min_pt = np.min(x)\n",
      "    if max_pt is None:\n",
      "        max_pt = np.max(x)\n",
      "    if step is None:\n",
      "        step = (max_pt - min_pt) / 100\n",
      "    if vmax == 'auto':\n",
      "        vmax = max_pt\n",
      "    if vmin == 'auto':\n",
      "        vmin = min_pt\n",
      "    if scale == 'log':\n",
      "        vmax = np.log(vmax)\n",
      "        vmin = np.log(vmin)\n",
      "    if vmax == 'auto':\n",
      "        vmax = max_pt\n",
      "    if vmin == 'auto':\n",
      "        vmin = min_pt\n",
      "    if crop:\n",
      "        x = x[(x >= min_pt) & (x <= max_pt)]\n",
      "        y = y[(y >= min_pt) & (y <= max_pt)]\n",
      "    if grid:\n",
      "        plt.grid()\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    plt.pcolormesh(x, y, np.zeros_\n",
      "==========score: 1.0===============\n",
      "def write(self, data):\n",
      "        \"\"\"\n",
      "        Writes data to the device.\n",
      "\n",
      "        :param data: data to write\n",
      "        :type data: string\n",
      "\n",
      "        :raises: py:class:`~alarmdecoder.util.CommError`\n",
      "        \"\"\"\n",
      "        self._write(data)\n",
      "\n",
      "    def read(self, size=1):\n",
      "        \"\"\"\n",
      "        Reads data from the device.\n",
      "\n",
      "        :param size: number of bytes to read\n",
      "        :type size: int\n",
      "\n",
      "        :returns: data read\n",
      "        :rtype: string\n",
      "        \"\"\"\n",
      "        return self._read(size)\n",
      "\n",
      "    def close(self):\n",
      "        \"\"\"\n",
      "        Closes the device.\n",
      "        \"\"\"\n",
      "        self._close()\n",
      "\n",
      "    def __enter__(self):\n",
      "        return self\n",
      "\n",
      "    def __exit__(self, exc_type, exc_value, traceback):\n",
      "        self.close()\n",
      "\n",
      "==========score: 1.0===============\n",
      "def _trim(cls, s):\n",
      "        \"\"\"\n",
      "        Remove trailing colons from the URI back to the first non-colon.\n",
      "\n",
      "        :param string s: input URI string\n",
      "        :returns: URI string with trailing colons removed\n",
      "        :rtype: string\n",
      "\n",
      "        TEST: trailing colons necessary\n",
      "\n",
      "        >>> s = '1:2::::'\n",
      "        >>> CPE._trim(s)\n",
      "        '1:2'\n",
      "\n",
      "        TEST: trailing colons not necessary\n",
      "\n",
      "        >>> s = '1:2:3:4:5:6'\n",
      "        >>> CPE._trim(s)\n",
      "        '1:2:3:4:5:6'\n",
      "        \"\"\"\n",
      "        return s.rstrip(':')\n",
      "\n",
      "    @classmethod\n",
      "    def _split(cls, s):\n",
      "        \"\"\"\n",
      "        Split a string into a list of components.\n",
      "\n",
      "        :param string s: input URI string\n",
      "        :returns: list of components\n",
      "        :rtype: list\n",
      "\n",
      "        TEST: split\n",
      "\n",
      "        >>> s = 'http://www.example.com/foo/bar'\n",
      "        >>> CPE._split(s)\n",
      "        ['http', 'www.example.com', 'foo', 'bar']\n",
      "\n",
      "        TEST: split with no path\n",
      "\n",
      "        >>> s = 'http://www.example.com'\n",
      "        >>> CPE._split(s)\n",
      "        ['http', 'www.example.com']\n",
      "\n",
      "        TEST: split with no scheme\n",
      "\n",
      "        >>> s = 'foo:bar'\n",
      "        >>> CPE._split(s)\n",
      "        ['foo', 'bar']\n",
      "\n",
      "        TEST: split with no scheme and no path\n",
      "\n",
      "        >>> s = 'foo'\n",
      "        >>> CPE._split(s)\n",
      "        ['foo']\n",
      "\n",
      "        TEST: split with no scheme and no path and no query\n",
      "\n",
      "        >>> s = '\n",
      "==========score: 1.0===============\n",
      "def fuzzmatch(self, fuzzkey, multi=False):\n",
      "        \"\"\"\n",
      "        Identify a filter by fuzzy string matching.\n",
      "\n",
      "        Partial ('fuzzy') matching performed by `fuzzywuzzy.fuzzy.ratio`\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        fuzzkey : str\n",
      "            A string that partially matches one filter name more than the others.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The name of the most closely matched filter. : str\n",
      "        \"\"\"\n",
      "        if not multi:\n",
      "            return self.fuzzymatch(fuzzkey, multi=True)\n",
      "\n",
      "        # If we have multiple filters, we need to do a fuzzy match\n",
      "        # against each one of them.\n",
      "        #\n",
      "        # We want to do a fuzzy match against the first filter\n",
      "        # that matches the fuzzkey.\n",
      "        #\n",
      "        # We also want to make sure that the rest of the filters\n",
      "        # are matched against the same fuzzkey.\n",
      "        #\n",
      "        # So we'll first find the first filter that matches the fuzzkey.\n",
      "        #\n",
      "        # We'll then use that filter's name as the key to the dictionary\n",
      "        # of filters.\n",
      "        #\n",
      "        # We'll then go through the rest of the filters and see if any\n",
      "        # of them match the fuzzkey.\n",
      "        #\n",
      "        # If so, we'll update the dictionary of filters with the new\n",
      "        # filter's name as the key.\n",
      "        #\n",
      "        # If not, we'll just keep going.\n",
      "        #\n",
      "        # If we've gone through the whole list of filters without\n",
      "        # finding any that match, we'll raise an error.\n",
      "\n",
      "==========score: 0.9977===============\n",
      "def read(self, size):\n",
      "\t\t\"\"\"\n",
      "\t\tRead raw data from the serial connection. This function is not\n",
      "\t\tmeant to be called directly.\n",
      "\n",
      "\t\t:param int size: The number of bytes to read from the serial connection.\n",
      "\t\t\"\"\"\n",
      "\t\tif not self.is_open:\n",
      "\t\t\traise PortNotOpenError()\n",
      "\n",
      "\t\tif size > 0:\n",
      "\t\t\tdata = self.connection.read(size)\n",
      "\t\t\tif self.logger:\n",
      "\t\t\t\tself.logger.log(self.logging.RECV, data)\n",
      "\t\t\treturn data\n",
      "\t\telse:\n",
      "\t\t\treturn b''\n",
      "\n",
      "\tdef write(self, data):\n",
      "\t\t\"\"\"\n",
      "\t\tWrite raw data to the serial connection. This function is not\n",
      "\t\tmeant to be called directly.\n",
      "\n",
      "\t\t:param bytes data: The data to write to the serial connection.\n",
      "\t\t\"\"\"\n",
      "\t\tif not self.is_open:\n",
      "\t\t\traise PortNotOpenError()\n",
      "\n",
      "\t\tif self.logger:\n",
      "\t\t\tself.logger.log(self.logging.SEND, data)\n",
      "\t\treturn self.connection.write(data)\n",
      "\n",
      "\tdef reset_input_buffer(self):\n",
      "\t\t\"\"\"Clear input buffer, discarding all that is in the buffer.\"\"\"\n",
      "\t\tif not self.is_open:\n",
      "\t\t\traise PortNotOpenError()\n",
      "\t\tself.connection.reset_input_buffer()\n",
      "\n",
      "\tdef reset_output_buffer(self):\n",
      "==========score: 1.0===============\n",
      "def lreg(self, xcol, ycol, name=\"Regression\"):\n",
      "        \"\"\"\n",
      "        Add a column to the main dataframe populted with\n",
      "        the model's linear regression for a column\n",
      "        \"\"\"\n",
      "        self.df[name] = self.df[xcol].apply(lambda x: self.model.predict(x)[0])\n",
      "        return self.df\n",
      "\n",
      "    def lreg_predict(self, xcol, ycol, name=\"Regression\"):\n",
      "        \"\"\"\n",
      "        Add a column to the main dataframe populted with\n",
      "        the model's linear regression for a column\n",
      "        \"\"\"\n",
      "        self.df[name] = self.model.predict(self.df[xcol])\n",
      "        return self.df\n",
      "\n",
      "    def lreg_predict_proba(self, xcol, ycol, name=\"Regression\"):\n",
      "        \"\"\"\n",
      "        Add a column to the main dataframe populted with\n",
      "        the model's linear regression for a column\n",
      "        \"\"\"\n",
      "        self.df[name] = self.model.predict_proba(self.df[xcol])\n",
      "        return self.df\n",
      "\n",
      "    def lreg_predict_proba_samples(self, xcol, ycol, name=\"Regression\"):\n",
      "        \"\"\"\n",
      "        Add a column to the main dataframe populted with\n",
      "        \n",
      "==========score: 1.0===============\n",
      "def conn(host=None, user=None, password=None, init_fun=None, reset=False):\n",
      "    \"\"\"\n",
      "    Returns a persistent connection object to be shared by multiple modules.\n",
      "    If the connection is not yet established or reset=True, a new connection is set up.\n",
      "    If connection information is not provided, it is taken from config which takes the\n",
      "    information from dj_local_conf.json. If the password is not specified in that file\n",
      "    datajoint prompts for the password.\n",
      "\n",
      "    :param host: hostname\n",
      "    :param user: mysql user\n",
      "    :param password: mysql password\n",
      "    :param init_fun: initialization function\n",
      "    :param reset: whether the connection should be reset or not\n",
      "    \"\"\"\n",
      "    if host is None:\n",
      "        host = config.get('db_host')\n",
      "    if user is None:\n",
      "        user = config.get('db_user')\n",
      "    if password is None:\n",
      "        password = config.get('db_password')\n",
      "    if init_fun is None:\n",
      "        init_fun = config.get('db_init_fun')\n",
      "    if reset:\n",
      "        conn = None\n",
      "    else:\n",
      "        conn = get_connection(host, user, password)\n",
      "    if conn is None:\n",
      "        if init_fun is None:\n",
      "            init_fun = config.get('db_init_fun')\n",
      "        if init_fun is None:\n",
      "            raise ValueError('No connection information provided')\n",
      "        conn = get_connection(host, user, password, init_fun)\n",
      "    return conn\n",
      "==========score: 1.0===============\n",
      "def __datetime_to_epoch(self, date_time):\n",
      "        \"\"\"\n",
      "        Converts a python datetime to unix epoch, accounting for\n",
      "        time zones and such.\n",
      "\n",
      "        Assumes UTC if timezone is not given.\n",
      "        \"\"\"\n",
      "        if date_time.tzinfo is None:\n",
      "            date_time = date_time.replace(tzinfo=pytz.UTC)\n",
      "        else:\n",
      "            date_time = date_time.astimezone(pytz.UTC)\n",
      "        return int(calendar.timegm(date_time.timetuple()))\n",
      "\n",
      "    def __get_data_from_file(self, filename):\n",
      "        \"\"\"\n",
      "        Reads the data from the given file.\n",
      "\n",
      "        :param filename: The name of the file to read.\n",
      "        :type filename: str\n",
      "        :return: The data read from the file.\n",
      "        :rtype: dict\n",
      "        \"\"\"\n",
      "        with open(filename, 'r') as f:\n",
      "            return json.load(f)\n",
      "\n",
      "    def __get_data_from_url(self, url):\n",
      "        \"\"\"\n",
      "        Reads the data from the given url.\n",
      "\n",
      "        :param url: The url to read.\n",
      "        :type url: str\n",
      "        :return: The data read from the url.\n",
      "        :rtype: dict\n",
      "        \"\"\"\n",
      "        return requests.get(url).json()\n",
      "\n",
      "    def __get\n",
      "==========score: 1.0===============\n",
      "def escape(t):\n",
      "    \"\"\"HTML-escape the text in `t`.\"\"\"\n",
      "    return escape(t).replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
      "\n",
      "def quote(t):\n",
      "    \"\"\"Quote the text in `t`.\"\"\"\n",
      "    return \"\\\"\" + escape(t) + \"\\\"\"\n",
      "\n",
      "\n",
      "==========score: 1.0===============\n",
      "def _flush_queue(self, q, ignore_priority=False):\n",
      "        \"\"\"\n",
      "        :param q: PriorityQueue instance holding GarbageCollector entries\n",
      "        :param ignore_priority: If True - all GarbageCollector entries should be resubmitted\n",
      "                If False - only those entries whose waiting time has expired will be resubmitted\n",
      "        \"\"\"\n",
      "        while not q.empty():\n",
      "            gc = q.get()\n",
      "            if not ignore_priority or gc.priority == 0:\n",
      "                gc.submit()\n",
      "            else:\n",
      "                gc.submit(priority=0)\n",
      "            gc.waiting_time = 0\n",
      "            gc.priority = 0\n",
      "\n",
      "    def _get_garbage_collector(self, gc_id):\n",
      "        \"\"\"\n",
      "        :param gc_id: ID of the garbage collector\n",
      "        :return: The garbage collector with the given ID\n",
      "        \"\"\"\n",
      "        return self.garbage_collectors[gc_id]\n",
      "\n",
      "    def _get_garbage_collector_by_name(self, gc_name):\n",
      "        \"\"\"\n",
      "        :param gc_name: Name of the garbage collector\n",
      "        :return: The garbage collector with the given name\n",
      "        \"\"\"\n",
      "        for gc in self.garbage_collectors.values():\n",
      "            if gc.name == gc_name:\n",
      "                return gc\n",
      "        return None\n",
      "\n",
      "    def _get_garbage_collector_by_id(self, gc_id):\n",
      "        \"\"\"\n",
      "        :\n",
      "==========score: 1.0===============\n",
      "def encode(self):\n",
      "        \"\"\"\n",
      "        Encodes the current state of the object into a string.\n",
      "\n",
      "        :return: The encoded string\n",
      "        \"\"\"\n",
      "        return self.__str__()\n",
      "\n",
      "    def __str__(self):\n",
      "        \"\"\"\n",
      "        Encodes the current state of the object into a string.\n",
      "\n",
      "        :return: The encoded string\n",
      "        \"\"\"\n",
      "        return self.__repr__()\n",
      "\n",
      "    def __repr__(self):\n",
      "        \"\"\"\n",
      "        Encodes the current state of the object into a string.\n",
      "\n",
      "        :return: The encoded string\n",
      "        \"\"\"\n",
      "        return self.__str__()\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        \"\"\"\n",
      "        Compares the current state of the object with another object for equality.\n",
      "\n",
      "        :param other: The other object to compare with\n",
      "        :return: True if the objects are equal, False otherwise\n",
      "        \"\"\"\n",
      "        if not isinstance(other, State):\n",
      "            return False\n",
      "        return self.__dict__ == other.__dict__\n",
      "\n",
      "    def __ne__(self, other):\n",
      "        \"\"\"\n",
      "        Compares the current state of the object with another object for inequality.\n",
      "\n",
      "        :param other: The other object to compare with\n",
      "        :return: True if the objects are not equal, False otherwise\n",
      "        \"\"\"\n",
      "        return not self\n",
      "==========score: 1.0===============\n",
      "def read_field_report(path, data_flag = \"*DATA\", meta_data_flag = \"*METADATA\"):\n",
      "  \"\"\"\n",
      "  Reads a field output report.\n",
      "  \"\"\"\n",
      "  \n",
      "  # Open the file\n",
      "  f = open(path, 'r')\n",
      "  \n",
      "  # Read the header\n",
      "  header = f.readline()\n",
      "  \n",
      "  # Read the data\n",
      "  data = f.readlines()\n",
      "  \n",
      "  # Close the file\n",
      "  f.close()\n",
      "  \n",
      "  # Split the header\n",
      "  header = header.split()\n",
      "  \n",
      "  # Create a dictionary\n",
      "  data_dict = {}\n",
      "  \n",
      "  # Loop over the data\n",
      "  for i in range(len(data)):\n",
      "    \n",
      "    # Split the data\n",
      "    data[i] = data[i].split()\n",
      "    \n",
      "    # Get the data type\n",
      "    data_type = data[i][0]\n",
      "    \n",
      "    # Get the data\n",
      "    data_value = data[i][1]\n",
      "    \n",
      "    # Get the unit\n",
      "    data_unit = data[i][2]\n",
      "    \n",
      "    # Get the data name\n",
      "    data_name = data[i][3]\n",
      "    \n",
      "    # Get the data description\n",
      "    data_desc = data[i][4]\n",
      "    \n",
      "    # Get the data flag\n",
      "    data_\n",
      "==========score: 1.0===============\n",
      "def write_csvs(self,\n",
      "                   asset_map,\n",
      "                   show_progress=False,\n",
      "                   invalid_data_behavior='warn'):\n",
      "        \"\"\"Read CSVs as DataFrames from our asset map.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        asset_map : dict[int -> str]\n",
      "            A mapping from asset id to file path with the CSV data for that\n",
      "            asset\n",
      "        show_progress : bool\n",
      "            Whether or not to show a progress bar while writing.\n",
      "        invalid_data_behavior : {'warn', 'raise', 'ignore'}\n",
      "            What to do when data is encountered that is outside the range of\n",
      "            a uint32.\n",
      "        \"\"\"\n",
      "        if show_progress:\n",
      "            progress_bar = progress.progress_bar()\n",
      "            progress_bar.start(len(asset_map))\n",
      "\n",
      "        for asset_id, csv_path in asset_map.items():\n",
      "            if show_progress:\n",
      "                progress_bar.update(asset_id)\n",
      "            # If a column is not found, pandas will skip it.\n",
      "            # This is fine, since it means that a column is not\n",
      "            # present for an asset.\n",
      "            df = pd.read_csv(csv_path, skipinitialspace=True,\n",
      "                             dtype=np.uint32,\n",
      "                             usecols=['datetime', 'open', 'high', 'low',\n",
      "                                      'close', 'volume'])\n",
      "            df.datetime = pd.to_datetime(df.datetime)\n",
      "            df.set_index('datetime', inplace=True)\n",
      "            df.sort_index(inplace=True)\n",
      "            df.rename(columns={'open': 'open_price',\n",
      "                               'high': 'high_price',\n",
      "                               'low': '\n",
      "==========score: 1.0===============\n",
      "def encrypt(self, key):\n",
      "        \"\"\"This method encrypts and signs the state to make it unreadable by\n",
      "        the server, since it contains information that would allow faking\n",
      "        proof of storage.\n",
      "\n",
      "        :param key: the key to encrypt and sign with\n",
      "        \"\"\"\n",
      "        self.state = self.state.encrypt(key)\n",
      "        self.signature = self.signature.encrypt(key)\n",
      "\n",
      "    def decrypt(self, key):\n",
      "        \"\"\"This method decrypts and signs the state to make it readable by\n",
      "        the server, since it contains information that would allow faking\n",
      "        proof of storage.\n",
      "\n",
      "        :param key: the key to decrypt and sign with\n",
      "        \"\"\"\n",
      "        self.state = self.state.decrypt(key)\n",
      "        self.signature = self.signature.decrypt(key)\n",
      "\n",
      "    def verify(self, key):\n",
      "        \"\"\"This method verifies the signature of the state to make sure\n",
      "        it is not forged.\n",
      "\n",
      "        :param key: the key to verify the signature with\n",
      "        \"\"\"\n",
      "        self.state.verify(key)\n",
      "        self.signature.verify(key)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return '<%s %s>' % (self.__class__.__name__, self.state)\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        if not isinstance(other, self.__class\n",
      "==========score: 1.0===============\n",
      "def _unzip_file(self, src_path, dest_path, filename):\n",
      "        \"\"\"unzips file located at src_path into destination_path\"\"\"\n",
      "        with zipfile.ZipFile(src_path, 'r') as zip_ref:\n",
      "            zip_ref.extractall(dest_path)\n",
      "        os.remove(src_path)\n",
      "\n",
      "    def _get_file_path(self, file_name):\n",
      "        \"\"\"returns the path of a file in the current directory\"\"\"\n",
      "        return os.path.join(os.getcwd(), file_name)\n",
      "\n",
      "    def _get_file_name(self, file_path):\n",
      "        \"\"\"returns the name of a file in the current directory\"\"\"\n",
      "        return os.path.basename(file_path)\n",
      "\n",
      "    def _get_file_extension(self, file_name):\n",
      "        \"\"\"returns the extension of a file in the current directory\"\"\"\n",
      "        return os.path.splitext(file_name)[1]\n",
      "\n",
      "    def _get_file_size(self, file_path):\n",
      "        \"\"\"returns the size of a file in the current directory\"\"\"\n",
      "        return os.path.getsize(file_path)\n",
      "\n",
      "    def _get_file_modified_time(self, file_path):\n",
      "        \"\"\"return\n",
      "==========score: 1.0===============\n",
      "def _regex_span(_regex, _str, case_insensitive=True):\n",
      "    \"\"\"Return all matches in an input string.\n",
      "    :rtype : regex.match.span\n",
      "    :param _regex: A regular expression pattern.\n",
      "    :param _str: Text on which to run the pattern.\n",
      "    \"\"\"\n",
      "    if case_insensitive:\n",
      "        _str = _str.lower()\n",
      "    return _regex.search(_str).span()\n",
      "==========score: 1.0===============\n",
      "def get_binary_path(executable, logging_level='INFO'):\n",
      "    \"\"\"Gets the software name and returns the path of the binary.\"\"\"\n",
      "    if executable is None:\n",
      "        return None\n",
      "    if not os.path.exists(executable):\n",
      "        logging.error(f'{executable} does not exist.')\n",
      "        return None\n",
      "    if not os.path.isfile(executable):\n",
      "        logging.error(f'{executable} is not a file.')\n",
      "        return None\n",
      "    if not os.access(executable, os.X_OK):\n",
      "        logging.error(f'{executable} is not executable.')\n",
      "        return None\n",
      "    if not executable.endswith('.exe'):\n",
      "        logging.error(f'{executable} is not an executable.')\n",
      "        return None\n",
      "    if not executable.startswith('/'):\n",
      "        logging.error(f'{executable} is not an executable.')\n",
      "        return None\n",
      "    return executable\n",
      "==========score: 1.0===============\n",
      "def heatmap(x, y, step=None, min_pt=None, max_pt=None,\n",
      "                 colormap='Blues', alpha=1, grid=False,\n",
      "                 colorbar=True, scale='lin',\n",
      "                 vmax='auto', vmin='auto', crop=True):\n",
      "    \"\"\"\n",
      "    function to take vectors x and y and hist them\n",
      "    \"\"\"\n",
      "    if min_pt is None:\n",
      "        min_pt = np.min(x)\n",
      "    if max_pt is None:\n",
      "        max_pt = np.max(x)\n",
      "    if step is None:\n",
      "        step = (max_pt - min_pt) / 100\n",
      "    if vmax == 'auto':\n",
      "        vmax = max_pt\n",
      "    if vmin == 'auto':\n",
      "        vmin = min_pt\n",
      "    if grid:\n",
      "        plt.grid(True)\n",
      "    if colorbar:\n",
      "        plt.colorbar()\n",
      "    plt.imshow(x, cmap=colormap, alpha=alpha,\n",
      "               vmin=vmin, vmax=vmax, extent=[min_pt, max_pt, min_pt, max_pt])\n",
      "    plt.xlabel(x.name)\n",
      "    plt.ylabel(y.name)\n",
      "    plt.title(x.name +'vs'+ y.name)\n",
      "    if crop:\n",
      "        plt.xlim(min_pt, max_pt)\n",
      "        plt.ylim(min_pt, max\n",
      "==========score: 1.0===============\n",
      "def sample_lists(items_list, num=1, seed=None):\n",
      "    r\"\"\"\n",
      "    Args:\n",
      "        items_list (list):\n",
      "        num (int): (default = 1)\n",
      "        seed (None): (default = None)\n",
      "\n",
      "    Returns:\n",
      "        list: samples_list\n",
      "\n",
      "    CommandLine:\n",
      "        python -m utool.util_list --exec-sample_lists\n",
      "\n",
      "    Example:\n",
      "        >>> # DISABLE_DOCTEST\n",
      "        >>> from utool.util_list import *  # NOQA\n",
      "        >>> items_list = [[], [1, 2, 3], [4], [5, 6], [7, 8, 9, 10]]\n",
      "        >>> num = 2\n",
      "        >>> seed = 0\n",
      "        >>> samples_list = sample_lists(items_list, num, seed)\n",
      "        >>> result = ('samples_list = %s' % (str(samples_list),))\n",
      "        >>> print(result)\n",
      "        samples_list = [[], [3, 2], [4], [5, 6], [10, 9]]\n",
      "    \"\"\"\n",
      "    if seed is not None:\n",
      "        np.random.seed(seed)\n",
      "    if num == 1:\n",
      "        return items_list\n",
      "    if num > len(items_list):\n",
      "        raise Exception('num > len(items_list)')\n",
      "    if num < 1:\n",
      "        raise Exception('num < 1')\n",
      "    return_list = []\n",
      "    for _ in range(num):\n",
      "        return_list.append(np.random.choice(items_list))\n",
      "    return return_list\n",
      "==========score: 1.0===============\n",
      "def _dt_to_epoch(self, dt):\n",
      "        \"\"\"\n",
      "        Convert a offset-aware datetime to POSIX time.\n",
      "        \"\"\"\n",
      "        return (dt - self._dtstart).total_seconds()\n",
      "\n",
      "    def _parse_rfc(self, s):\n",
      "        lines = s.splitlines()\n",
      "        if not lines:\n",
      "            raise ValueError(\"empty string\")\n",
      "\n",
      "        # Unfold\n",
      "        i = 0\n",
      "        while i < len(lines):\n",
      "            line = lines[i].rstrip()\n",
      "            if not line:\n",
      "                del lines[i]\n",
      "            elif i > 0 and line[0] == \" \":\n",
      "                lines[i-1] += line[1:]\n",
      "                del lines[i]\n",
      "            else:\n",
      "                i += 1\n",
      "\n",
      "        tzid = None\n",
      "        comps = []\n",
      "        invtz = False\n",
      "        comptype = None\n",
      "        for line in lines:\n",
      "            if not line:\n",
      "                continue\n",
      "            name, value = line.split(':', 1)\n",
      "            parms = name.split(';')\n",
      "            if not parms:\n",
      "                raise ValueError(\"empty property name\")\n",
      "            name = parms[0].upper()\n",
      "            parms = parms[1:]\n",
      "            if invtz:\n",
      "                if name == \"\n",
      "==========score: 1.0===============\n",
      "def register_work(self, work, deps=None, manager=None, workdir=None):\n",
      "        \"\"\"\n",
      "        Register a new :class:`Work` and add it to the internal list, taking into account possible dependencies.\n",
      "\n",
      "        Args:\n",
      "            work: :class:`Work` object.\n",
      "            deps: List of :class:`Dependency` objects specifying the dependency of this node.\n",
      "                  An empy list of deps implies that this node has no dependencies.\n",
      "            manager: The :class:`TaskManager` responsible for the submission of the task.\n",
      "                     If manager is None, we use the `TaskManager` specified during the creation of the work.\n",
      "            workdir: The name of the directory used for the :class:`Work`.\n",
      "\n",
      "        Returns:\n",
      "            The registered :class:`Work`.\n",
      "        \"\"\"\n",
      "        if deps is None:\n",
      "            deps = []\n",
      "        if manager is None:\n",
      "            manager = self.manager\n",
      "        if workdir is None:\n",
      "            workdir = self.workdir\n",
      "        work = Work(work, deps, manager, workdir)\n",
      "        self.work_list.append(work)\n",
      "        return work\n",
      "\n",
      "    def register_workdir(self, workdir):\n",
      "        \"\"\"\n",
      "        Register a new :class:`WorkDir` and add it to the internal list.\n",
      "\n",
      "        Args:\n",
      "            workdir: :class:`WorkDir` object.\n",
      "\n",
      "        Returns:\n",
      "            The registered :class:`WorkDir`.\n",
      "        \"\"\"\n",
      "        workdir = WorkDir(workdir)\n",
      "        self.workdir_list.append(workdir)\n",
      "        return workdir\n",
      "\n",
      "    def register_task(self, task, manager=None, workdir=None):\n",
      "        \"\"\"\n",
      "        Register a new :class:`Task` and add it to the internal list.\n",
      "\n",
      "        Args:\n",
      "            task: :class:`Task` object.\n",
      "            manager: The :class:`TaskManager` responsible for the submission of the task\n",
      "==========score: 1.0===============\n",
      "def create_cookie(\n",
      "        key: str,\n",
      "        value: str='',\n",
      "        max_age: Optional[Union[int, timedelta]]=None,\n",
      "        expires: Optional[Union[int, float, datetime]]=None,\n",
      "        path: str='/',\n",
      "        domain: Optional[str]=None,\n",
      "        secure: bool=False,\n",
      "        httponly: bool=False,\n",
      ") -> SimpleCookie:\n",
      "    \"\"\"Create a Cookie given the options set\n",
      "\n",
      "    The arguments are the standard cookie morsels and this is a\n",
      "    wrapper around the stdlib SimpleCookie code.\n",
      "    \"\"\"\n",
      "    # SimpleCookie expects the content of the Morsel object as a string,\n",
      "    # not unicode. We need to encode it.\n",
      "    morsel_value = value.encode() if isinstance(value, text_type) else value\n",
      "\n",
      "    cookie = SimpleCookie()\n",
      "    cookie[key] = morsel_value\n",
      "    cookie[key]['max-age'] = max_age if max_age is not None else -1\n",
      "    cookie[key]['expires'] = expires if expires is not None else -1\n",
      "    cookie[key]['path'] = path\n",
      "    if domain is not None:\n",
      "        cookie[key]['domain'] = domain\n",
      "    if secure:\n",
      "        cookie[key]['secure'] = True\n",
      "    if httponly:\n",
      "        if not cookie[key]._header_value:\n",
      "            cookie[key]['httponly'] = True\n",
      "        else:\n",
      "            cookie[key]['httponly'] = False\n",
      "\n",
      "    return cookie\n",
      "==========score: 1.0===============\n",
      "def _hexvalue_to_rgb(hexvalue):\n",
      "        \"\"\"\n",
      "        Converts the hexvalue used by tuya for colour representation into\n",
      "        an RGB value.\n",
      "        \n",
      "        Args:\n",
      "            hexvalue(string): The hex representation generated by BulbDevice._rgb_to_hexvalue()\n",
      "        \"\"\"\n",
      "        hexvalue = hexvalue.lstrip('#')\n",
      "        return tuple(int(hexvalue[i:i+2], 16) for i in (0, 2, 4))\n",
      "\n",
      "    def _rgb_to_hexvalue(self, rgb):\n",
      "        \"\"\"\n",
      "        Converts an RGB value into a hex representation used by tuya for\n",
      "        colour representation.\n",
      "        \n",
      "        Args:\n",
      "            rgb(tuple): The RGB value to convert\n",
      "        \"\"\"\n",
      "        return '#{:02x}{:02x}{:02x}'.format(rgb[0], rgb[1], rgb[2])\n",
      "\n",
      "    def _hexvalue_to_rgb(self, hexvalue):\n",
      "        \"\"\"\n",
      "        Converts a hex representation used by tuya for colour representation\n",
      "        into an RGB value.\n",
      "        \n",
      "        Args:\n",
      "            hexvalue(string): The hex representation generated by BulbDevice._rgb_to_hexvalue()\n",
      "        \"\"\"\n",
      "        hexvalue = hexvalue.lstrip('#')\n",
      "        return tuple(int(hexvalue[i:i+2], 16) for i in (0, 2, 4))\n",
      "\n",
      "    def _\n",
      "==========score: 1.0===============\n",
      "def distinct_permutations(iterable):\n",
      "    \"\"\"Yield successive distinct permutations of the elements in *iterable*.\n",
      "\n",
      "        >>> sorted(distinct_permutations([1, 0, 1]))\n",
      "        [(0, 1, 1), (1, 0, 1), (1, 1, 0)]\n",
      "\n",
      "    Equivalent to ``set(permutations(iterable))``, except duplicates are not\n",
      "    generated and thrown away. For larger input sequences this is much more\n",
      "    efficient.\n",
      "\n",
      "    Duplicate permutations arise when there are duplicated elements in the\n",
      "    input iterable. The number of items returned is\n",
      "    `n! / (x_1! * x_2! * ... * x_n!)`, where `n` is the total number of\n",
      "    items input, and each `x_i` is the count of a distinct item in the input\n",
      "    sequence.\n",
      "\n",
      "    \"\"\"\n",
      "    def make_new_permutations(permutations, e):\n",
      "        yield from permutations\n",
      "        for p in permutations:\n",
      "            if p[e] > 0:\n",
      "                p[e] -= 1\n",
      "                yield p\n",
      "\n",
      "    permutations = [list(p) for p in permutations]\n",
      "    for e in range(len(permutations[0])):\n",
      "        yield from make_new_permutations(permutations, e)\n",
      "==========score: 1.0===============\n",
      "def make_dict_observable(matrix_observable):\n",
      "    \"\"\"Convert an observable in matrix form to dictionary form.\n",
      "\n",
      "    Takes in a diagonal observable as a matrix and converts it to a dictionary\n",
      "    form. Can also handle a list sorted of the diagonal elements.\n",
      "\n",
      "    Args:\n",
      "        matrix_observable (list): The observable to be converted to dictionary\n",
      "        form. Can be a matrix or just an ordered list of observed values\n",
      "\n",
      "    Returns:\n",
      "        Dict: A dictionary with all observable states as keys, and corresponding\n",
      "        values being the observed value for that state\n",
      "    \"\"\"\n",
      "    if isinstance(matrix_observable, list):\n",
      "        return {i: matrix_observable[i] for i in range(len(matrix_observable))}\n",
      "    else:\n",
      "        return {i: matrix_observable[i, i] for i in range(len(matrix_observable))}\n",
      "==========score: 1.0===============\n",
      "def permutations(x):\n",
      "    '''Given a listlike, x, return all permutations of x\n",
      "\n",
      "    Returns the permutations of x in the lexical order of their indices:\n",
      "    e.g.\n",
      "    >>> x = [ 1, 2, 3, 4 ]\n",
      "    >>> for p in permutations(x):\n",
      "    >>>   print p\n",
      "    [ 1, 2, 3, 4 ]\n",
      "    [ 1, 2, 4, 3 ]\n",
      "    [ 1, 3, 2, 4 ]\n",
      "    [ 1, 3, 4, 2 ]\n",
      "    [ 1, 4, 2, 3 ]\n",
      "    [ 1, 4, 3, 2 ]\n",
      "    [ 2, 1, 3, 4 ]\n",
      "    ...\n",
      "    [ 4, 3, 2, 1 ]\n",
      "    '''\n",
      "    if len(x) <= 1:\n",
      "        yield x\n",
      "    else:\n",
      "        for i in range(len(x)):\n",
      "            for p in permutations(x[:i] + x[i+1:]):\n",
      "                yield [x[i]] + p\n",
      "==========score: 1.0===============\n",
      "def timer(fn, miniter=3, minwall=3.0):\n",
      "    \"\"\"Runs fn() multiple times and returns the results.\n",
      "\n",
      "    Runs for at least ``miniter`` iterations and ``minwall`` wall time.\n",
      "    \"\"\"\n",
      "    wall = 0.0\n",
      "    results = []\n",
      "    for i in range(miniter):\n",
      "        start = time.time()\n",
      "        fn()\n",
      "        end = time.time()\n",
      "        wall += end - start\n",
      "        if i >= minwall:\n",
      "            results.append(wall)\n",
      "            wall = 0.0\n",
      "    return results\n",
      "==========score: 1.0===============\n",
      "def to_excel(self, *args):\r\n",
      "        \"\"\"\r\n",
      "        Dump all the data to excel, fname and path can be passed as args\r\n",
      "        \"\"\"\r\n",
      "        if len(args) == 0:\r\n",
      "            fname = self.fname\r\n",
      "            path = self.path\r\n",
      "        else:\r\n",
      "            fname = args[0]\r\n",
      "            path = args[1]\r\n",
      "        if not os.path.exists(path):\r\n",
      "            os.makedirs(path)\r\n",
      "        writer = pd.ExcelWriter(os.path.join(path, fname), engine='xlsxwriter')\r\n",
      "        self.df.to_excel(writer, sheet_name=self.sheet_name)\r\n",
      "        writer.save()\r\n",
      "\r\n",
      "    def to_csv(self, *args):\r\n",
      "        \"\"\"\r\n",
      "        Dump all the data to csv, fname and path can be passed as args\r\n",
      "        \"\"\"\r\n",
      "        if len(args) == 0:\r\n",
      "            fname = self.fname\r\n",
      "            path = self.path\r\n",
      "        else:\r\n",
      "            fname = args[0]\r\n",
      "            path = args[1]\r\n",
      "        if not os.path.exists(path):\r\n",
      "            os.m\n",
      "==========score: 1.0===============\n",
      "def _from_binary_reparse(cls, binary_stream):\n",
      "    \"\"\"See base class.\"\"\"\n",
      "    return cls.from_binary_reparse(binary_stream)\n",
      "\n",
      "  def _to_binary_reparse(self):\n",
      "    \"\"\"See base class.\"\"\"\n",
      "    return self.to_binary_reparse()\n",
      "\n",
      "  def _from_binary_reparse(self, binary_stream):\n",
      "    \"\"\"See base class.\"\"\"\n",
      "    return self.from_binary_reparse(binary_stream)\n",
      "\n",
      "  def _to_binary_reparse(self):\n",
      "    \"\"\"See base class.\"\"\"\n",
      "    return self.to_binary_reparse()\n",
      "\n",
      "  def _from_binary_reparse(self, binary_stream):\n",
      "    \"\"\"See base class.\"\"\"\n",
      "    return self.from_binary_reparse(binary_stream)\n",
      "\n",
      "  def _to_binary_reparse(self):\n",
      "    \"\"\"See base class.\"\"\"\n",
      "    return self.to_binary_reparse()\n",
      "\n",
      "  def _from_binary_reparse(self, binary_stream):\n",
      "    \"\"\"See base class.\"\"\"\n",
      "    return self.from_binary_reparse(binary_stream)\n",
      "\n",
      "  def _to_binary_reparse(self):\n",
      "    \"\"\"See\n",
      "==========score: 1.0===============\n",
      "def set_checkbox(self, data, uncheck_other_boxes=True):\n",
      "        \"\"\"Set the *checked*-attribute of input elements of type \"checkbox\"\n",
      "        specified by ``data`` (i.e. check boxes).\n",
      "\n",
      "        :param data: Dict of ``{name: value, ...}``.\n",
      "            In the family of checkboxes whose *name*-attribute is ``name``,\n",
      "            check the box whose *value*-attribute is ``value``. All boxes in\n",
      "            the family can be checked (unchecked) if ``value`` is True (False).\n",
      "            To check multiple specific boxes, let ``value`` be a tuple or list.\n",
      "        :param uncheck_other_boxes: If True (default), before checking any\n",
      "            boxes specified by ``data``, uncheck the entire checkbox family.\n",
      "            Consider setting to False if some boxes are checked by default when\n",
      "            the HTML is served.\n",
      "        \"\"\"\n",
      "        for name, value in data.items():\n",
      "            if isinstance(value, (tuple, list)):\n",
      "                for v in value:\n",
      "                    self.set_checkbox(name, v, uncheck_other_boxes)\n",
      "            else:\n",
      "                self.set_checkbox(name, value, uncheck_other_boxes)\n",
      "\n",
      "    def set_checkbox(self, name, value, uncheck_other_boxes=True):\n",
      "        \"\"\"Set the *checked*-attribute of input elements of type \"checkbox\"\n",
      "        specified by ``name`` and ``value``.\n",
      "\n",
      "        :param name: Name of the input element whose *value*-attribute is\n",
      "            being set.\n",
      "        :param value: True (1) or False (0) or a tuple or list of those.\n",
      "        :param uncheck_other_boxes: If True (default), before checking any\n",
      "            boxes specified by ``data``, uncheck the entire checkbox family.\n",
      "            Consider setting to False if some boxes are checked by default when\n",
      "            the HTML is served.\n",
      "        \"\"\"\n",
      "        if uncheck_other_boxes:\n",
      "            self.uncheck_all_\n",
      "==========score: 1.0===============\n",
      "def to_excel(self, *args):\r\n",
      "        \"\"\"\r\n",
      "        Dump all the data to excel, fname and path can be passed as args\r\n",
      "        \"\"\"\r\n",
      "        if len(args) == 0:\r\n",
      "            fname = self.fname\r\n",
      "            path = self.path\r\n",
      "        else:\r\n",
      "            fname = args[0]\r\n",
      "            path = args[1]\r\n",
      "        if not os.path.exists(path):\r\n",
      "            os.mkdir(path)\r\n",
      "        writer = pd.ExcelWriter(os.path.join(path, fname), engine='xlsxwriter')\r\n",
      "        self.df.to_excel(writer, sheet_name='Sheet1')\r\n",
      "        writer.save()\r\n",
      "\r\n",
      "    def to_csv(self, *args):\r\n",
      "        \"\"\"\r\n",
      "        Dump all the data to csv, fname and path can be passed as args\r\n",
      "        \"\"\"\r\n",
      "        if len(args) == 0:\r\n",
      "            fname = self.fname\r\n",
      "            path = self.path\r\n",
      "        else:\r\n",
      "            fname = args[0]\r\n",
      "            path = args[1]\r\n",
      "        if not os.path.exists(path):\r\n",
      "            os.mkdir(path\n",
      "==========score: 1.0===============\n",
      "def get_pid(self):\n",
      "        \"\"\"\n",
      "        @rtype:  int\n",
      "        @return: Parent process global ID.\n",
      "\n",
      "        @raise WindowsError: An error occured when calling a Win32 API function.\n",
      "        @raise RuntimeError: The parent process ID can't be found.\n",
      "        \"\"\"\n",
      "        hProcess = self.get_handle(win32.PROCESS_QUERY_INFORMATION)\n",
      "        return win32.GetProcessId(hProcess)\n",
      "\n",
      "    def get_ppid(self):\n",
      "        \"\"\"\n",
      "        @rtype:  int\n",
      "        @return: Parent process global ID.\n",
      "\n",
      "        @raise WindowsError: An error occured when calling a Win32 API function.\n",
      "        @raise RuntimeError: The parent process ID can't be found.\n",
      "        \"\"\"\n",
      "        hProcess = self.get_handle(win32.PROCESS_QUERY_INFORMATION)\n",
      "        return win32.GetParentProcessId(hProcess)\n",
      "\n",
      "    def is_alive(self):\n",
      "        \"\"\"\n",
      "        @rtype:  bool\n",
      "        @return: Returns C{True} if the process is currently running.\n",
      "        @raise WindowsError: An error occured when calling a Win32 API function.\n",
      "        @raise RuntimeError: The process ID can't be found.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            win32.WaitForSingleObject(self.get_handle(win32.SYNCHRONIZE), 0)\n",
      "        except WindowsError:\n",
      "==========score: 1.0===============\n",
      "def fromHTML(html, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Creates abstraction using HTML\n",
      "\n",
      "        :param str html: HTML\n",
      "        :return: TreeOfContents object\n",
      "        \"\"\"\n",
      "        return TreeOfContents.fromHTML(html, *args, **kwargs)\n",
      "\n",
      "    @classmethod\n",
      "    def fromXML(cls, xml, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Creates abstraction using XML\n",
      "\n",
      "        :param str xml: XML\n",
      "        :return: TreeOfContents object\n",
      "        \"\"\"\n",
      "        return TreeOfContents.fromXML(xml, *args, **kwargs)\n",
      "\n",
      "    def toXML(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Saves the tree of contents to an XML\n",
      "\n",
      "        :return: XML\n",
      "        \"\"\"\n",
      "        return self.__super.toXML(*args, **kwargs)\n",
      "\n",
      "    def toHTML(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Saves the tree of contents to an HTML\n",
      "\n",
      "        :return: HTML\n",
      "        \"\"\"\n",
      "        return self.__super.toHTML(*args, **kwargs)\n",
      "\n",
      "    def toString(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Returns the tree of contents as a string\n",
      "\n",
      "        :return: String\n",
      "        \"\"\"\n",
      "        return self.__super.toString(*args, **\n",
      "==========score: 1.0===============\n",
      "def get_process_info(self, pid=None):\n",
      "        '''\n",
      "        get_process_info(self, pid=None)\n",
      "\n",
      "        Get process general information.\n",
      "\n",
      "        :Parameters:\n",
      "        * *pid* (`string`) -- Identifier of an existing process\n",
      "\n",
      "        '''\n",
      "        if pid is None:\n",
      "            pid = self.pid\n",
      "        if pid is None:\n",
      "            raise ValueError('pid is None')\n",
      "        return self.get_process(pid).get_process_info()\n",
      "\n",
      "    def get_process_memory_info(self, pid=None):\n",
      "        '''\n",
      "        get_process_memory_info(self, pid=None)\n",
      "\n",
      "        Get process memory information.\n",
      "\n",
      "        :Parameters:\n",
      "        * *pid* (`string`) -- Identifier of an existing process\n",
      "\n",
      "        '''\n",
      "        if pid is None:\n",
      "            pid = self.pid\n",
      "        if pid is None:\n",
      "            raise ValueError('pid is None')\n",
      "        return self.get_process(pid).get_process_memory_info()\n",
      "\n",
      "    def get_process_io_counters(self, pid=None):\n",
      "        '''\n",
      "        get_process_io_counters(self, pid=None)\n",
      "\n",
      "        Get process I/O information.\n",
      "\n",
      "        :Parameters:\n",
      "        * *pid* (`string`) -- Identifier of an existing process\n",
      "\n",
      "        '''\n",
      "        if pid is None:\n",
      "            pid = self\n",
      "==========score: 1.0===============\n",
      "def heatmap(self, partition=None, cmap=CM.Blues):\n",
      "        \"\"\" Plots a visual representation of a distance matrix \"\"\"\n",
      "        if partition is None:\n",
      "            partition = self.partition\n",
      "        if partition is None:\n",
      "            raise ValueError(\"No partition was provided\")\n",
      "        if not isinstance(partition, np.ndarray):\n",
      "            partition = np.array(partition)\n",
      "        if partition.shape[0]!= partition.shape[1]:\n",
      "            raise ValueError(\"Partition must be square\")\n",
      "        if partition.shape[0]!= self.n_samples:\n",
      "            raise ValueError(\"Partition must have the same number of samples as the distance matrix\")\n",
      "        if not isinstance(cmap, matplotlib.colors.Colormap):\n",
      "            raise ValueError(\"cmap must be a matplotlib colormap\")\n",
      "        if not isinstance(cmap.colors, list):\n",
      "            raise ValueError(\"cmap must be a matplotlib colormap with colors\")\n",
      "        if len(cmap.colors)!= partition.shape[0]:\n",
      "            raise ValueError(\"cmap must have the same number of colors as the partition\")\n",
      "        if len(cmap.colors)!= len(set(cmap.\n",
      "==========score: 1.0===============\n",
      "def get_pid(self):\n",
      "        \"\"\"\n",
      "        @rtype:  int\n",
      "        @return: Parent process global ID.\n",
      "\n",
      "        @raise WindowsError: An error occured when calling a Win32 API function.\n",
      "        @raise RuntimeError: The parent process ID can't be found.\n",
      "        \"\"\"\n",
      "        hProcess = self.get_handle(win32.PROCESS_QUERY_INFORMATION)\n",
      "        return win32.GetProcessId(hProcess)\n",
      "\n",
      "    def get_parent_process(self):\n",
      "        \"\"\"\n",
      "        @rtype:  L{Process}\n",
      "        @return: Parent process object.\n",
      "\n",
      "        @raise WindowsError: An error occured when calling a Win32 API function.\n",
      "        @raise RuntimeError: The parent process can't be found.\n",
      "        \"\"\"\n",
      "        if not self.is_alive():\n",
      "            raise RuntimeError('Process is not started!')\n",
      "\n",
      "        from process import Process\n",
      "        handle = self.get_handle(win32.PROCESS_QUERY_INFORMATION)\n",
      "        return Process(pid=self.get_pid(), handle=handle)\n",
      "\n",
      "    def get_ppid(self):\n",
      "        \"\"\"\n",
      "        @rtype:  int\n",
      "        @return: Parent process global ID.\n",
      "\n",
      "        @raise WindowsError: An error occured when calling a Win32 API function.\n",
      "        @raise RuntimeError: The parent process ID can't be found.\n",
      "        \"\"\"\n",
      "        hProcess = self.get_handle\n",
      "==========score: 1.0===============\n",
      "def groupby_count(i, key=None, force_keys=None):\n",
      "    \"\"\" Aggregate iterator values into buckets based on how frequently the\n",
      "    values appear.\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> list(groupby_count([1, 1, 1, 2, 3]))\n",
      "        [(1, 3), (2, 1), (3, 1)]\n",
      "    \"\"\"\n",
      "    if key is None:\n",
      "        key = lambda x: x\n",
      "    if force_keys is None:\n",
      "        force_keys = []\n",
      "    return groupby(i, key=key, force_keys=force_keys)\n",
      "==========score: 1.0===============\n",
      "def distinct_permutations(iterable):\n",
      "    \"\"\"Yield successive distinct permutations of the elements in *iterable*.\n",
      "\n",
      "        >>> sorted(distinct_permutations([1, 0, 1]))\n",
      "        [(0, 1, 1), (1, 0, 1), (1, 1, 0)]\n",
      "\n",
      "    Equivalent to ``set(permutations(iterable))``, except duplicates are not\n",
      "    generated and thrown away. For larger input sequences this is much more\n",
      "    efficient.\n",
      "\n",
      "    Duplicate permutations arise when there are duplicated elements in the\n",
      "    input iterable. The number of items returned is\n",
      "    `n! / (x_1! * x_2! * ... * x_n!)`, where `n` is the total number of\n",
      "    items input, and each `x_i` is the count of a distinct item in the input\n",
      "    sequence.\n",
      "\n",
      "    \"\"\"\n",
      "    def make_new_permutations(permutations, e):\n",
      "        yield from permutations\n",
      "        for p in permutations:\n",
      "            if p.startswith(e):\n",
      "                yield p\n",
      "    permutations = [()]\n",
      "    for e in iterable:\n",
      "        permutations = make_new_permutations(permutations, e)\n",
      "    return permutations\n",
      "==========score: 1.0===============\n",
      "def fromHTML(html, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Creates abstraction using HTML\n",
      "\n",
      "        :param str html: HTML\n",
      "        :return: TreeOfContents object\n",
      "        \"\"\"\n",
      "        return TreeOfContents.fromHTML(html, *args, **kwargs)\n",
      "\n",
      "    @classmethod\n",
      "    def fromXML(cls, xml, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Creates abstraction using XML\n",
      "\n",
      "        :param str xml: XML\n",
      "        :return: TreeOfContents object\n",
      "        \"\"\"\n",
      "        return TreeOfContents.fromXML(xml, *args, **kwargs)\n",
      "\n",
      "    @classmethod\n",
      "    def fromYAML(cls, yaml, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Creates abstraction using YAML\n",
      "\n",
      "        :param str yaml: YAML\n",
      "        :return: TreeOfContents object\n",
      "        \"\"\"\n",
      "        return TreeOfContents.fromYAML(yaml, *args, **kwargs)\n",
      "\n",
      "    def toXML(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Serializes the object to XML\n",
      "\n",
      "        :returns: XML representation of the object\n",
      "        :rtype: `lxml.etree.Element`\n",
      "        \"\"\"\n",
      "        return self.__toXML(*args, **kwargs)\n",
      "\n",
      "    def toHTML(self,\n",
      "==========score: 1.0===============\n",
      "def epoch(self):\n",
      "        \"\"\"\n",
      "        Returns the total seconds since epoch associated with\n",
      "        the Delorean object.\n",
      "\n",
      "        .. testsetup::\n",
      "\n",
      "            from datetime import datetime\n",
      "            from delorean import Delorean\n",
      "\n",
      "        .. doctest::\n",
      "\n",
      "            >>> d = Delorean(datetime(2015, 1, 1), timezone='US/Pacific')\n",
      "            >>> d.epoch\n",
      "            1420099200.0\n",
      "\n",
      "        \"\"\"\n",
      "        return self._epoch\n",
      "\n",
      "    def now(self, tz=None):\n",
      "        \"\"\"\n",
      "        Returns a new Delorean object with the current date and time\n",
      "        in the specified timezone.\n",
      "\n",
      "        :param tz: (optional) A timezone string.  Defaults to the timezone\n",
      "                   associated with the Delorean object.\n",
      "\n",
      "       .. testsetup::\n",
      "\n",
      "            from datetime import datetime\n",
      "            from delorean import Delorean\n",
      "\n",
      "       .. doctest::\n",
      "\n",
      "            >>> d = Delorean(datetime(2015, 1, 1), timezone='US/Pacific')\n",
      "            >>> d.now()\n",
      "            <Delorean 2020-01-01 00:00:00+00:00>\n",
      "\n",
      "        \"\"\"\n",
      "        if tz is None:\n",
      "            tz = self.timezone\n",
      "\n",
      "        return Delorean(datetime.now(pytz.timezone(tz)), timezone=tz)\n",
      "\n",
      "    def utcnow(self):\n",
      "        \"\"\"\n",
      "        Returns a new Delorean object with the current date and time\n",
      "        in UTC.\n",
      "\n",
      "       .. testsetup::\n",
      "\n",
      "            from datetime import datetime\n",
      "            from delorean import Delorean\n",
      "\n",
      "       .. doctest::\n",
      "==========score: 1.0===============\n",
      "def set_working_dir(self, working_dir):\n",
      "        \"\"\"\n",
      "        Sets the working directory for this hypervisor.\n",
      "\n",
      "        :param working_dir: path to the working directory\n",
      "        \"\"\"\n",
      "        self._working_dir = working_dir\n",
      "\n",
      "    def get_working_dir(self):\n",
      "        \"\"\"\n",
      "        Gets the working directory for this hypervisor.\n",
      "\n",
      "        :return: path to the working directory\n",
      "        \"\"\"\n",
      "        return self._working_dir\n",
      "\n",
      "    def set_hostname(self, hostname):\n",
      "        \"\"\"\n",
      "        Sets the hostname for this hypervisor.\n",
      "\n",
      "        :param hostname: hostname\n",
      "        \"\"\"\n",
      "        self._hostname = hostname\n",
      "\n",
      "    def get_hostname(self):\n",
      "        \"\"\"\n",
      "        Gets the hostname for this hypervisor.\n",
      "\n",
      "        :return: hostname\n",
      "        \"\"\"\n",
      "        return self._hostname\n",
      "\n",
      "    def set_ip(self, ip):\n",
      "        \"\"\"\n",
      "        Sets the IP address for this hypervisor.\n",
      "\n",
      "        :param ip: IP address\n",
      "        \"\"\"\n",
      "        self._ip = ip\n",
      "\n",
      "    def get_ip(self):\n",
      "        \"\"\"\n",
      "        Gets the IP address for this hypervisor.\n",
      "\n",
      "        :return: IP address\n",
      "        \"\"\"\n",
      "        return self._ip\n",
      "\n",
      "    def set_port(self, port):\n",
      "        \"\"\"\n",
      "        Sets the port for this hyper\n",
      "==========score: 1.0===============\n",
      "def scatter_plot(self, ax, topic_dims, t=None, ms_limits=True, **kwargs_plot):\n",
      "        \"\"\" 2D or 3D scatter plot.\n",
      "\n",
      "            :param axes ax: matplotlib axes (use Axes3D if 3D data)\n",
      "\n",
      "            :param tuple topic_dims: list of (topic, dims) tuples, where topic is a string and dims is a list of dimensions to be plotted for that topic.\n",
      "\n",
      "            :param int t: time indexes to be plotted\n",
      "\n",
      "            :param dict kwargs_plot: argument to be passed to matplotlib's plot function, e.g. the style of the plotted points 'or'\n",
      "\n",
      "            :param bool ms_limits: if set to True, automatically set axes boundaries to the sensorimotor boundaries (default: True)\n",
      "        \"\"\"\n",
      "        if self.data.ndim == 2:\n",
      "            self.scatter_plot_2d(ax, topic_dims, t, ms_limits, **kwargs_plot)\n",
      "        elif self.data.ndim == 3:\n",
      "            self.scatter_plot_3d(ax, topic_dims, t, ms_limits, **kwargs_plot)\n",
      "        else:\n",
      "            raise ValueError('Data must be 2D or 3D')\n",
      "\n",
      "    def scatter_plot_2d(self, ax, topic_dims, t=None, ms_limits=True, **kwargs_plot):\n",
      "        \"\"\" 2D scatter plot.\n",
      "\n",
      "            :param axes ax: matplotlib axes (use Axes3D if 3D data)\n",
      "\n",
      "            :param tuple topic_dims: list of (topic, dims) tuples, where topic is a string and dims is a list of dimensions to be plotted for that topic.\n",
      "\n",
      "            :param int t: time indexes to be plotted\n",
      "\n",
      "            :param dict kwargs_plot: argument to be passed to matplotlib's plot function, e.g. the style of the plotted\n",
      "==========score: 1.0===============\n",
      "def to_uint8(self):\n",
      "        \"\"\"\n",
      "        Convert this heatmaps object to a 0-to-255 array.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        arr_uint8 : (H,W,C) ndarray\n",
      "            Heatmap as a 0-to-255 array (dtype is uint8).\n",
      "\n",
      "        \"\"\"\n",
      "        return self.to_float().to_uint8()\n",
      "\n",
      "    def to_float(self):\n",
      "        \"\"\"\n",
      "        Convert this heatmaps object to a 0.0 to 1.0 float array.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        arr_float : (H,W,C) ndarray\n",
      "            Heatmap as a 0.0 to 1.0 float array (dtype is float).\n",
      "\n",
      "        \"\"\"\n",
      "        arr_float = self.arr.astype(np.float32)\n",
      "        if self.keypoints is not None:\n",
      "            arr_float[..., 2] -= self.keypoints.min(axis=0)[0]\n",
      "            arr_float[..., 3] -= self.keypoints.min(axis=0)[1]\n",
      "        return arr_float\n",
      "\n",
      "    def to_image(self, source_format='rgb'):\n",
      "        \"\"\"\n",
      "        Convert this heatmaps object to an RGB image array.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        source_format : str\n",
      "            The source format of the heatmaps. Can be either 'rgb' or 'lab'.\n",
      "            Defaults to 'rgb'.\n",
      "\n",
      "        Returns\n",
      "\n",
      "==========score: 1.0===============\n",
      "def parse_rc_json():\n",
      "    \"\"\" Reads the json configuration file(.yasirc.json), parses it and returns the\n",
      "    dictionary\n",
      "    \"\"\"\n",
      "    with open(CONFIG_FILE, 'r') as f:\n",
      "        config = json.load(f)\n",
      "    return config\n",
      "==========score: 1.0===============\n",
      "def sort(self, ids):\n",
      "        \"\"\"\n",
      "        Sort the given list of identifiers,\n",
      "        returning a new (sorted) list.\n",
      "\n",
      "        :param list ids: the list of identifiers to be sorted\n",
      "        :rtype: list\n",
      "        \"\"\"\n",
      "        return ids\n",
      "\n",
      "    def get_sorted_ids(self, ids):\n",
      "        \"\"\"\n",
      "        Sort the given list of identifiers,\n",
      "        returning a new (sorted) list.\n",
      "\n",
      "        :param list ids: the list of identifiers to be sorted\n",
      "        :rtype: list\n",
      "        \"\"\"\n",
      "        return ids\n",
      "\n",
      "    def get_sorted_values(self, values):\n",
      "        \"\"\"\n",
      "        Sort the given list of values,\n",
      "        returning a new (sorted) list.\n",
      "\n",
      "        :param list values: the list of values to be sorted\n",
      "        :rtype: list\n",
      "        \"\"\"\n",
      "        return values\n",
      "\n",
      "    def get_sorted_dict(self, values):\n",
      "        \"\"\"\n",
      "        Sort the given list of values,\n",
      "        returning a new (sorted) list.\n",
      "\n",
      "        :param list values: the list of values to be sorted\n",
      "        :rtype: list\n",
      "        \"\"\"\n",
      "        return values\n",
      "\n",
      "    def get_sorted_dict_by_key(self, values, key):\n",
      "        \"\"\"\n",
      "        Sort the given list of dictionaries by\n",
      "        the given key, returning a new (sorted) list.\n",
      "\n",
      "        \n",
      "==========score: 1.0===============\n",
      "def fit_model(regressor_type,\n",
      "              regressor_kwargs,\n",
      "              tf_matrix,\n",
      "              target_gene_expression,\n",
      "              early_stop_window_length=EARLY_STOP_WINDOW_LENGTH,\n",
      "              seed=DEMON_SEED):\n",
      "    \"\"\"\n",
      "    :param regressor_type: string. Case insensitive.\n",
      "    :param regressor_kwargs: a dictionary of key-value pairs that configures the regressor.\n",
      "    :param tf_matrix: the predictor matrix (transcription factor matrix) as a numpy array.\n",
      "    :param target_gene_expression: the target (y) gene expression to predict in function of the tf_matrix (X).\n",
      "    :param early_stop_window_length: window length of the early stopping monitor.\n",
      "    :param seed: (optional) random seed for the regressors.\n",
      "    :return: a trained regression model.\n",
      "    \"\"\"\n",
      "    # TODO: implement this method.\n",
      "    # TODO: use the regressor_type and regressor_kwargs to train the regression model.\n",
      "    # TODO: return the trained model.\n",
      "    # TODO: use the tf_matrix and target_gene_expression to train the regression model.\n",
      "    # TODO: return the trained model.\n",
      "    # TODO: use the regressor_type and regressor_kwargs to train the regression model.\n",
      "    # TODO: return the trained model.\n",
      "    # TODO: use the tf_matrix and target_gene_expression to train the regression model.\n",
      "    # TODO: return the trained model.\n",
      "    # TODO: use the regressor_type and regressor_kwargs to train the regression model.\n",
      "    # TODO: return the trained model.\n",
      "    # TODO: use the tf_matrix and target_gene_expression to train the regression model.\n",
      "    # TODO: return the trained model.\n",
      "    # TODO: use the regressor_type and regressor_kwargs to train the regression model.\n",
      "    # TODO: return the trained\n",
      "==========score: 1.0===============\n",
      "def diff(s1, s2):\n",
      "    \"\"\"\n",
      "    Return a normalised Levenshtein distance between two strings.\n",
      "\n",
      "    Distance is normalised by dividing the Levenshtein distance of the\n",
      "    two strings by the max(len(s1), len(s2)).\n",
      "\n",
      "    Examples:\n",
      "\n",
      "        >>> text.diff(\"foo\", \"foo\")\n",
      "        0\n",
      "\n",
      "        >>> text.diff(\"foo\", \"fooo\")\n",
      "        1\n",
      "\n",
      "        >>> text.diff(\"foo\", \"\")\n",
      "        1\n",
      "\n",
      "        >>> text.diff(\"1234\", \"1 34\")\n",
      "        1\n",
      "\n",
      "    Arguments:\n",
      "\n",
      "        s1 (str): Argument A.\n",
      "        s2 (str): Argument B.\n",
      "\n",
      "    Returns:\n",
      "\n",
      "        float: Normalised distance between the two strings.\n",
      "    \"\"\"\n",
      "    if s1 == s2:\n",
      "        return 0\n",
      "    if len(s1) == 0:\n",
      "        return len(s2)\n",
      "    if len(s2) == 0:\n",
      "        return len(s1)\n",
      "    if s1[-1] == s2[-1]:\n",
      "        return diff(s1[:-1], s2[:-1])\n",
      "    else:\n",
      "        return 1 + min(diff(s1, s2[:-1]), diff(s1[:-1], s2),\n",
      "                       diff(s1[:-1], s2[:-1])) / max(len(s1), len(s2))\n",
      "==========score: 1.0===============\n",
      "def distinct_counts(self):\n",
      "        \"\"\"Return counts for each distinct haplotype.\"\"\"\n",
      "        return self.haplotypes.distinct_counts()\n",
      "\n",
      "    def haplotype_counts(self):\n",
      "        \"\"\"Return counts for each haplotype.\"\"\"\n",
      "        return self.haplotypes.haplotype_counts()\n",
      "\n",
      "    def haplotype_proportions(self):\n",
      "        \"\"\"Return proportions for each haplotype.\"\"\"\n",
      "        return self.haplotypes.haplotype_proportions()\n",
      "\n",
      "    def haplotype_proportions_by_depth(self):\n",
      "        \"\"\"Return proportions for each haplotype by depth.\"\"\"\n",
      "        return self.haplotypes.haplotype_proportions_by_depth()\n",
      "\n",
      "    def haplotype_proportions_by_depth_and_allele(self):\n",
      "        \"\"\"Return proportions for each haplotype by depth and allele.\"\"\"\n",
      "        return self.haplotypes.haplotype_proportions_by_depth_and_allele()\n",
      "\n",
      "    def haplotype_proportions_by_depth_and_allele_by_depth(self):\n",
      "        \"\"\"Return proportions for each haplotype by depth and allele.\"\"\"\n",
      "        return self.haplotypes.hapl\n",
      "==========score: 1.0===============\n",
      "def extract_zipdir(zip_file):\n",
      "    \"\"\"\n",
      "    Extract contents of zip file into subfolder in parent directory.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    zip_file : str\n",
      "        Path to zip file\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "        str : folder where the zip was extracted\n",
      "    \"\"\"\n",
      "    \n",
      "    if not os.path.exists(os.path.dirname(zip_file)):\n",
      "        os.makedirs(os.path.dirname(zip_file))\n",
      "    \n",
      "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
      "        zip_ref.extractall(os.path.dirname(zip_file))\n",
      "    \n",
      "    return os.path.dirname(zip_file)\n",
      "\n",
      "def extract_zipfile(zip_file, out_dir):\n",
      "    \"\"\"\n",
      "    Extract contents of zip file into subfolder in parent directory.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    zip_file : str\n",
      "        Path to zip file\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "        str : folder where the zip was extracted\n",
      "    \"\"\"\n",
      "    \n",
      "    if not os.path.exists(out_dir):\n",
      "        os.makedirs(out_dir)\n",
      "    \n",
      "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
      "        zip_ref.extractall(out_dir)\n",
      "    \n",
      "    return out_dir\n",
      "\n",
      "\n",
      "==========score: 1.0===============\n",
      "def fromEpoch(cls, epoch_time):\r\n",
      "\r\n",
      "        ''' a method for constructing a labDT object from epoch timestamp\r\n",
      "\r\n",
      "        :param epoch_time: number with epoch timestamp info\r\n",
      "        :return: labDT object\r\n",
      "        '''\r\n",
      "\r\n",
      "        return cls(datetime.datetime.fromtimestamp(epoch_time))\r\n",
      "\r\n",
      "    def toEpoch(self):\r\n",
      "\r\n",
      "        ''' a method for converting labDT object to epoch timestamp\r\n",
      "\r\n",
      "        :return: number with epoch timestamp info\r\n",
      "        '''\r\n",
      "\r\n",
      "        return int(time.mktime(self.timetuple()))\r\n",
      "\r\n",
      "    def toDate(self):\r\n",
      "\r\n",
      "        ''' a method for converting labDT object to date\r\n",
      "\r\n",
      "        :return: date object\r\n",
      "        '''\r\n",
      "\r\n",
      "        return self.date()\r\n",
      "\r\n",
      "    def toTime(self):\r\n",
      "\r\n",
      "        ''' a method for converting labDT object to time\r\n",
      "\r\n",
      "        :return: time object\r\n",
      "        '''\r\n",
      "\r\n",
      "        return self.time()\r\n",
      "\r\n",
      "    def toTimeStamp(self):\r\n",
      "\r\n",
      "        ''' a method for converting labDT object to time stamp\r\n",
      "\r\n",
      "        :return: time stamp\r\n",
      "        '''\r\n",
      "\r\n",
      "        return self.toEpoch()\r\n",
      "\r\n",
      "\n",
      "==========score: 1.0===============\n",
      "def _search_for_executable(self, executable):\n",
      "        \"\"\"\n",
      "        Search for file give in \"executable\". If it is not found, we try the environment PATH.\n",
      "        Returns either the absolute path to the found executable, or None if the executable\n",
      "        couldn't be found.\n",
      "        \"\"\"\n",
      "        if executable in self._executables:\n",
      "            return self._executables[executable]\n",
      "\n",
      "        for path in os.environ[\"PATH\"].split(os.pathsep):\n",
      "            path = path.strip('\"')\n",
      "            exe_file = os.path.join(path, executable)\n",
      "            if os.path.isfile(exe_file):\n",
      "                self._executables[executable] = exe_file\n",
      "                return exe_file\n",
      "\n",
      "        return None\n",
      "\n",
      "    def _get_executable(self, executable):\n",
      "        \"\"\"\n",
      "        Returns the absolute path to the executable given in \"executable\". If the executable\n",
      "        couldn't be found, we try the environment PATH.\n",
      "        \"\"\"\n",
      "        if executable in self._executables:\n",
      "            return self._executables[executable]\n",
      "\n",
      "        return self._search_for_executable(executable)\n",
      "\n",
      "    def _get_executables(self, executable):\n",
      "        \"\"\"\n",
      "        Returns a list of absolute paths to the executable given in \"executable\". If the\n",
      "        executable couldn't be found, we try the environment PATH.\n",
      "        \"\"\"\n",
      "\n",
      "==========score: 1.0===============\n",
      "def utc_epoch():\n",
      "    \"\"\"\n",
      "    Gets the epoch in the users timezone\n",
      "    :return:\n",
      "    \"\"\"\n",
      "    return datetime.utcfromtimestamp(0)\n",
      "==========score: 1.0===============\n",
      "def to_uint8(self):\n",
      "        \"\"\"\n",
      "        Convert this heatmaps object to a 0-to-255 array.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        arr_uint8 : (H,W,C) ndarray\n",
      "            Heatmap as a 0-to-255 array (dtype is uint8).\n",
      "\n",
      "        \"\"\"\n",
      "        return self.to_float().to_uint8()\n",
      "\n",
      "    def __getitem__(self, item):\n",
      "        \"\"\"\n",
      "        Access self[key].\n",
      "\n",
      "        Note that the following behavior has been changed since release v0.20:\n",
      "        Accessing a single element using an integer or slice returns a ColorMap\n",
      "        object, including the element itself. However, accessing using a\n",
      "        string or other sequence does not return a ColorMap object, but rather\n",
      "        a ColorMap object that is a view on the element.\n",
      "\n",
      "        \"\"\"\n",
      "        if isinstance(item, numbers.Integral):\n",
      "            if item >= len(self):\n",
      "                raise IndexError(\n",
      "                    'index {} is out of bounds for axis 0 with size {}'.format(\n",
      "                        item, len(self)))\n",
      "            return self._arr[item]\n",
      "        elif isinstance(item, slice):\n",
      "            return self._arr[item]\n",
      "        elif isinstance(item, tuple):\n",
      "            raise ValueError('Multi-element access through sequence is '\n",
      "                             'deprecated. Use `__getitem__` or `__getitem_by_idx` '\n",
      "                             'instead.')\n",
      "\n",
      "==========score: 1.0===============\n",
      "def diff(s1, s2):\n",
      "    \"\"\"\n",
      "    Return a normalised Levenshtein distance between two strings.\n",
      "\n",
      "    Distance is normalised by dividing the Levenshtein distance of the\n",
      "    two strings by the max(len(s1), len(s2)).\n",
      "\n",
      "    Examples:\n",
      "\n",
      "        >>> text.diff(\"foo\", \"foo\")\n",
      "        0\n",
      "\n",
      "        >>> text.diff(\"foo\", \"fooo\")\n",
      "        1\n",
      "\n",
      "        >>> text.diff(\"foo\", \"\")\n",
      "        1\n",
      "\n",
      "        >>> text.diff(\"1234\", \"1 34\")\n",
      "        1\n",
      "\n",
      "    Arguments:\n",
      "\n",
      "        s1 (str): Argument A.\n",
      "        s2 (str): Argument B.\n",
      "\n",
      "    Returns:\n",
      "\n",
      "        float: Normalised distance between the two strings.\n",
      "    \"\"\"\n",
      "    if s1 == s2:\n",
      "        return 0\n",
      "    if s1 == \"\":\n",
      "        return len(s2)\n",
      "    if s2 == \"\":\n",
      "        return len(s1)\n",
      "    if s1 == \" \":\n",
      "        return len(s2)\n",
      "    if s2 == \" \":\n",
      "        return len(s1)\n",
      "    if s1[0] == s2[0]:\n",
      "        return diff(s1[1:], s2[1:])\n",
      "    return 1 + min(\n",
      "        diff(s1, s2[1:]),\n",
      "        diff(s1[1:], s2),\n",
      "        diff(s2, s1[1:]),\n",
      "        diff(s2[1:], s1),\n",
      "    ) / max(len(s1), len(s2))\n",
      "==========score: 1.0===============\n",
      "def recv_with_timeout(self, timeout=1):\n",
      "        \"\"\"Receive a complete ISOTP message, blocking until a message is\n",
      "        received or the specified timeout is reached.\n",
      "        If timeout is 0, then this function doesn't block and returns the\n",
      "        first frame in the receive buffer or None if there isn't any.\"\"\"\n",
      "        if timeout == 0:\n",
      "            return self._recv_buffer[0]\n",
      "\n",
      "        # Check if we have messages in the receive buffer\n",
      "        if len(self._recv_buffer) > 0:\n",
      "            # Return the first frame in the buffer\n",
      "            return self._recv_buffer.popleft()\n",
      "\n",
      "        # No message in the buffer, try to receive one\n",
      "        self._recv_mutex.acquire()\n",
      "        try:\n",
      "            while len(self._recv_buffer) == 0:\n",
      "                self._recv_cond.wait(timeout)\n",
      "\n",
      "                if timeout!= 0 and timeout <= 0:\n",
      "                    # Timeout is 0, just return None\n",
      "                    self._recv_mutex.release()\n",
      "                    return None\n",
      "\n",
      "                # Check if we have messages in the receive buffer\n",
      "                if len(self._recv_buffer) > 0:\n",
      "                    # Return the first frame in the buffer\n",
      "                    return self._recv_buffer.popleft()\n",
      "\n",
      "            # We have a message in the buffer\n",
      "            self._recv_cond.release()\n",
      "            return self._recv_buffer.popleft()\n",
      "        finally\n",
      "==========score: 1.0===============\n",
      "def convert(self, value):\n",
      "        \"\"\" Convert the specified value to the type of the option.\n",
      "\n",
      "        Args:\n",
      "            value: The value that should be converted.\n",
      "\n",
      "        Returns:\n",
      "            The value with the type given by the option.\n",
      "        \"\"\"\n",
      "        return value\n",
      "\n",
      "    def get_value(self, value):\n",
      "        \"\"\" Get the value of the option.\n",
      "\n",
      "        Args:\n",
      "            value: The value that should be converted.\n",
      "\n",
      "        Returns:\n",
      "            The value with the type given by the option.\n",
      "        \"\"\"\n",
      "        return value\n",
      "\n",
      "    def get_type(self):\n",
      "        \"\"\" Get the type of the option.\n",
      "\n",
      "        Returns:\n",
      "            The type of the option.\n",
      "        \"\"\"\n",
      "        return self.type\n",
      "\n",
      "    def get_default(self):\n",
      "        \"\"\" Get the default value of the option.\n",
      "\n",
      "        Returns:\n",
      "            The default value of the option.\n",
      "        \"\"\"\n",
      "        return self.default\n",
      "\n",
      "    def get_description(self):\n",
      "        \"\"\" Get the description of the option.\n",
      "\n",
      "        Returns:\n",
      "            The description of the option.\n",
      "        \"\"\"\n",
      "        return self.description\n",
      "\n",
      "    def get_help(self):\n",
      "        \"\"\" Get the help of the option.\n",
      "\n",
      "        Returns:\n",
      "            The help of the option.\n",
      "        \"\"\"\n",
      "        return self.help\n",
      "\n",
      "    def get_choices(self):\n",
      "        \"\"\" Get the choices of the option.\n",
      "\n",
      "        Returns:\n",
      "            The choices of the option\n",
      "==========score: 1.0===============\n",
      "def timer(fn, miniter=3, minwall=3.0):\n",
      "    \"\"\"Runs fn() multiple times and returns the results.\n",
      "\n",
      "    Runs for at least ``miniter`` iterations and ``minwall`` wall time.\n",
      "    \"\"\"\n",
      "    results = []\n",
      "    for i in range(miniter):\n",
      "        start = time.time()\n",
      "        results.append(fn())\n",
      "        if i >= miniter - 1:\n",
      "            break\n",
      "        time.sleep(1)\n",
      "    end = time.time()\n",
      "    wall = end - start\n",
      "    if wall < minwall:\n",
      "        time.sleep(minwall - wall)\n",
      "    return results\n",
      "==========score: 1.0===============\n",
      "def distinct_counts(self):\n",
      "        \"\"\"Return counts for each distinct haplotype.\"\"\"\n",
      "        return self.haplotypes.counts()\n",
      "\n",
      "    def haplotypes(self):\n",
      "        \"\"\"Return all haplotypes.\"\"\"\n",
      "        return self.haplotypes.values()\n",
      "\n",
      "    def haplotypes_by_id(self):\n",
      "        \"\"\"Return all haplotypes by id.\"\"\"\n",
      "        return self.haplotypes.by_id()\n",
      "\n",
      "    def haplotypes_by_name(self):\n",
      "        \"\"\"Return all haplotypes by name.\"\"\"\n",
      "        return self.haplotypes.by_name()\n",
      "\n",
      "    def haplotypes_by_index(self):\n",
      "        \"\"\"Return all haplotypes by index.\"\"\"\n",
      "        return self.haplotypes.by_index()\n",
      "\n",
      "    def haplotypes_by_position(self):\n",
      "        \"\"\"Return all haplotypes by position.\"\"\"\n",
      "        return self.haplotypes.by_position()\n",
      "\n",
      "    def haplotypes_by_id_or_name(self):\n",
      "        \"\"\"Return all haplotypes by id or name.\"\"\"\n",
      "        return self.haplotypes.by_id_or_name()\n",
      "\n",
      "    def haplotypes_by_index_or_position(self):\n",
      "        \"\"\"Return all haplotypes by index or\n",
      "==========score: 1.0===============\n",
      "def median(self, **kwargs):\n",
      "        \"\"\"Returns median of each column or row.\n",
      "\n",
      "        Returns:\n",
      "            A new QueryCompiler object containing the median of each column or row.\n",
      "        \"\"\"\n",
      "        return self.__class__(self.__class__.median(self, **kwargs))\n",
      "\n",
      "    @classmethod\n",
      "    def mode(cls, *args, **kwargs):\n",
      "        \"\"\"Returns mode of each column or row.\n",
      "\n",
      "        Returns:\n",
      "            A new QueryCompiler object containing the mode of each column or row.\n",
      "        \"\"\"\n",
      "        return cls(cls.mode(*args, **kwargs))\n",
      "\n",
      "    @classmethod\n",
      "    def std(cls, *args, **kwargs):\n",
      "        \"\"\"Returns standard deviation of each column or row.\n",
      "\n",
      "        Returns:\n",
      "            A new QueryCompiler object containing the standard deviation of each column or row.\n",
      "        \"\"\"\n",
      "        return cls(cls.std(*args, **kwargs))\n",
      "\n",
      "    @classmethod\n",
      "    def var(cls, *args, **kwargs):\n",
      "        \"\"\"Returns variance of each column or row.\n",
      "\n",
      "        Returns:\n",
      "            A new QueryCompiler object containing the variance of each column or row.\n",
      "        \"\"\"\n",
      "        return cls(cls.var(*args, **kwargs))\n",
      "\n",
      "    @classmethod\n",
      "    def quantile(cls,\n",
      "==========score: 1.0===============\n",
      "def get_connection(engine, host, user, port, password, database, ssl={}):\n",
      "    \"\"\" Returns a PostgreSQL or MySQL connection \"\"\"\n",
      "    if engine == \"postgres\":\n",
      "        return psycopg2.connect(host=host, user=user, password=password, database=database, port=port, sslmode=ssl.get(\"mode\", \"prefer\"))\n",
      "    elif engine == \"mysql\":\n",
      "        return mysql.connect(host=host, user=user, password=password, database=database, port=port, ssl={})\n",
      "    else:\n",
      "        raise Exception(\"Unknown engine\")\n",
      "==========score: 1.0===============\n",
      "def paste(self):\r\n",
      "        \"\"\"Import text/data/code from clipboard\"\"\"\r\n",
      "        if self.text_edit.textCursor().hasSelection():\r\n",
      "            self.text_edit.paste()\r\n",
      "        else:\r\n",
      "            self.text_edit.setTextCursor(QTextCursor(self.text_edit.document()))\r\n",
      "            self.text_edit.insertPlainText(QApplication.clipboard().text())\r\n",
      "\r\n",
      "    def cut(self):\r\n",
      "        \"\"\"Copy text/data/code to clipboard\"\"\"\r\n",
      "        if self.text_edit.textCursor().hasSelection():\r\n",
      "            self.text_edit.textCursor().removeSelectedText()\r\n",
      "            QApplication.clipboard().setText(self.text_edit.toPlainText())\r\n",
      "        else:\r\n",
      "            QApplication.clipboard().setText(self.text_edit.toPlainText())\r\n",
      "\r\n",
      "    def select_all(self):\r\n",
      "        \"\"\"Select all text/data/code in text editor\"\"\"\r\n",
      "        self.text_edit.selectAll()\r\n",
      "\r\n",
      "    def find(self):\r\n",
      "        \"\"\"Find text/data/code in text editor\n",
      "==========score: 1.0===============\n",
      "def heatmap(z, x=None, y=None, colorscale='Viridis'):\n",
      "    \"\"\"Create a heatmap.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    z : TODO\n",
      "    x : TODO, optional\n",
      "    y : TODO, optional\n",
      "    colorscale : TODO, optional\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    Chart\n",
      "\n",
      "\n",
      "    \"\"\"\n",
      "    if x is None:\n",
      "        x = []\n",
      "    if y is None:\n",
      "        y = []\n",
      "    return make_chart(\n",
      "        \"heatmap\",\n",
      "        z,\n",
      "        x=x,\n",
      "        y=y,\n",
      "        colorscale=colorscale,\n",
      "    )\n",
      "==========score: 1.0===============\n",
      "def _readline(self, ignore_comments=True):\n",
      "        \"\"\"The next line of the DETX file, optionally ignores comments\"\"\"\n",
      "        line = self._file.readline()\n",
      "        if ignore_comments:\n",
      "            line = line.split('#')[0]\n",
      "        return line\n",
      "\n",
      "    def _read_next_line(self):\n",
      "        \"\"\"Reads the next line of the DETX file\"\"\"\n",
      "        line = self._readline()\n",
      "        if line == '':\n",
      "            return None\n",
      "        return line\n",
      "\n",
      "    def _read_next_line_as_int(self):\n",
      "        \"\"\"Reads the next line of the DETX file as an int\"\"\"\n",
      "        line = self._read_next_line()\n",
      "        if line is None:\n",
      "            return None\n",
      "        return int(line)\n",
      "\n",
      "    def _read_next_line_as_float(self):\n",
      "        \"\"\"Reads the next line of the DETX file as a float\"\"\"\n",
      "        line = self._read_next_line()\n",
      "        if line is None:\n",
      "            return None\n",
      "        return float(line)\n",
      "\n",
      "    def _read_next_line_as_string(self):\n",
      "        \"\"\"Reads the next line of the DETX file as a string\"\"\"\n",
      "        line = self._read_next_line()\n",
      "==========score: 1.0===============\n",
      "def push(self, item, priority=None):\n",
      "        \"\"\"Push the item in the priority queue.\n",
      "        if priority is not given, priority is set to the value of item.\n",
      "        \"\"\"\n",
      "        if priority is None:\n",
      "            priority = item\n",
      "        if priority > self.max_priority:\n",
      "            self.max_priority = priority\n",
      "        if self.size == 0:\n",
      "            self.min_priority = priority\n",
      "        heapq.heappush(self.heap, (priority, item))\n",
      "        self.size += 1\n",
      "\n",
      "    def pop(self):\n",
      "        \"\"\"Pop the item with the highest priority.\n",
      "        \"\"\"\n",
      "        priority, item = heapq.heappop(self.heap)\n",
      "        self.size -= 1\n",
      "        if priority == self.min_priority:\n",
      "            self.min_priority = self.heap[0][0]\n",
      "        return item\n",
      "\n",
      "    def __len__(self):\n",
      "        return self.size\n",
      "\n",
      "    def __str__(self):\n",
      "        return str(self.heap)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return str(self.heap)\n",
      "==========score: 1.0===============\n",
      "def multiply(self, matrix):\n",
      "        \"\"\"\n",
      "        Multiply this matrix by a local dense matrix on the right.\n",
      "\n",
      "        :param matrix: a local dense matrix whose number of rows must match the number of columns\n",
      "                       of this matrix\n",
      "        :returns: :py:class:`IndexedRowMatrix`\n",
      "\n",
      "        >>> mat = IndexedRowMatrix(sc.parallelize([(0, (0, 1)), (1, (2, 3))]))\n",
      "        >>> mat.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()\n",
      "        [IndexedRow(0, [2.0,3.0]), IndexedRow(1, [6.0,11.0])]\n",
      "        \"\"\"\n",
      "        if not isinstance(matrix, DenseMatrix):\n",
      "            raise ValueError(\"Only multiplication with DenseMatrix \"\n",
      "                             \"is supported.\")\n",
      "        mat_rows = matrix.rows\n",
      "        mat_cols = matrix.cols\n",
      "        new_rows = self.rows\n",
      "        new_cols = self.cols\n",
      "        new_data = []\n",
      "\n",
      "        if self.cols!= mat_cols:\n",
      "            raise ValueError(\"Number of columns of the \"\n",
      "                             \"matrix must match the number of columns of this matrix: \"\n",
      "                             \"{} vs {}\".format(self.cols, mat_cols))\n",
      "\n",
      "        if self.rows!= mat_rows:\n",
      "            raise ValueError(\"Number of rows of the \"\n",
      "                             \"matrix must match the number of rows of this matrix: \"\n",
      "                             \"{} vs {}\".format(self.rows, mat_rows))\n",
      "\n",
      "        for i in range(self.rows):\n",
      "            for j in range(self.cols):\n",
      "                new_data.append(self.data[i * self.cols + j] *\n",
      "                                mat_rows[i] * mat_cols\n",
      "==========score: 1.0===============\n",
      "def pick(self):\n",
      "        \"\"\" picks a value accoriding to the given density \"\"\"\n",
      "        return self.random_state.choice(self.values, p=self.probabilities)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"{self.__class__.__name__}({self.values}, {self.probabilities})\"\n",
      "==========score: 1.0===============\n",
      "def render_pdf(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Render the PDF and returns as bytes.\n",
      "\n",
      "        :rtype: bytes\n",
      "        \"\"\"\n",
      "        options = self.get_rendering_options()\n",
      "        return self.render(*args, **options).getvalue()\n",
      "\n",
      "    def render(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Render the PDF.\n",
      "\n",
      "        :rtype: flectra.http.response.HttpResponse\n",
      "        \"\"\"\n",
      "        options = self.get_rendering_options()\n",
      "        return self.env['ir.qweb'].render(self.name, args=args,\n",
      "                                          qcontext=self.get_rendering_context(),\n",
      "                                          website=self.website_id, **options)\n",
      "\n",
      "    def _get_rendering_options(self):\n",
      "        return {\n",
      "            'load_components': True,\n",
      "            'context': self.env.context,\n",
      "            # context are the values before the context is overriden by\n",
      "            # website\n",
      "            'context_with_inactive_lang': self._context_with_inactive_lang(),\n",
      "            'context_lang': self.env.lang,\n",
      "            'context_tz': self.env.context.get('tz') or self.env.\n",
      "==========score: 1.0===============\n",
      "def setcookie(self, key, value, max_age=None, expires=None, path='/', domain=None, secure=None, httponly=False):\n",
      "        \"\"\"\n",
      "        Add a new cookie\n",
      "        \"\"\"\n",
      "        self.cookies[key] = value\n",
      "        if max_age is not None:\n",
      "            self.cookies[key]['max-age'] = max_age\n",
      "        if expires is not None:\n",
      "            self.cookies[key]['expires'] = expires\n",
      "        if path is not None:\n",
      "            self.cookies[key]['path'] = path\n",
      "        if domain is not None:\n",
      "            self.cookies[key]['domain'] = domain\n",
      "        if secure:\n",
      "            self.cookies[key]['secure'] = True\n",
      "        if httponly:\n",
      "            self.cookies[key]['httponly'] = True\n",
      "\n",
      "    def deletecookie(self, key):\n",
      "        \"\"\"\n",
      "        Delete a cookie\n",
      "        \"\"\"\n",
      "        del self.cookies[key]\n",
      "\n",
      "    def setheaders(self, headers):\n",
      "        \"\"\"\n",
      "        Set the headers\n",
      "        \"\"\"\n",
      "        self.headers = headers\n",
      "\n",
      "    def setstatus(self, status):\n",
      "        \"\"\"\n",
      "        Set the status\n",
      "        \"\"\"\n",
      "        self.status = status\n",
      "\n",
      "    def setheader(self, key, value):\n",
      "        \"\"\"\n",
      "        Set a header\n",
      "        \"\"\"\n",
      "==========score: 1.0===============\n",
      "def _cbc_encrypt(self, content, final_key):\n",
      "        \"\"\"This method encrypts the content.\"\"\"\n",
      "        # The content is divided into blocks of 16 bytes.\n",
      "        # Each block is encrypted using the key and the final key.\n",
      "        # The encrypted blocks are concatenated and returned.\n",
      "        # The length of the content is rounded up to the next multiple of 16.\n",
      "        content_length = len(content)\n",
      "        content_length = content_length + (content_length % 16)\n",
      "        content = content + (b'\\0' * (content_length - len(content)))\n",
      "        encrypted_blocks = []\n",
      "        for i in range(0, content_length, 16):\n",
      "            block = content[i:i + 16]\n",
      "            encrypted_blocks.append(self._cbc_encrypt_block(block, final_key))\n",
      "        return b''.join(encrypted_blocks)\n",
      "\n",
      "    def _cbc_encrypt_block(self, block, final_key):\n",
      "        \"\"\"This method encrypts a single block.\"\"\"\n",
      "        block = self._pad(block)\n",
      "        state = self._create_state(block)\n",
      "        encrypted_block = b''\n",
      "        for i in range(16):\n",
      "            state = self._cbc\n",
      "==========score: 1.0===============\n",
      "def unzip_recursive(zip_file_name):\n",
      "    \"\"\"Unzip file with all recursive zip files inside and delete zip files after that.\n",
      "\n",
      "    :param zip_file_name: file name of zip file\n",
      "    :return: list of archive members by name.\n",
      "    \"\"\"\n",
      "    zip_file = zipfile.ZipFile(zip_file_name, 'r')\n",
      "    members = zip_file.namelist()\n",
      "    for member in members:\n",
      "        if member.endswith('.zip'):\n",
      "            zip_file.extract(member, '.')\n",
      "            unzip_recursive(member)\n",
      "            zip_file.extract(member, '.')\n",
      "            os.remove(member)\n",
      "    zip_file.close()\n",
      "    return members\n",
      "==========score: 1.0===============\n",
      "def scatter_plot(self, ax, topic_dims, t=None, ms_limits=True, **kwargs_plot):\n",
      "        \"\"\" 2D or 3D scatter plot.\n",
      "\n",
      "            :param axes ax: matplotlib axes (use Axes3D if 3D data)\n",
      "\n",
      "            :param tuple topic_dims: list of (topic, dims) tuples, where topic is a string and dims is a list of dimensions to be plotted for that topic.\n",
      "\n",
      "            :param int t: time indexes to be plotted\n",
      "\n",
      "            :param dict kwargs_plot: argument to be passed to matplotlib's plot function, e.g. the style of the plotted points 'or'\n",
      "\n",
      "            :param bool ms_limits: if set to True, automatically set axes boundaries to the sensorimotor boundaries (default: True)\n",
      "        \"\"\"\n",
      "        if not self.ms_data:\n",
      "            raise ValueError('No data to plot')\n",
      "\n",
      "        # get the topic dimensions\n",
      "        topic_dims = self._get_topic_dims(topic_dims)\n",
      "\n",
      "        # get the topic data\n",
      "        topic_data = self._get_topic_data(topic_dims, t)\n",
      "\n",
      "        # get the topic labels\n",
      "        topic_labels = self._get_topic_labels(topic_dims)\n",
      "\n",
      "        # get the topic colors\n",
      "        topic_colors = self._get_topic_colors(topic_dims)\n",
      "\n",
      "        # get the topic markers\n",
      "        topic_markers = self._get_topic_markers(topic_dims)\n",
      "\n",
      "        # get the topic marker sizes\n",
      "        topic_marker_sizes = self._get_topic_marker_sizes(topic_dims)\n",
      "\n",
      "        # get the topic marker edgecolors\n",
      "        topic_marker_edgecolors = self._get_topic_marker_edgecolors(topic_dims)\n",
      "\n",
      "        # get the topic marker edgecolors\n",
      "        topic_marker_facecolors =\n",
      "==========score: 1.0===============\n",
      "def send(self, msg):\n",
      "        \"\"\"Encodes data to slip protocol and then sends over serial port\n",
      "\n",
      "        Uses the SlipLib module to convert the message data into SLIP format.\n",
      "        The message is then sent over the serial port opened with the instance\n",
      "        of the Faraday class used when invoking send().\n",
      "\n",
      "        Args:\n",
      "            msg (bytes): Bytes format message to send over serial port.\n",
      "\n",
      "        Returns:\n",
      "            int: Number of bytes transmitted over the serial port.\n",
      "\n",
      "        \"\"\"\n",
      "        return self.slip_lib.send(msg)\n",
      "\n",
      "    def recv(self):\n",
      "        \"\"\"Decodes data from slip protocol and then receives over serial port\n",
      "\n",
      "        Uses the SlipLib module to convert the message data into SLIP format.\n",
      "        The message is then received over the serial port opened with the\n",
      "        instance of the Faraday class used when invoking recv().\n",
      "\n",
      "        Returns:\n",
      "            bytes: Bytes format message received over serial port.\n",
      "\n",
      "        \"\"\"\n",
      "        return self.slip_lib.recv()\n",
      "\n",
      "    def close(self):\n",
      "        \"\"\"Closes serial port connection\n",
      "\n",
      "        Closes the serial port connection opened with the instance of the\n",
      "        Faraday class used when invoking send().\n",
      "\n",
      "        \"\"\"\n",
      "        self.slip_lib.close()\n",
      "\n",
      "    def __del__(self):\n",
      "        \"\"\"Closes serial port connection\n",
      "\n",
      "        Closes the serial port connection opened with the instance of the\n",
      "        Faraday class used when invoking send().\n",
      "\n",
      "        \"\"\"\n",
      "        self.close()\n",
      "==========score: 1.0===============\n",
      "def convert(self, value):\n",
      "        \"\"\" Convert the specified value to the type of the option.\n",
      "\n",
      "        Args:\n",
      "            value: The value that should be converted.\n",
      "\n",
      "        Returns:\n",
      "            The value with the type given by the option.\n",
      "        \"\"\"\n",
      "        if self.type == \"int\":\n",
      "            return int(value)\n",
      "        elif self.type == \"float\":\n",
      "            return float(value)\n",
      "        elif self.type == \"bool\":\n",
      "            return value.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
      "        elif self.type == \"str\":\n",
      "            return str(value)\n",
      "        else:\n",
      "            raise ValueError(\"Unknown type: %s\" % self.type)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return \"<Option: %s>\" % self.name\n",
      "==========score: 1.0===============\n",
      "def conn(host=None, user=None, password=None, init_fun=None, reset=False):\n",
      "    \"\"\"\n",
      "    Returns a persistent connection object to be shared by multiple modules.\n",
      "    If the connection is not yet established or reset=True, a new connection is set up.\n",
      "    If connection information is not provided, it is taken from config which takes the\n",
      "    information from dj_local_conf.json. If the password is not specified in that file\n",
      "    datajoint prompts for the password.\n",
      "\n",
      "    :param host: hostname\n",
      "    :param user: mysql user\n",
      "    :param password: mysql password\n",
      "    :param init_fun: initialization function\n",
      "    :param reset: whether the connection should be reset or not\n",
      "    \"\"\"\n",
      "    if host is None:\n",
      "        host = config.get('database', 'host')\n",
      "    if user is None:\n",
      "        user = config.get('database', 'user')\n",
      "    if password is None:\n",
      "        password = config.get('database', 'password')\n",
      "    if not password:\n",
      "        password = getpass.getpass('Password: ')\n",
      "    if not host:\n",
      "        host = input('Host: ')\n",
      "    if not user:\n",
      "        user = input('User: ')\n",
      "    if not password:\n",
      "        password = getpass.getpass('Password: ')\n",
      "    if not init_fun:\n",
      "        init_fun = config.get('database', 'init_fun')\n",
      "    if not init_fun:\n",
      "        init_fun = 'dj_local_conn'\n",
      "    if not reset:\n",
      "        return Connection(host, user, password, init_fun)\n",
      "    else:\n",
      "        return Connection(host, user, password, init_fun, reset=True)\n",
      "==========score: 1.0===============\n",
      "def set_working_dir(self, working_dir):\n",
      "        \"\"\"\n",
      "        Sets the working directory for this hypervisor.\n",
      "\n",
      "        :param working_dir: path to the working directory\n",
      "        \"\"\"\n",
      "        self._working_dir = working_dir\n",
      "\n",
      "    def get_working_dir(self):\n",
      "        \"\"\"\n",
      "        Gets the working directory for this hypervisor.\n",
      "\n",
      "        :returns: path to the working directory\n",
      "        \"\"\"\n",
      "        return self._working_dir\n",
      "\n",
      "    def set_hostname(self, hostname):\n",
      "        \"\"\"\n",
      "        Sets the hostname for this hypervisor.\n",
      "\n",
      "        :param hostname: hostname\n",
      "        \"\"\"\n",
      "        self._hostname = hostname\n",
      "\n",
      "    def get_hostname(self):\n",
      "        \"\"\"\n",
      "        Gets the hostname for this hypervisor.\n",
      "\n",
      "        :returns: hostname\n",
      "        \"\"\"\n",
      "        return self._hostname\n",
      "\n",
      "    def set_username(self, username):\n",
      "        \"\"\"\n",
      "        Sets the username for this hypervisor.\n",
      "\n",
      "        :param username: username\n",
      "        \"\"\"\n",
      "        self._username = username\n",
      "\n",
      "    def get_username(self):\n",
      "        \"\"\"\n",
      "        Gets the username for this hypervisor.\n",
      "\n",
      "        :returns: username\n",
      "        \"\"\"\n",
      "        return self._username\n",
      "\n",
      "    def set_password(self, password):\n",
      "        \"\"\"\n",
      "        Sets the password for this hypervisor\n",
      "==========score: 1.0===============\n",
      "def plot_confusion_matrix(cm, classes,\n",
      "                          normalize=False,\n",
      "                          title='Confusion matrix',\n",
      "                          cmap=plt.cm.Blues):\n",
      "    \"\"\"\n",
      "    This function prints and plots the confusion matrix.\n",
      "    Normalization can be applied by setting `normalize=True`.\n",
      "    \"\"\"\n",
      "    if normalize:\n",
      "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
      "        print(\"Normalized confusion matrix\")\n",
      "    else:\n",
      "        print('Confusion matrix, without normalization')\n",
      "\n",
      "    print(cm)\n",
      "\n",
      "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
      "    plt.title(title)\n",
      "    plt.colorbar()\n",
      "    tick_marks = np.arange(len(classes))\n",
      "    plt.xticks(tick_marks, classes, rotation=45)\n",
      "    plt.yticks(tick_marks, classes)\n",
      "\n",
      "    fmt = '.2f' if normalize else 'd'\n",
      "    thresh = cm.max() / 2.\n",
      "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
      "        plt.text(j, i, format(cm[i, j], fmt),\n",
      "                 horizontalalignment=\"center\",\n",
      "                 color=\"white\" if cm[i, j] > thresh else \"black\n",
      "==========score: 1.0===============\n",
      "def copy_file(src, dst, ignore=None):\n",
      "   \"\"\" this function will simply copy the file from the source path to the dest\n",
      "   path given as input\n",
      "   \"\"\"\n",
      "   if not os.path.exists(src):\n",
      "      print(\"Source file does not exist\")\n",
      "      return\n",
      "   if os.path.exists(dst):\n",
      "      print(\"Destination file already exists\")\n",
      "      return\n",
      "   else:\n",
      "      shutil.copyfile(src, dst)\n",
      "      print(\"File copied\")\n",
      "\n",
      "def copy_dir(src, dst, ignore=None):\n",
      "   \"\"\" this function will simply copy the directory from the source path to the\n",
      "   destination path given as input\n",
      "   \"\"\"\n",
      "   if not os.path.exists(src):\n",
      "      print(\"Source file does not exist\")\n",
      "      return\n",
      "   if os.path.exists(dst):\n",
      "      print(\"Destination file already exists\")\n",
      "      return\n",
      "   else:\n",
      "      shutil.copytree(src, dst)\n",
      "      print(\"Directory copied\")\n",
      "\n",
      "\n",
      "==========score: 1.0===============\n",
      "def from_text_file(file_path):\n",
      "        \"\"\"Load MonsoonData objects from a text file generated by\n",
      "        MonsoonData.save_to_text_file.\n",
      "\n",
      "        Args:\n",
      "            file_path: The full path of the file load from, including the file\n",
      "                name.\n",
      "\n",
      "        Returns:\n",
      "            A list of MonsoonData objects.\n",
      "        \"\"\"\n",
      "        with open(file_path, 'r') as f:\n",
      "            lines = f.readlines()\n",
      "        return [\n",
      "            MonsoonData.from_json(line.strip()) for line in lines\n",
      "            if line.strip()\n",
      "        ]\n",
      "\n",
      "    @staticmethod\n",
      "    def from_json(json_str):\n",
      "        \"\"\"Load MonsoonData objects from a json string.\n",
      "\n",
      "        Args:\n",
      "            json_str: The json string to load.\n",
      "\n",
      "        Returns:\n",
      "            A MonsoonData object.\n",
      "        \"\"\"\n",
      "        return json.loads(json_str)\n",
      "\n",
      "    def to_json(self):\n",
      "        \"\"\"Serialize the MonsoonData object to a json string.\n",
      "\n",
      "        Returns:\n",
      "            The json string representing the MonsoonData object.\n",
      "        \"\"\"\n",
      "        return json.dumps(self.to_dict())\n",
      "\n",
      "    def to_dict(self):\n",
      "        \"\"\"Serialize the MonsoonData object to a dictionary.\n",
      "\n",
      "        Returns:\n",
      "            The dictionary representing the MonsoonData object.\n",
      "        \"\"\"\n",
      "        return {\n",
      "            'id': self.id,\n",
      "            'voltage': self.voltage,\n",
      "            'current': self.\n",
      "==========score: 1.0===============\n",
      "def findAllSubstrings(string, substring):\n",
      "    \"\"\" Returns a list of all substring starting positions in string or an empty\n",
      "    list if substring is not present in string.\n",
      "\n",
      "    :param string: a template string\n",
      "    :param substring: a string, which is looked for in the ``string`` parameter.\n",
      "\n",
      "    :returns: a list of substring starting positions in the template string\n",
      "    \"\"\"\n",
      "    if not substring:\n",
      "        return []\n",
      "    return [i for i in range(len(string)) if string.startswith(substring, i)]\n",
      "==========score: 1.0===============\n",
      "def normal_distribution(mean, variance,\n",
      "                        minimum=None, maximum=None, weight_count=23):\n",
      "    \"\"\"\n",
      "    Return a list of weights approximating a normal distribution.\n",
      "\n",
      "    Args:\n",
      "        mean (float): The mean of the distribution\n",
      "        variance (float): The variance of the distribution\n",
      "        minimum (float): The minimum outcome possible to\n",
      "            bound the output distribution to\n",
      "        maximum (float): The maximum outcome possible to\n",
      "            bound the output distribution to\n",
      "        weight_count (int): The number of weights that will\n",
      "            be used to approximate the distribution\n",
      "\n",
      "    Returns:\n",
      "        list: a list of ``(float, float)`` weight tuples\n",
      "        approximating a normal distribution.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: ``if maximum < minimum``\n",
      "        TypeError: if both ``minimum`` and ``maximum`` are ``None``\n",
      "\n",
      "    Example:\n",
      "        >>> weights = normal_distribution(10, 3,\n",
      "        ...                               minimum=0, maximum=20,\n",
      "        ...                               weight_count=5)\n",
      "        >>> rounded_weights = [(round(value, 2), round(strength, 2))\n",
      "        ...                    for value, strength in weights]\n",
      "        >>> rounded_weights\n",
      "        [(1.34, 0.0), (4.8, 0.0), (8.27, 0.14), (11.73, 0.14), (15.2, 0.0)]\n",
      "    \"\"\"\n",
      "    if maximum < minimum:\n",
      "        raise ValueError(\"maximum < minimum\")\n",
      "    if minimum is None:\n",
      "        minimum = 0\n",
      "    if maximum is None:\n",
      "        maximum = 100\n",
      "    if maximum < minimum:\n",
      "        raise ValueError(\"maximum < minimum\")\n",
      "    if weight_count < 2:\n",
      "        raise ValueError(\"weight_count < 2\")\n",
      "    if weight_count % 2 == 1:\n",
      "        raise ValueError(\"weight_count must be even\")\n",
      "    weights = []\n",
      "    for i in range(weight_count):\n",
      "        value = (maximum - minimum) / 2.0 + minimum\n",
      "        strength = (maximum - minimum) / 2.0\n",
      "        weights.append((value, strength))\n",
      "        value = (maximum + minimum) / 2.0 + minimum\n",
      "        strength = (maximum + minimum) / 2.0\n",
      "        weights.append((value, strength))\n",
      "    return weights\n",
      "==========score: 1.0===============\n",
      "def image_from(from_value):\n",
      "    \"\"\"\n",
      "    :param from_value: string like \"image:tag\" or \"image:tag AS name\"\n",
      "    :return: tuple of the image and stage name, e.g. (\"image:tag\", None)\n",
      "    \"\"\"\n",
      "    if \":\" in from_value:\n",
      "        image, stage = from_value.split(\":\")\n",
      "        return image, stage\n",
      "    else:\n",
      "        return from_value, None\n",
      "==========score: 1.0===============\n",
      "def int_to_hex(i):\n",
      "    \"\"\"Create a hex-representation of the given serial.\n",
      "\n",
      "    >>> int_to_hex(12345678)\n",
      "    'BC:61:4E'\n",
      "    \"\"\"\n",
      "    return ':'.join(hex(i)[2:].zfill(2).upper() for i in range(16))\n",
      "==========score: 1.0===============\n",
      "async def deserialize(data: dict):\n",
      "        \"\"\"\n",
      "        Builds a Proof object with defined attributes.\n",
      "        Attributes are provided by a previous call to the serialize function.\n",
      "        :param data:\n",
      "        Example:\n",
      "        name = \"proof name\"\n",
      "        requested_attrs = [{\"name\": \"age\", \"restrictions\": [{\"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\" } ] }, { \"name\":\"name\", \"restrictions\": [ { \"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\"}]}]\n",
      "        proof = await Proof.create(source_id, name, requested_attrs)\n",
      "        data = proof.serialize()\n",
      "        proof2 = await Proof.deserialize(data)\n",
      "        :return: Proof Object\n",
      "        \"\"\"\n",
      "        proof = Proof(source_id, name, requested_attrs)\n",
      "        proof.deserialize(data)\n",
      "        return proof\n",
      "\n",
      "    @staticmethod\n",
      "    async def create(source_id: str, name: str, requested_attrs: list):\n",
      "        \"\"\"\n",
      "        Creates a Proof object with defined attributes.\n",
      "        Attributes are provided by a previous call to the serialize function.\n",
      "        :param source_id:\n",
      "        :param name:\n",
      "        :param requested_attrs:\n",
      "        :return: Proof Object\n",
      "        \"\"\"\n",
      "        proof = Proof(source_id, name, requested_attrs)\n",
      "        proof.create(requested_attrs)\n",
      "        return proof\n",
      "\n",
      "    @staticmethod\n",
      "    async def create_from_json(data: dict):\n",
      "        \"\"\"\n",
      "        Creates a Proof object with defined attributes.\n",
      "        Attributes are provided by a previous call to the serialize function.\n",
      "        :param data:\n",
      "        :return: Proof Object\n",
      "        \"\"\"\n",
      "        proof = Proof(data[\"source_id\"], data[\"name\"], data[\"requested_attrs\"])\n",
      "        proof.deserialize(data)\n",
      "\n",
      "==========score: 1.0===============\n",
      "def run(args):\n",
      "    \"\"\"\n",
      "    Args:\n",
      "        args (argparse.Namespace)\n",
      "    \"\"\"\n",
      "    if args.verbose:\n",
      "        logging.basicConfig(level=logging.DEBUG)\n",
      "    else:\n",
      "        logging.basicConfig(level=logging.INFO)\n",
      "\n",
      "    if args.config_file:\n",
      "        with open(args.config_file, 'r') as f:\n",
      "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
      "    else:\n",
      "        config = {}\n",
      "\n",
      "    if args.config_file_override:\n",
      "        with open(args.config_file_override, 'r') as f:\n",
      "            config.update(yaml.load(f, Loader=yaml.FullLoader))\n",
      "\n",
      "    if args.config_file_override_update:\n",
      "        with open(args.config_file_override_update, 'r') as f:\n",
      "            config.update(yaml.load(f, Loader=yaml.FullLoader))\n",
      "\n",
      "    if args.config_file_override_update_update:\n",
      "        with open(args.config_file_override_update_update, 'r') as f:\n",
      "            config.update(y\n",
      "==========score: 1.0===============\n",
      "def html_to_pdf(content, encoding=\"utf-8\",\n",
      "                link_callback=fetch_resources, **kwargs):\n",
      "    \"\"\"\n",
      "    Converts html ``content`` into PDF document.\n",
      "\n",
      "    :param unicode content: html content\n",
      "    :returns: PDF content\n",
      "    :rtype: :class:`bytes`\n",
      "    :raises: :exc:`~easy_pdf.exceptions.PDFRenderingError`\n",
      "    \"\"\"\n",
      "    return render_to_pdf(content, encoding, link_callback, **kwargs)\n",
      "==========score: 1.0===============\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for data in conf_mat['tp']:\n",
    "    print(data['func'])\n",
    "    print(f\"==========score: {round(data['score'],4)}===============\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from utils import load_json_dataset\n",
    "human_code = load_json_dataset(\"dataset/csn/human_code_prompt_extracted.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(13945, 15264)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_line = 5\n",
    "max_line = 50\n",
    "cnt = 0\n",
    "selected = []\n",
    "for data in human_code:\n",
    "    completion_start = len(data['prompt'])\n",
    "    lines = data['human_code'][completion_start:].split(\"\\n\")\n",
    "    line_cnt = 0\n",
    "    for line in lines:\n",
    "        if line not in [\"\\n\",\"\"]:\n",
    "           line_cnt += 1\n",
    "    if line_cnt >= min_line and line_cnt <= max_line:\n",
    "        cnt += 1\n",
    "        selected.append(data)\n",
    "\n",
    "cnt,len(human_code)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import json\n",
    "gen_dataset = load_json_dataset(\"dataset/csn/codegen-2B-mono-0.2-0.95-256-10.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "min_line = 10\n",
    "max_line = 50\n",
    "for data in gen_dataset:\n",
    "    new_comp = []\n",
    "    for comp in data['completions']:\n",
    "        lines = comp.split(\"\\n\")\n",
    "        line_cnt = 0\n",
    "        for line in lines:\n",
    "            if line not in [\"\\n\",\"\"]:\n",
    "               line_cnt += 1\n",
    "            if line_cnt >= min_line and line_cnt <= max_line:\n",
    "                new_comp.append(comp)\n",
    "\n",
    "    data['completions'] = new_comp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "1594"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_gen_dataset = []\n",
    "for data in gen_dataset:\n",
    "    unique_set = set(data['completions'])\n",
    "    for comp in unique_set:\n",
    "        flat_gen_dataset.append({\"func_str\":data['prompt'] + comp,\"label\":1})\n",
    "len(flat_gen_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "{'func_str': \"def correct_word(word_string):\\n    '''\\n    Finds all valid one and two letter corrections for word_string, returning the word\\n    with the highest relative probability as type str.\\n    '''\\n    # TODO: Implement this function.\\n    # Hint: You should use the helper function correct_word_helper() to find the best\\n    #       one and two letter correction for a word.\\n    # Hint: You should use the helper function get_word_probability_helper() to find the\\n    #       probability of a word.\\n    # Hint: You should use the helper function get_word_probability_helper() to find the\\n    #       probability of a word.\\n    # Hint: You should use the helper function get_word_probability_helper() to find the\\n    #       probability of a word.\\n    # Hint: You should use the helper function get_word_probability_helper() to find the\\n    #       probability of a word.\\n    # Hint: You should use the helper function get_word_probability_helper() to find the\\n    #       probability of a word.\\n    # Hint: You should use the helper function get_word_probability_helper() to find\",\n 'label': 1}"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_gen_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['source','model_name','prompt','completion','top_p','temp','top_k','completion_max_token','label','full_code'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "17780"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_json_dataset\n",
    "source = \"humaneval\"\n",
    "temp = 0.2\n",
    "top_p = 1.0\n",
    "top_k = None\n",
    "max_token = 350\n",
    "n = 5\n",
    "model_name = \"codegen-2B-mono\"\n",
    "\n",
    "gen_data = load_json_dataset(f\"dataset/{source}/{model_name}-{temp}-{top_p}-{max_token}-{n}.json\")\n",
    "for data in gen_data:\n",
    "    prompt = data['prompt']\n",
    "    unique_set = set(data['completions'])\n",
    "    for comp in unique_set:\n",
    "        new_row = {\"source\":[source],\"model_name\":[model_name],\"prompt\":[prompt],\"completion\":[comp],\"top_p\":[top_p],\"temp\":[temp],\"top_k\":[top_k],\"completion_max_token\":[max_token],\"label\":[1],\"full_code\":[prompt + comp]}\n",
    "        new_row = pd.DataFrame.from_dict(new_row)\n",
    "        df = pd.concat([df,new_row])\n",
    "\n",
    "len(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "22752"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_json_dataset\n",
    "source = \"csn\"\n",
    "temp = 0.5\n",
    "top_p = 1.0\n",
    "top_k = None\n",
    "max_token = 350\n",
    "n = 5\n",
    "model_name = \"code_davinci_002\"\n",
    "\n",
    "gen_data = load_json_dataset(f\"dataset/{source}/{model_name}-{temp}-{top_p}-{max_token}-{n}_response.json\")\n",
    "for data in gen_data:\n",
    "    prompt = data['prompt']\n",
    "    unique_set = set([item['text'] for item in data['raw_response']['choices']])\n",
    "    for comp in unique_set:\n",
    "        new_row = {\"source\":[source],\"model_name\":[model_name],\"prompt\":[prompt],\"completion\":[comp],\"top_p\":[top_p],\"temp\":[temp],\"top_k\":[top_k],\"completion_max_token\":[max_token],\"label\":[1],\"full_code\":[prompt + comp]}\n",
    "        new_row = pd.DataFrame.from_dict(new_row)\n",
    "        df = pd.concat([df,new_row])\n",
    "\n",
    "len(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "      source        model_name  \\\n0  humaneval  code_davinci_002   \n\n                                              prompt  \\\n0  \\ndef generate_integers(a, b):\\n    \"\"\"\\n    G...   \n\n                                          completion  top_p  temp top_k  \\\n0      if a > b:\\n        a, b = b, a\\n    return...    1.0   0.2  None   \n\n   completion_max_token  label  \\\n0                   350      1   \n\n                                           full_code  \n0  \\ndef generate_integers(a, b):\\n    \"\"\"\\n    G...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>model_name</th>\n      <th>prompt</th>\n      <th>completion</th>\n      <th>top_p</th>\n      <th>temp</th>\n      <th>top_k</th>\n      <th>completion_max_token</th>\n      <th>label</th>\n      <th>full_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>humaneval</td>\n      <td>code_davinci_002</td>\n      <td>\\ndef generate_integers(a, b):\\n    \"\"\"\\n    G...</td>\n      <td>if a &gt; b:\\n        a, b = b, a\\n    return...</td>\n      <td>1.0</td>\n      <td>0.2</td>\n      <td>None</td>\n      <td>350</td>\n      <td>1</td>\n      <td>\\ndef generate_integers(a, b):\\n    \"\"\"\\n    G...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_row"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "df.to_pickle(\"dataset/all_gen_code.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hum_df = pd.DataFrame(columns=['source','prompt','completion','label','full_code'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "from utils import load_json_dataset\n",
    "human_code = load_json_dataset(\"dataset/csn/human_code_prompt_extracted.json\")\n",
    "new_data = []\n",
    "for data in human_code:\n",
    "    completion_start = len(data['prompt'])\n",
    "    offset = data['human_code'][completion_start:].find('\"\"\"')\n",
    "    if offset == -1:\n",
    "        offset = data['human_code'][completion_start:].find(\"'''\")\n",
    "    if offset != -1:\n",
    "        completion_start += offset + 3\n",
    "        data['prompt'] = data['human_code'][:completion_start]\n",
    "        data['completion'] = data['human_code'][completion_start:]\n",
    "        new_data.append(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "for data in new_data:\n",
    "    new_row = {\"source\":[\"csn\"],\"prompt\":[data['prompt']],\"completion\":[data['completion']],\"label\":[0],\"full_code\":[data['human_code']]}\n",
    "    new_row = pd.DataFrame(new_row)\n",
    "    hum_df = pd.concat([hum_df,new_row])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "hum_df.to_pickle(\"dataset/all_human_code.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"dataset/all_gen_code.pkl\")\n",
    "hum_df = pd.read_pickle(\"dataset/all_human_code.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "def comp_line_cnt(x):\n",
    "    lines = x.split(\"\\n\")\n",
    "    cnt = 0\n",
    "    for line in lines:\n",
    "        if line not in [\"\\n\",\"\"]:\n",
    "            cnt += 1\n",
    "\n",
    "    return cnt\n",
    "\n",
    "def input_id_len(x):\n",
    "    input_id = tokenizer(x,add_special_tokens=True).input_ids\n",
    "    return len(input_id)\n",
    "\n",
    "df[\"completion_line_num\"] = df[\"completion\"].apply(comp_line_cnt)\n",
    "hum_df[\"completion_line_num\"] = hum_df[\"completion\"].apply(comp_line_cnt)\n",
    "df[\"prompt_line_num\"] = df[\"prompt\"].apply(comp_line_cnt)\n",
    "hum_df[\"prompt_line_num\"] = hum_df[\"prompt\"].apply(comp_line_cnt)\n",
    "# df['input_id_len'] = df['full_code'].apply(input_id_len)\n",
    "# hum_df['input_id_len'] = hum_df['full_code'].apply(input_id_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "slice = df[((df[\"completion_line_num\"] >= 5) & (df[\"completion_line_num\"] < 25)& (df[\"source\"] == \"csn\"))]\n",
    "slice = slice.drop_duplicates(subset=['full_code'],keep=\"first\")\n",
    "\n",
    "# slice['completion_line_num'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "slice_hm = hum_df[((hum_df[\"completion_line_num\"] >= 5) & (hum_df[\"completion_line_num\"] < 25)) & (hum_df[\"source\"] == \"csn\")]\n",
    "slice_hm = slice_hm.drop_duplicates(subset=['full_code'],keep=\"first\")\n",
    "# slice_hm['completion_line_num'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "## generate dataset split\n",
    "slice = slice.sample(frac=1)\n",
    "slice_hm = slice_hm.sample(frac=1)\n",
    "train_split = pd.concat([slice.iloc[:1000],slice_hm.iloc[:7000]])\n",
    "valid_split = pd.concat([slice.iloc[1000:1200],slice_hm.iloc[7000:8400]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "data": {
      "text/plain": "count                                                  8000\nunique                                                 8000\ntop       def get_binary_path(executable, logging_level=...\nfreq                                                      1\nName: full_code, dtype: object"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split['full_code'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "test_slice = df[((df[\"completion_line_num\"] >= 5) & (df[\"completion_line_num\"] < 25)& (df[\"source\"] == \"humaneval\"))]\n",
    "test_slice = test_slice.drop_duplicates(subset=['full_code'],keep=\"first\")\n",
    "test_slice = test_slice.sample(frac=1)\n",
    "test_split = pd.concat([test_slice.iloc[:3000],slice_hm.iloc[8400:11400]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "train_split.to_pickle(\"detection_data/train_csn_8k.pkl\")\n",
    "valid_split.to_pickle(\"detection_data/valid_csn_8k.pkl\")\n",
    "test_split.to_pickle(\"detection_data/test_he+csn_6k.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "data": {
      "text/plain": "'csn'"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split.iloc[0]['source']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def selection(df,source=None,model_name=None,temp=None,top_p=None,top_k=None,\n",
    "              completion_line_min=None,completion_line_max=None,\n",
    "              prompt_line_min=None,prompt_line_max=None):\n",
    "    condition_df = ~df['source'].isna()\n",
    "    if source:\n",
    "        condition_df &= df.source == source\n",
    "    if model_name:\n",
    "        condition_df &= df.model_name == model_name\n",
    "    if temp:\n",
    "        condition_df &= df.temp == temp\n",
    "    if top_p:\n",
    "        condition_df &= df.top_p == top_p\n",
    "    if top_k:\n",
    "        condition_df &= df.top_k == top_k\n",
    "    if completion_line_min:\n",
    "        condition_df &= (df.completion_line_num >= completion_line_min)\n",
    "    if completion_line_max:\n",
    "        condition_df &= (df.completion_line_num <= completion_line_max)\n",
    "    if prompt_line_min:\n",
    "        condition_df &= (df.prompt_line_num >= prompt_line_min)\n",
    "    if prompt_line_max:\n",
    "        condition_df &= (df.prompt_line_num <= prompt_line_max)\n",
    "\n",
    "    return df[condition_df]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(2100, 600, 1448)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainvalid_gen = selection(df,source='csn',completion_line_min=5,completion_line_max=30,temp=0.8).sample(frac=1.0)\n",
    "train_gen = trainvalid_gen.iloc[:2100]\n",
    "valid_gen = trainvalid_gen.iloc[2100:]\n",
    "test_gen = selection(df,source='humaneval',model_name=\"code_davinci_002\",completion_line_min=5,completion_line_max=30)\n",
    "len(train_gen),len(valid_gen),len(test_gen)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "   source       model_name                                             prompt  \\\n0     csn  codegen-2B-mono  def get_process_info(self, pid=None):\\n       ...   \n0     csn  codegen-2B-mono  def find_number(regex, s):\\n    \"\"\"Find a numb...   \n0     csn  codegen-2B-mono  def draw(self, size=None, cmap=\"jet\"):\\n      ...   \n0     csn  codegen-2B-mono  def cluster_mini_batch_kmeans(data=None, k=100...   \n0     csn  codegen-2B-mono  def __get_current_datetime(self):\\n        \"\"\"...   \n..    ...              ...                                                ...   \n0     csn  codegen-2B-mono  def scatter(x, y, **kwargs):\\n    \"\"\"Draw a sc...   \n0     csn  codegen-2B-mono  def get_figure(self, heatmap_kw=None, **kwargs...   \n0     csn  codegen-2B-mono  def replace_text(filepath, to_replace, replace...   \n0     csn  codegen-2B-mono  def memoize(f):\\n    \"\"\"Cache results of compu...   \n0     csn  codegen-2B-mono  def extract_zip(zip_name, exclude_term=None):\\...   \n\n                                           completion top_p temp top_k  \\\n0   \\n        req_data = {'key': self.key}\\n      ...   1.0  0.8  None   \n0   \\n    import re\\n    pattern = re.compile(rege...   1.0  0.8  None   \n0   \\n        heatmaps_i = self.to_keypoints()\\n  ...   1.0  0.8  None   \n0   \\n    return MiniBatchKmeansClustering(coordin...   1.0  0.8  None   \n0   \\n        return self.__current_datetime\\n\\n  ...   1.0  0.8  None   \n..                                                ...   ...  ...   ...   \n0   \\n    from matplotlib.collections import PathC...   1.0  0.8  None   \n0   \\n        return self.get_heatmap(self.data, h...   1.0  0.8  None   \n0   \\n        with open(filepath, \"r+\") as f:\\n   ...   1.0  0.8  None   \n0   \\n    cache = {}\\n\\n    def helper(x):\\n      ...   1.0  0.8  None   \n0   \\n    assert zip_name, \"zip_name parameter mus...   1.0  0.8  None   \n\n   completion_max_token label  \\\n0                   256     1   \n0                   256     1   \n0                   256     1   \n0                   256     1   \n0                   256     1   \n..                  ...   ...   \n0                   256     1   \n0                   256     1   \n0                   256     1   \n0                   256     1   \n0                   256     1   \n\n                                            full_code  completion_line_num  \\\n0   def get_process_info(self, pid=None):\\n       ...                   18   \n0   def find_number(regex, s):\\n    \"\"\"Find a numb...                    9   \n0   def draw(self, size=None, cmap=\"jet\"):\\n      ...                   15   \n0   def cluster_mini_batch_kmeans(data=None, k=100...                    5   \n0   def __get_current_datetime(self):\\n        \"\"\"...                   18   \n..                                                ...                  ...   \n0   def scatter(x, y, **kwargs):\\n    \"\"\"Draw a sc...                   20   \n0   def get_figure(self, heatmap_kw=None, **kwargs...                   20   \n0   def replace_text(filepath, to_replace, replace...                   19   \n0   def memoize(f):\\n    \"\"\"Cache results of compu...                    6   \n0   def extract_zip(zip_name, exclude_term=None):\\...                   16   \n\n    prompt_line_num  \n0                 7  \n0                12  \n0                18  \n0                27  \n0                 2  \n..              ...  \n0                17  \n0                14  \n0                 7  \n0                 2  \n0                 2  \n\n[1615 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>model_name</th>\n      <th>prompt</th>\n      <th>completion</th>\n      <th>top_p</th>\n      <th>temp</th>\n      <th>top_k</th>\n      <th>completion_max_token</th>\n      <th>label</th>\n      <th>full_code</th>\n      <th>completion_line_num</th>\n      <th>prompt_line_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>csn</td>\n      <td>codegen-2B-mono</td>\n      <td>def get_process_info(self, pid=None):\\n       ...</td>\n      <td>\\n        req_data = {'key': self.key}\\n      ...</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>None</td>\n      <td>256</td>\n      <td>1</td>\n      <td>def get_process_info(self, pid=None):\\n       ...</td>\n      <td>18</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>csn</td>\n      <td>codegen-2B-mono</td>\n      <td>def find_number(regex, s):\\n    \"\"\"Find a numb...</td>\n      <td>\\n    import re\\n    pattern = re.compile(rege...</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>None</td>\n      <td>256</td>\n      <td>1</td>\n      <td>def find_number(regex, s):\\n    \"\"\"Find a numb...</td>\n      <td>9</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>csn</td>\n      <td>codegen-2B-mono</td>\n      <td>def draw(self, size=None, cmap=\"jet\"):\\n      ...</td>\n      <td>\\n        heatmaps_i = self.to_keypoints()\\n  ...</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>None</td>\n      <td>256</td>\n      <td>1</td>\n      <td>def draw(self, size=None, cmap=\"jet\"):\\n      ...</td>\n      <td>15</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>csn</td>\n      <td>codegen-2B-mono</td>\n      <td>def cluster_mini_batch_kmeans(data=None, k=100...</td>\n      <td>\\n    return MiniBatchKmeansClustering(coordin...</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>None</td>\n      <td>256</td>\n      <td>1</td>\n      <td>def cluster_mini_batch_kmeans(data=None, k=100...</td>\n      <td>5</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>csn</td>\n      <td>codegen-2B-mono</td>\n      <td>def __get_current_datetime(self):\\n        \"\"\"...</td>\n      <td>\\n        return self.__current_datetime\\n\\n  ...</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>None</td>\n      <td>256</td>\n      <td>1</td>\n      <td>def __get_current_datetime(self):\\n        \"\"\"...</td>\n      <td>18</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>csn</td>\n      <td>codegen-2B-mono</td>\n      <td>def scatter(x, y, **kwargs):\\n    \"\"\"Draw a sc...</td>\n      <td>\\n    from matplotlib.collections import PathC...</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>None</td>\n      <td>256</td>\n      <td>1</td>\n      <td>def scatter(x, y, **kwargs):\\n    \"\"\"Draw a sc...</td>\n      <td>20</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>csn</td>\n      <td>codegen-2B-mono</td>\n      <td>def get_figure(self, heatmap_kw=None, **kwargs...</td>\n      <td>\\n        return self.get_heatmap(self.data, h...</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>None</td>\n      <td>256</td>\n      <td>1</td>\n      <td>def get_figure(self, heatmap_kw=None, **kwargs...</td>\n      <td>20</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>csn</td>\n      <td>codegen-2B-mono</td>\n      <td>def replace_text(filepath, to_replace, replace...</td>\n      <td>\\n        with open(filepath, \"r+\") as f:\\n   ...</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>None</td>\n      <td>256</td>\n      <td>1</td>\n      <td>def replace_text(filepath, to_replace, replace...</td>\n      <td>19</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>csn</td>\n      <td>codegen-2B-mono</td>\n      <td>def memoize(f):\\n    \"\"\"Cache results of compu...</td>\n      <td>\\n    cache = {}\\n\\n    def helper(x):\\n      ...</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>None</td>\n      <td>256</td>\n      <td>1</td>\n      <td>def memoize(f):\\n    \"\"\"Cache results of compu...</td>\n      <td>6</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>csn</td>\n      <td>codegen-2B-mono</td>\n      <td>def extract_zip(zip_name, exclude_term=None):\\...</td>\n      <td>\\n    assert zip_name, \"zip_name parameter mus...</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>None</td>\n      <td>256</td>\n      <td>1</td>\n      <td>def extract_zip(zip_name, exclude_term=None):\\...</td>\n      <td>16</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>1615 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection(train_gen,model_name='codegen-2B-mono')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "(7000, 1500, 3877)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = selection(hum_df,source='csn',completion_line_min=5,completion_line_max=30)\n",
    "selected = selected.sample(frac=1)\n",
    "train_he = selected.iloc[:7000]\n",
    "valid_he = selected.iloc[7000:8500]\n",
    "test_he = selected.iloc[8500:]\n",
    "len(train_he),len(valid_he),len(test_he)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "train = pd.concat([train_gen,train_he])\n",
    "valid = pd.concat([valid_gen,valid_he])\n",
    "test = pd.concat([test_gen,test_he])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "train.to_pickle(\"detection_data/train_csn_temp0.8.pkl\")\n",
    "valid.to_pickle(\"detection_data/valid_csn_temp0.8.pkl\")\n",
    "# test.to_pickle(\"detection_data/test_csn_temp0.5.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}