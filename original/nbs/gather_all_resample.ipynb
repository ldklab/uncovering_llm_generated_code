{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"/home/yangkai/codegen-detection\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hum_data = pd.read_pickle(\"dataset/mbpp_hum_test100.pkl\")\n",
    "gen_data = pd.read_pickle(\"dataset/mbpp_gen_test100_gpt4.pkl\")\n",
    "\n",
    "# hum_data = pd.read_pickle(\"dataset/csn_1k_hum_test.pkl\")\n",
    "# gen_data = pd.read_pickle(\"dataset/csn_gpt_gen_test.pkl\")\n",
    "hum_data['label'] = 0\n",
    "gen_data['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangkai/anaconda3/envs/torch20-cu117/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/yangkai/anaconda3/envs/torch20-cu117/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/yangkai/anaconda3/envs/torch20-cu117/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/yangkai/anaconda3/envs/torch20-cu117/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangkai/anaconda3/envs/torch20-cu117/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/yangkai/anaconda3/envs/torch20-cu117/lib/libcudart.so.11.0'), PosixPath('/home/yangkai/anaconda3/envs/torch20-cu117/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-26 17:20:35,549] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "{'pooler_type': 'cls', 'temp': 0.1, 'mlp_only_train': True}\n"
     ]
    }
   ],
   "source": [
    "from simcse.simcse import RobertaForCL\n",
    "from transformers import AutoTokenizer\n",
    "device = 1\n",
    "model_args = {'pooler_type':'cls','temp':0.1,'mlp_only_train':True}\n",
    "# model = RobertaForCL.from_pretrained(\"simcse/ckpt_gcb4_csn/best_model\",**model_args)\n",
    "model = RobertaForCL.from_pretrained(\"simcse/ckpt_gcb4_codenet_py_no_overlap/best_model\",**model_args)\n",
    "\n",
    "# model_args = {'pooler_type':'avg','temp':0.1,'mlp_only_train':True}\n",
    "# model = RobertaForCL.from_pretrained(\"microsoft/graphcodebert-base\",**model_args)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "bsz = 32\n",
    "\n",
    "@torch.inference_mode()\n",
    "def pipeline(texts):\n",
    "    inputs = tokenizer(texts,return_tensors='pt',padding=True,add_special_tokens=True,truncation=True)\n",
    "    inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "    sent_embed = model(**inputs,sent_emb=True).pooler_output\n",
    "\n",
    "    return sent_embed\n",
    "\n",
    "@torch.inference_mode()\n",
    "def batch_encode(texts):\n",
    "    embeds = []\n",
    "    for i in tqdm(range(0,len(texts),bsz)):\n",
    "        chunk = texts[i:i+bsz]\n",
    "        embeds.append(pipeline(chunk))\n",
    "    \n",
    "    return torch.cat(embeds,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_resample_keys = ['gpt_resample-nocot_code_cleaned1','gpt_resample-nocot_code_cleaned2']\n",
    "# gen_resample_keys = ['vicuna_resampled_code_cleaned','vicuna_resampled_code_cleaned2'] + [f'gpt_resample_code_cleaned{i}' for i in range(3,9)]\n",
    "gen_resample_keys = [f'gpt_resample_code_cleaned{i}' for i in range(1,9)]\n",
    "# gen_resample_keys = [f'vicuna7b_resample_code_cleaned{i}' for i in range(1,9)]\n",
    "gen_orig_key = 'extracted_full_func'\n",
    "\n",
    "hum_resample_keys = ['vicuna_resampled_code_cleaned','vicuna_resampled_code_cleaned2'] + [f'gpt_resample_code_cleand{i}' for i in range(3,9)]\n",
    "# hum_resample_keys = [f'gpt_resample_code_cleaned{i}' for i in range(1,9)]\n",
    "# hum_resample_keys = [f'vicuna7b_resample_code_cleaned{i}' for i in range(1,9)]\n",
    "hum_orig_key = 'extracted_full_func'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem_id\n",
      "question\n",
      "solutions\n",
      "difficulty\n",
      "url\n",
      "starter_code\n",
      "apps_split\n",
      "code_source\n",
      "solution\n",
      "extracted_full_func\n",
      "extracted_full_func_word_prob_CodeLlama-13b-Instruct-hf\n",
      "extracted_full_func_word_rank_CodeLlama-13b-Instruct-hf\n",
      "extracted_full_func_entropy_CodeLlama-13b-Instruct-hf\n",
      "extracted_full_func_word_prob_starchat-alpha\n",
      "extracted_full_func_word_rank_starchat-alpha\n",
      "extracted_full_func_entropy_starchat-alpha\n",
      "extracted_full_func_word_prob_incoder-6B\n",
      "extracted_full_func_word_rank_incoder-6B\n",
      "extracted_full_func_entropy_incoder-6B\n",
      "extracted_full_func_word_prob_codegen-2B-mono\n",
      "extracted_full_func_word_rank_codegen-2B-mono\n",
      "extracted_full_func_entropy_codegen-2B-mono\n",
      "extracted_full_func_word_prob_PolyCoder-160M\n",
      "extracted_full_func_word_rank_PolyCoder-160M\n",
      "extracted_full_func_entropy_PolyCoder-160M\n",
      "label\n"
     ]
    }
   ],
   "source": [
    "for name in gen_data.columns:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6100000000000001"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "def load_openai_embed(path):\n",
    "    response = [json.loads(line.strip()) for line in open(path,'r').readlines()]\n",
    "    all_embeds = [item['data'][0]['embedding'] for item in response]\n",
    "    return torch.FloatTensor(all_embeds)\n",
    "\n",
    "hum_orig = load_openai_embed(\"prompt_gpt/apps_hum_test100_orig_openaiemb.jsonl\")\n",
    "gen_orig = load_openai_embed(\"prompt_gpt/apps_gen_test100_orig_openaiemb.jsonl\")\n",
    "orig = torch.cat([gen_orig,hum_orig])\n",
    "\n",
    "hum_regpt1 = load_openai_embed(\"prompt_gpt/apps_hum_test100_re3_openaiemb.jsonl\")\n",
    "gen_regpt1 = load_openai_embed(\"prompt_gpt/apps_gen_test100_re3_openaiemb.jsonl\")\n",
    "regpt1 = torch.cat([gen_regpt1,hum_regpt1])\n",
    "\n",
    "hum_regpt2 = load_openai_embed(\"prompt_gpt/apps_hum_test100_re4_openaiemb.jsonl\")\n",
    "gen_regpt2 = load_openai_embed(\"prompt_gpt/apps_gen_test100_re4_openaiemb.jsonl\")\n",
    "regpt2 = torch.cat([gen_regpt2,hum_regpt2])\n",
    "\n",
    "hum_resc1 = load_openai_embed(\"prompt_gpt/apps_hum_test100_reStarcoder1_openaiemb.jsonl\")\n",
    "gen_resc1 = load_openai_embed(\"prompt_gpt/apps_gen_test100_reStarcoder1_openaiemb.jsonl\")\n",
    "resc1 = torch.cat([gen_resc1,hum_resc1])\n",
    "\n",
    "hum_resc2 = load_openai_embed(\"prompt_gpt/apps_hum_test100_reStarcoder2_openaiemb.jsonl\")\n",
    "gen_resc2 = load_openai_embed(\"prompt_gpt/apps_gen_test100_reStarcoder2_openaiemb.jsonl\")\n",
    "resc2 = torch.cat([gen_resc2,hum_resc2])\n",
    "\n",
    "\n",
    "gpt_sim1 = (orig * regpt1).sum(-1)\n",
    "gpt_sim2 = (orig * regpt2).sum(-1)\n",
    "gpt_sim = (gpt_sim1 + gpt_sim2) / 2\n",
    "\n",
    "sc_sim1 = (orig * resc1).sum(-1)\n",
    "sc_sim2 = (orig * resc2).sum(-1)\n",
    "sc_sim = (sc_sim1 + sc_sim2) / 2\n",
    "\n",
    "roc_auc_score([1] * 100 + [0] * 100, sc_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(data,resample_keys,orig_key):\n",
    "    orig_embed = batch_encode(data[orig_key].tolist())\n",
    "    resample_embeds = []\n",
    "    for key in resample_keys:\n",
    "        resample_embeds.append(batch_encode(data[key].tolist()))\n",
    "    \n",
    "    all_sim = []\n",
    "    normed_orig_embed = torch.nn.functional.normalize(orig_embed,p=2,dim=-1)\n",
    "    for embed in resample_embeds:\n",
    "        normed_embed = torch.nn.functional.normalize(embed,p=2,dim=-1)\n",
    "        sim = (normed_orig_embed * normed_embed).sum(-1)\n",
    "        all_sim.append(sim)\n",
    "    \n",
    "    return all_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  5.75it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.97it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  6.50it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  6.19it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  6.11it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.84it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  6.04it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.92it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.86it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.86it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.87it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.81it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  6.11it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.81it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.84it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.94it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.64it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.79it/s]\n"
     ]
    }
   ],
   "source": [
    "hum_sim = get_sim(hum_data,hum_resample_keys,hum_orig_key)\n",
    "gen_sim = get_sim(gen_data,gen_resample_keys,gen_orig_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8202"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "all_data = pd.concat([gen_data,hum_data])\n",
    "all_sim = [torch.concat([sim1,sim2]) for sim1,sim2 in zip(gen_sim,hum_sim)]\n",
    "m = 2\n",
    "score = sum([all_sim[i] for i in range(m)]) / m\n",
    "# score = (all_sim[0] + all_sim[1]) / 2\n",
    "\n",
    "label = all_data['label'].tolist()\n",
    "score = score.tolist()\n",
    "\n",
    "roc_auc = roc_auc_score(label,score)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5259921259842519"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no cot resample apps 81.21\n",
    "# cot resample apps 78.49\n",
    "\n",
    "# direct gen code 80.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ident replaced 0.2 64.57\n",
    "# ident replaced 0.5 52.00\n",
    "# ident replaced 0.1 74.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPS GCB avg_last GPT-3.5-turbo\n",
    "# m = 2 70.23 67.91(100)\n",
    "# m = 4 71.53\n",
    "# m = 8 72.15\n",
    "\n",
    "# APPS GCB avg_last StarCoder\n",
    "# m = 2 66.98 65.22(100)\n",
    "# m = 4 66.19\n",
    "# m = 8 64.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBPP GCB avg_last GPT-3.5-turbo\n",
    "# m = 2 75.89\n",
    "# m = 4 77.09\n",
    "# m = 8 76.94\n",
    "\n",
    "# MBPP GCB avg_last StarCoder\n",
    "# m = 2 80.02\n",
    "# m = 4 82.60\n",
    "# m = 8 81.04"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20-cu117",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
