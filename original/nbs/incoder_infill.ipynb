{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/yangkai/codegen-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangkai/anaconda3/envs/vllm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../llm_checkpoints/incoder-6B/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../llm_checkpoints/incoder-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 6\n",
    "torch.cuda.set_device(device=device)\n",
    "model = model.half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signals the start of a document\n",
    "BOS = \"<|endoftext|>\"\n",
    "# signals the end of a generated infill\n",
    "EOM = \"<|endofmask|>\"\n",
    "\n",
    "def make_sentinel(i):\n",
    "    # signals (1) a location to insert an infill and (2) the start of the infill generation\n",
    "    return f\"<|mask:{i}|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(input: str, max_to_generate: int=128, temperature: float=0.2):\n",
    "    \"\"\"\n",
    "    Do standard left-to-right completion of the prefix `input` by sampling from the model\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.cuda()\n",
    "    max_length = max_to_generate + input_ids.flatten().size(0)\n",
    "    if max_length > 2048:\n",
    "        print(\"warning: max_length {} is greater than the context window {}\".format(max_length, 2048))\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids=input_ids, do_sample=True, top_p=0.95, temperature=temperature, max_length=max_length)\n",
    "    # pass clean_up_tokenization_spaces=False to avoid removing spaces before punctuation, e.g. \"from .\" -> \"from.\"\n",
    "    detok_hypo_str = tokenizer.decode(output.flatten(), clean_up_tokenization_spaces=False)\n",
    "    if detok_hypo_str.startswith(BOS):\n",
    "        detok_hypo_str = detok_hypo_str[len(BOS):]\n",
    "    return detok_hypo_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mask_and_infill(input_str,mask_lines=8,temperature=0.8,max_to_generate=128,max_retries=2):\n",
    "    lines = input_str.split(\"\\n\")\n",
    "    mask_lines = min(mask_lines,len(lines) // 2)\n",
    "    if mask_lines == 0:\n",
    "        return input_str\n",
    "    \n",
    "    done = False\n",
    "    retries_attempted = 0\n",
    "    while (not done) and (retries_attempted < max_retries):\n",
    "        retries_attempted += 1\n",
    "        lines = input_str.split(\"\\n\")\n",
    "        masked_idx = random.sample(list(range(len(lines))),mask_lines)\n",
    "        masked_idx.sort()\n",
    "        for sentital_idx,idx in enumerate(masked_idx):\n",
    "            lines[idx] = make_sentinel(sentital_idx)\n",
    "        \n",
    "        masked_input = \"\\n\".join(lines)\n",
    "        done = True\n",
    "\n",
    "        # for sentinel_ix in range(mask_lines):\n",
    "        prompt = masked_input + make_sentinel(0)\n",
    "        # TODO: this is inefficient as it requires re-encoding prefixes repeatedly\n",
    "        # print(prompt)\n",
    "        completion = generate(prompt, max_to_generate, temperature)\n",
    "        completion = completion[len(prompt):]\n",
    "        # print(completion)\n",
    "        # if EOM not in completion:\n",
    "        #     print(f\"warning: {EOM} not found\")\n",
    "        #     completion += EOM\n",
    "        #     done = False\n",
    "        # completion = completion[:completion.index(EOM) + len(EOM)]\n",
    "        # infilled = completion[:-len(EOM)]\n",
    "        infilled = completion\n",
    "        # prompt += completion\n",
    "        \n",
    "    # for sentinel_ix,infill in enumerate(infills):\n",
    "    #     lines[masked_idxs[sentinel_ix]] = infill\n",
    "    \n",
    "    return infilled,prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \\\n",
    "'''\n",
    "def count_words(filename):\n",
    "    \"\"\" Count words in a file. \"\"\"\n",
    "    counts = Counter()\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            words = line.split(' ')\n",
    "            counts.update(words)\n",
    "    return counts\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "infill,masked_input = mask_and_infill(test_str,max_to_generate=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"\"\"Count the number of lines in a file.\"\"\"\n",
      "    with open(file_path) as file:\n",
      "        total = sum(1 for _ in file)\n",
      "        return total\n",
      "\n",
      "\n",
      "def count_words(file_path):<|endofmask|><|endofmask|><|endofmask|><|endofmask|>\n",
      "        with open(file_path) as file:\n",
      "            total = sum(1 for _ in file)\n",
      "            return total\n",
      "\n",
      "\n",
      "def word_freq(file_path):\n",
      "    \"\"\" Count the frequency of words in a file. \"\"\"\n",
      "    with open(file_path) as file:\n",
      "        counts = defaultdict(int)<|endofmask|><|endofmask|><|endofmask|>\n",
      "        for line in file:\n",
      "            words = line.split(' ')\n",
      "            counts.update(words)\n",
      "        return counts\n",
      "\n",
      "\n",
      "def word_freq_avg(file_path):\n",
      "    \"\"\" Count the frequency of words in a file. \"\"\"\n",
      "    with open(file_path) as file:\n",
      "        counts = defaultdict(int)\n",
      "        for line in file:\n",
      "            words = line.split(' ')\n",
      "            counts.update(words)\n",
      "        return counts\n",
      "\n",
      "\n",
      "def word_freq_std(file_path):\n",
      "    \"\"\" Count the frequency of words in a file. \"\"\"\n",
      "    with open(file_path) as file:\n",
      "        counts = defaultdict(int)\n",
      "        for line in file:\n",
      "            words = line.split(' ')\n",
      "            counts.update(words)\n",
      "        return counts\n",
      "\n",
      "\n",
      "def word_freq_var(file_path):\n",
      "    \"\"\" Count the frequency of words in a file. \"\"\"\n",
      "    with open(file_path) as file:\n",
      "        counts = defaultdict(int)\n",
      "        for line in file:\n",
      "            words = line.split(' ')\n",
      "            counts.update(words)\n",
      "        return counts\n",
      "\n",
      "\n",
      "def word_freq_kurt(file_path):\n",
      "    \"\"\" Count the frequency of words in a file. \"\"\"\n",
      "    with open(file_path) as file:\n",
      "        counts = defaultdict(\n"
     ]
    }
   ],
   "source": [
    "print(infill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|mask:0|>\n",
      "    \"\"\" Count words in a file. \"\"\"\n",
      "<|mask:1|>\n",
      "<|mask:2|>\n",
      "        for line in file:\n",
      "            words = line.split(' ')\n",
      "            counts.update(words)\n",
      "<|mask:3|>\n",
      "<|mask:4|><|mask:0|>\n"
     ]
    }
   ],
   "source": [
    "print(masked_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangkai/anaconda3/envs/vllm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.sharedctypes import Value\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteriaList, StoppingCriteria\n",
    "import json\n",
    "\n",
    "tokenizers_version = tuple(int(n) for n in tokenizers.__version__.split('.'))\n",
    "if tokenizers_version < (0, 12, 1):\n",
    "    print(\"warning: Your tokenizers version looks old and you will likely have formatting issues. We recommend installing tokenizers >= 0.12.1\")\n",
    "\n",
    "PAD = \"<pad>\"\n",
    "# signals the start of a document\n",
    "BOS = \"<|endoftext|>\"\n",
    "# signals the end of a generated infill\n",
    "EOM = \"<|endofmask|>\"\n",
    "\n",
    "def make_sentinel(i):\n",
    "    # signals (1) a location to insert an infill and (2) the start of the infill generation\n",
    "    return f\"<|mask:{i}|>\"\n",
    "\n",
    "def remove_extra_code(input):\n",
    "    min_stop_position = len(input)\n",
    "    stop_tokens = [\"\\nclass\", \"\\ndef\", \"\\n#\", \"\\nif\", \"\\nassert\", \"\\nclass\", \"<|/ file\"]\n",
    "    for stop_token in stop_tokens:\n",
    "        if stop_token in input:\n",
    "            min_stop_position = min(min_stop_position, input.index(stop_token)) \n",
    "    return input[:min_stop_position]\n",
    "\n",
    "# # monkey-patch transformers to avoid nans in padded generation with float16\n",
    "# def _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n",
    "#     \"\"\"\n",
    "#     Make causal mask used for bi-directional self-attention.\n",
    "#     \"\"\"\n",
    "#     bsz, tgt_len = input_ids_shape\n",
    "#     # mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min / 10))\n",
    "#     mask = torch.full((tgt_len, tgt_len), torch.tensor(-1e4))\n",
    "#     mask_cond = torch.arange(mask.size(-1))\n",
    "#     mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "#     mask = mask.to(dtype)\n",
    "\n",
    "#     if past_key_values_length > 0:\n",
    "#         mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n",
    "#     return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
    "\n",
    "# transformers.models.xglm.modeling_xglm._make_causal_mask = _make_causal_mask\n",
    "\n",
    "class StopWordsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, init_lengths: List[int], stop_words_encoded: List[List[int]]):\n",
    "        super().__init__()\n",
    "        self.init_lengths = init_lengths\n",
    "        if stop_words_encoded is None:\n",
    "            stop_words_encoded = []\n",
    "        else:\n",
    "            assert isinstance(stop_words_encoded[0], list)\n",
    "        assert isinstance(stop_words_encoded, list)\n",
    "        self.stop_words_encoded = stop_words_encoded\n",
    "\n",
    "    def _contains_stop_words(self, tokens: List[int]):\n",
    "        if not bool(self.stop_words_encoded):\n",
    "            return False\n",
    "        for start_ix in range(len(tokens)):\n",
    "            for swe in self.stop_words_encoded:\n",
    "                if tokens[start_ix:start_ix+len(swe)] == swe:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for init_length, i_tokens in zip(self.init_lengths, input_ids):\n",
    "            if not self._contains_stop_words(i_tokens[init_length:].tolist()):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "class InfillingModel:\n",
    "    def __init__(self, model_name=\"facebook/incoder-1B\", cuda=True, device=None, tokenizer=None, half=True, model=None):\n",
    "        self.model_name = model_name\n",
    "\n",
    "        if cuda:\n",
    "            assert device is None or device.startswith(\"cuda\")\n",
    "            if device is None:\n",
    "                device = \"cuda\"\n",
    "        else:\n",
    "            assert device is None or device == \"cpu\"\n",
    "            if device is None:\n",
    "                device = \"cpu\"\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        if model_name == 'facebook/incoder-6B':\n",
    "            if cuda:\n",
    "                kwargs = dict(\n",
    "                    revision=\"float16\", \n",
    "                    torch_dtype=torch.float16,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                )\n",
    "            else:\n",
    "                kwargs = dict(\n",
    "                    low_cpu_mem_usage=True,\n",
    "                )\n",
    "        else:\n",
    "            kwargs = {}\n",
    "\n",
    "        if model is None:\n",
    "            print(\"loading model\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)\n",
    "        if tokenizer is None:\n",
    "            print(\"loading tokenizer\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        self.tokenizer.pad_token = PAD\n",
    "        assert self.tokenizer.pad_token_id == 1\n",
    "        print(\"loading complete\")\n",
    "        \n",
    "        if cuda and half:\n",
    "            self.half = True\n",
    "            model = model.half()\n",
    "        else:\n",
    "            self.half = False\n",
    "        model = model.to(device)\n",
    "        self.model = model\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def batched_generate(self, inputs: List[str], max_to_generate: int=128, temperature: float=0.2, trim: bool=True, stop_words=None):\n",
    "\n",
    "        assert self.tokenizer.padding_side == 'left'\n",
    "        assert isinstance(inputs, list)\n",
    "        batch = self.tokenizer(inputs, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "        batch = batch.to(self.device)\n",
    "        max_input_length = batch.input_ids.size(1)\n",
    "        max_length = max_input_length + max_to_generate\n",
    "        stopping_criteria = StoppingCriteriaList()\n",
    "        if stop_words is not None:\n",
    "            stop_words_encoded = [self.tokenizer.encode(word, add_special_tokens=False) for word in stop_words]\n",
    "            stopping_criteria.append(StopWordsStoppingCriteria([max_input_length for l in inputs], stop_words_encoded))\n",
    "        if max_length > 2048:\n",
    "            print(\"warning: max_length {} is greater than the context window {}\".format(max_length, 2048))\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(input_ids=batch.input_ids, attention_mask=batch.attention_mask, do_sample=True, top_p=0.95, temperature=temperature, max_length=max_length, stopping_criteria=stopping_criteria)\n",
    "        \n",
    "        hypo_strs = []\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            detok_hypo_str = self.tokenizer.decode(output.flatten(), clean_up_tokenization_spaces=False)\n",
    "            while detok_hypo_str.startswith(PAD):\n",
    "                detok_hypo_str = detok_hypo_str[len(PAD):]\n",
    "            if detok_hypo_str.startswith(BOS):\n",
    "                detok_hypo_str = detok_hypo_str[len(BOS):]\n",
    "\n",
    "            if trim:\n",
    "                detok_hypo_str = detok_hypo_str[len(input):]\n",
    "                detok_hypo_str = remove_extra_code(detok_hypo_str)\n",
    "            hypo_strs.append(detok_hypo_str)\n",
    "        return hypo_strs\n",
    "\n",
    "    def generate(self, input: str, max_to_generate: int=128, temperature: float=0.2, trim: bool=True):\n",
    "        \"\"\"\n",
    "        Do standard left-to-right completion of the prefix `input` by sampling from the model\n",
    "        \"\"\"\n",
    "        outputs = self.batched_generate([input], max_to_generate, temperature, trim)\n",
    "        assert len(outputs) == 1\n",
    "        return outputs[0]\n",
    "\n",
    "    def batched_infill(self, batched_parts: List[List[str]], max_to_generate: int=128, temperature: float=0.2, extra_sentinel: bool=True, max_retries: int=1):\n",
    "        assert isinstance(batched_parts, list)\n",
    "        assert isinstance(batched_parts[0], list)\n",
    "        batch_size = len(batched_parts)\n",
    "        num_parts = len(batched_parts[0])\n",
    "        assert all(len(l) == num_parts for l in batched_parts), \"all elements in the batch must have the same number of parts\"\n",
    "\n",
    "        # if max_retries > 1 and len(batched_parts) > 1:\n",
    "        #     raise NotImplementedError(\"multiple retries with batch > 1\")\n",
    "\n",
    "        # assert num_parts == 2\n",
    "\n",
    "        batched_retries_attempted = torch.zeros(batch_size).long()\n",
    "        retries_attempted = 0\n",
    "\n",
    "        batched_not_done = torch.ones(batch_size).bool()\n",
    "\n",
    "        done_batched_complete = [None for _ in range(batch_size)]\n",
    "        done_batched_infills = [None for _ in range(batch_size)]\n",
    "\n",
    "        while (batched_not_done.any()) and (retries_attempted < max_retries):\n",
    "            retries_attempted += 1\n",
    "\n",
    "            batched_infills = [[] for _ in range(batch_size)]\n",
    "            batched_complete = [[] for _ in range(batch_size)]\n",
    "            batched_prompts = []\n",
    "\n",
    "            not_done_indices = batched_not_done.nonzero().flatten()\n",
    "            batched_retries_attempted[not_done_indices] += 1\n",
    "            assert batched_retries_attempted.max().item() == retries_attempted\n",
    "\n",
    "            for parts in batched_parts:\n",
    "                ## (1) build the prompt\n",
    "                if len(parts) == 1:\n",
    "                    prompt = parts[0]\n",
    "                else:\n",
    "                    prompt = \"\"\n",
    "                    # encode parts separated by sentinel\n",
    "                    for sentinel_ix, part in enumerate(parts):\n",
    "                        prompt += part\n",
    "                        if extra_sentinel or (sentinel_ix < len(parts) - 1):\n",
    "                            prompt += make_sentinel(sentinel_ix)\n",
    "                batched_prompts.append(prompt)\n",
    "            \n",
    "            ## (2) generate infills\n",
    "            subbatch_not_done = batched_not_done[not_done_indices].clone()\n",
    "            assert subbatch_not_done.all()\n",
    "            subbatch_not_done[:] = False\n",
    "\n",
    "            for sentinel_ix in range(num_parts - 1):\n",
    "                batched_part = [parts[sentinel_ix] for parts in batched_parts]\n",
    "                batched_prompts = [prompt + make_sentinel(sentinel_ix) for prompt in batched_prompts]\n",
    "                for batch_index, parts in enumerate(batched_parts):\n",
    "                    batched_complete[batch_index].append(parts[sentinel_ix])\n",
    "\n",
    "                # TODO: this is inefficient as it requires re-encoding prefixes repeatedly\n",
    "                subbatch_prompts = [batched_prompts[ix] for ix in not_done_indices]\n",
    "                subbatch_outputs = self.batched_generate(subbatch_prompts, max_to_generate, temperature, trim=False, stop_words=[EOM])\n",
    "                for subbatch_ix, (completion, prompt) in enumerate(zip(subbatch_outputs, subbatch_prompts)):\n",
    "                    batch_ix = not_done_indices[subbatch_ix]\n",
    "                    completion = completion[len(prompt):]\n",
    "                    if EOM not in completion:\n",
    "                        completion += EOM\n",
    "                        subbatch_not_done[subbatch_ix] |= True\n",
    "                    completion = completion[:completion.index(EOM) + len(EOM)]\n",
    "                    infilled = completion[:-len(EOM)]\n",
    "                    batched_infills[batch_ix].append(infilled)\n",
    "                    batched_complete[batch_ix].append(infilled)\n",
    "                    batched_prompts[batch_ix] += completion\n",
    "            for batch_ix, parts in enumerate(batched_parts):\n",
    "                batched_complete[batch_ix].append(parts[-1])\n",
    "            \n",
    "            batched_not_done[not_done_indices] = subbatch_not_done\n",
    "            for batch_ix in not_done_indices:\n",
    "                if not batched_not_done[batch_ix] or retries_attempted >= max_retries:\n",
    "                    done_batched_complete[batch_ix] = batched_complete[batch_ix]\n",
    "                    done_batched_infills[batch_ix] = batched_infills[batch_ix]\n",
    "\n",
    "        done_batched_text = [''.join(complete) for complete in done_batched_complete]\n",
    "\n",
    "        return [{\n",
    "            'text': text, # str, the completed document (with infills inserted)\n",
    "            'parts': parts, # List[str], length N. Same as passed to the method\n",
    "            'infills': infills, # List[str], length N-1. The list of infills generated\n",
    "            'retries_attempted': int(this_retries_attempted.item()), # number of retries used (if max_retries > 1)\n",
    "            'completed': bool(not this_not_done),\n",
    "        }  for text, parts, infills, this_retries_attempted, this_not_done in zip(\n",
    "            done_batched_text, batched_parts, done_batched_infills, batched_retries_attempted, batched_not_done\n",
    "        )]\n",
    "\n",
    "    def infill(self, parts: List[str], max_to_generate: int=128, temperature: float=0.2, extra_sentinel: bool=True, max_retries: int=1):\n",
    "        \"\"\"\n",
    "        Generate infills to complete a partial document, e.g.\n",
    "        [A C E] -> [A B C D E], where B and D are infills that have been generated.\n",
    "        parts: List[str]. list of parts of the document. One string will be\n",
    "                inserted in between each element, i.e. infilling N-1 locations for a list\n",
    "                of length N.\n",
    "        max_to_generate: int. maximum number of tokens to generate. Keep in mind\n",
    "                that the model context size is 2048.\n",
    "        temperature: float. temperature parameter for sampling.\n",
    "        extra_sentinel: bool. we recommend setting this to True, as it makes it\n",
    "                easier for the model to end generated infills. See the footnote in \n",
    "                section 2.2 of our paper for details.\n",
    "        max_retries: int. if > 1, use rejection sampling to keep sampling infills until\n",
    "                all infills sample a completion token.\n",
    "        returns a dictionary containing the following:\n",
    "            text:  str, the completed document (with infills inserted)\n",
    "            parts:  List[str], length N. Same as passed to the method\n",
    "            infills:  List[str], length N-1. The list of infills generated\n",
    "            retries_attempted:  number of retries used (if max_retries > 1)\n",
    "        \"\"\"\n",
    "        outputs = self.batched_infill([parts], max_to_generate, temperature, extra_sentinel, max_retries)\n",
    "        assert len(outputs) == 1\n",
    "        return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "loading tokenizer\n",
      "loading complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 6\n",
    "torch.cuda.set_device(device=device)\n",
    "\n",
    "infilling_model = InfillingModel(\"../llm_checkpoints/incoder-6B/\", cuda=True, half=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examples =  [\n",
    "'''\\\n",
    "def count_words(filename):\n",
    "    \"\"\" <insert> \"\"\"\n",
    "    counts = Counter()\n",
    "<insert>\n",
    "        for line in file:\n",
    "            words = line.split(' ')\n",
    "            counts.update(words)\n",
    "    return counts\\\n",
    "''',\n",
    "'''\\\n",
    "def count_lines(filename):\n",
    "    \"\"\" <insert> \"\"\"\n",
    "    counts = Counter()\n",
    "<insert>\n",
    "        return(len(list(file)))\\\n",
    "'''\n",
    "]\n",
    "\n",
    "all_parts = [example.split(\"<insert>\") for example in all_examples]\n",
    "\n",
    "all_results = infilling_model.batched_infill(all_parts, max_to_generate=128, temperature=0.8, max_retries=5)\n",
    "\n",
    "# for result in all_results:\n",
    "#   print(\"completed document:\")\n",
    "#   print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def count_words(filename):\n",
      "    \"\"\" Count words in a file. \"\"\"\n",
      "    counts = Counter()\n",
      "    with open(filename) as file:\n",
      "        for line in file:\n",
      "            words = line.split(' ')\n",
      "            counts.update(words)\n",
      "    return counts\n"
     ]
    }
   ],
   "source": [
    "print(all_results[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "perturbed = json.load(open(\"dataset/apps_hum_test100_incoder_perturb_100.jsonl\"))\n",
    "cnt = 0\n",
    "for text in perturbed:\n",
    "    if len(text) == 0:\n",
    "        cnt += 1\n",
    "cnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
