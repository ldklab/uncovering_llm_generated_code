The given Node.js code defines a simple tarball implementation using streams. It provides functionality to create a tarball (`pack`) and extract entries from it (`extract`). Here's a breakdown of how it works:

1. **Entry Class**: Extends `PassThrough` stream which acts as a placeholder for each file's data in the tarball. It assigns a `header` object to characterize each entry and triggers an associated callback using `process.nextTick()`.

2. **Pack Stream**: 
   - `pack()` returns a `PassThrough` stream with two additional methods:
     - `entry(header, data, callback)`: Takes file header information and data, creates an `Entry` instance, pipes its output to the `Pack` stream without ending it, and returns the `Entry` object.
     - `finalize()`: Ends the pack stream, signifying no more data will be added.
   - Example usage involves packing data into a tar file by specifying entry headers and data, then finalizing the output stream.

3. **Extract Stream**:
   - `extract()` returns a `Writable` stream in object mode for reading entries.
   - It accumulates entries into an array (`entries`).
   - Implements `_write` method managing the reading logic and resumes each entry's data flow.
   - Overrides the `pipe` event to handle extraction of all entries in sequence, ending the stream afterward.
   - Example usage includes attaching it to a source stream to receive all entries and optionally process them.

The code then demonstrates how to pack a file into a tarball and how to mock extract its contents, printing the names of the entries.

Here is the rewritten code:

```javascript
// tar-stream.js
const { PassThrough, Writable } = require('stream');

// Simulated header and entry for packing example
class Entry extends PassThrough {
  constructor(header, callback) {
    super();
    this.header = header;
    process.nextTick(callback);
  }
}

// Packing stream creation
const pack = () => {
  const output = new PassThrough();

  output.entry = (header, data, callback) => {
    const entryStream = new Entry(header, callback);
    if (data) {
      entryStream.end(data);
    }
    entryStream.pipe(output, { end: false });
    return entryStream;
  };

  output.finalize = () => {
    output.end();
  };

  return output;
};

// Extracting simulation
const extract = () => {
  const extractStream = new Writable({ objectMode: true });

  extractStream.entries = [];

  extractStream._write = (entry, encoding, callback) => {
    extractStream.entries.push(entry);
    entry.resume();
    entry.on('end', callback);
  };

  extractStream.on('pipe', (src) => {
    src.unpipe(extractStream);
    for (const entry of src.entries) {
      extractStream.write(entry);
    }
    extractStream.end();
  });

  return extractStream;
};

module.exports = { pack, extract };

// Example usage
const fs = require('fs');

// Packing example
const p = pack();
p.entry({ name: 'hello.txt', size: 11 }, 'Hello world', (err) => {
  if (err) throw err;
  p.finalize();
});

p.pipe(fs.createWriteStream('example.tar'));

// Extracting example - placeholder for actual entry streams
const e = extract();
const source = pack();
source.entry({ name: 'test.txt' }, 'Test content').end();
source.finalize();

source.pipe(e);
e.on('finish', () => {
  console.log('Extracted entries:', e.entries.map(e => e.header.name));
});
```