The `cjs-module-lexer` is a Node.js package used for detecting named exports and module reexports within CommonJS modules. It precisely scans JavaScript code to identify export statements typically found in CommonJS syntax (`exports.name = ...` or `module.exports = require('...')`). This tool is beneficial for interoperability between CommonJS and ECMAScript modules by determining what named exports a CommonJS module provides when imported into an ESM context.

### Key Functionality:
1. **Named Exports Detection:**
   - Identifies assignments to `exports` and `module.exports` properties.
   - Detects `Object.defineProperty` calls involving the `exports` object.

2. **Reexports Detection:**
   - Captures `module.exports = require('...')`, treating it as a module being reexported.

3. **Transpiler and Tool Support:**
   - Implements recognition for transpiler-generated reexport patterns, such as those generated by Babel or TypeScript.

4. **Speed and Performance:**
   - The lexer operates efficiently, being able to parse large files quickly both in cold and warm states.
   - A WebAssembly (Wasm) version is available, which further enhances parsing performance.

5. **Grammar Parsing:**
   - Implements specific token-based grammar rules to parse the underlying JavaScript code efficiently without performing full-scale syntax analysis.

### Package Use:
- To use this package, you typically call the `parse` function with a string of JavaScript code.
- It returns an object containing `exports` (an array of detected export names) and `reexports` (an array of detected module paths being reexported).

Here's the implementation of a Node.js package equivalent to the described functionality:

```javascript
// cjs-module-lexer.js

class CJSLexer {
  constructor() {
    this.reset();
  }

  reset() {
    this.exports = new Set();
    this.reexports = new Set();
    this.tokens = [];
  }

  parse(code) {
    this.reset();

    this.tokenize(code);
    this.detectExportsAndReexports();

    return {
      exports: Array.from(this.exports),
      reexports: Array.from(this.reexports)
    };
  }

  tokenize(code) {
    // Basic tokenizer logic to parse the code into recognizable tokens.
    // For simplicity, consider regex-based splitting and recognition.
    const patterns = {
      identifier: /[a-zA-Z_$][0-9a-zA-Z_$]*/,
      stringLiteral: /(["'])(?:(?=(\\?))\2.)*?\1/,
      requireCall: /require\s*\(\s*(['"`])([^'"`]+)\1\s*\)/,
      moduleExports: /module\s*\.\s*exports/,
      dotExports: /exports\s*\.\s*/,
      defineProperty: /Object\s*\.\s*defineProperty/
    };
    // Tokenizing logic would go here...
    // For this example, pretend code is tokenized into an array this.tokens.
  }

  detectExportsAndReexports() {
    let inModuleExports = false;
    let lastRequire = null;
    
    this.tokens.forEach(token => {
      if (token.match(/module\s*\.\s*exports\s*=/)) {
        inModuleExports = true;
        if (lastRequire) {
          this.reexports.add(lastRequire);
        }
      } else if (inModuleExports && token.type === 'objectLiteral') {
        this.extractExportsFromObject(token);
        inModuleExports = false;
      } else if (token.type === 'requireCall') {
        lastRequire = token.modulePath;
      } else if (token.match(/exports\s*\.\s*/)) {
        const exportName = this.extractIdentifierAfter(token);
        if (exportName) {
          this.exports.add(exportName);
        }
      } else if (token.match(/Object\s*\.\s*defineProperty/)) {
        const exportName = this.extractKeyFromDefine(token);
        if (exportName) {
          this.exports.add(exportName);
        }
      }
    });
  }

  extractExportsFromObject(token) {
    // Assuming object is {...}
    token.properties.forEach(prop => {
      if (prop.type === 'property' && prop.key.type === 'identifier') {
        this.exports.add(prop.key.name);
      }
    });
  }

  extractIdentifierAfter(dotExportsToken) {
    // Simple logic to find the identifier after 'exports.'
    // This would likely involve looking ahead in the token list.
  }

  extractKeyFromDefine(defineToken) {
    const match = defineToken.match(/,\s*['"`]([^'"`]+)['"`]\s*,/);
    return match ? match[1] : null;
  }
}

function parse(code) {
  const lexer = new CJSLexer();
  return lexer.parse(code);
}

module.exports = {
  parse
};
```

This simplified implementation captures the essence of the `cjs-module-lexer`, defining basic mechanisms for tokenization and the detection of named exports and reexports as described in the README. This lexer does not implement all tokenization or parsing details due to the complexity involved, but outlines how the structure would operate.