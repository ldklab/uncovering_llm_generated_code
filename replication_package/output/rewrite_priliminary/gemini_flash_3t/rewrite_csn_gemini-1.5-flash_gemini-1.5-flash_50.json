[
    {
        "id": 2,
        "prompt": "def selectOptimalChunk(self, peer):\n        \"\"\"\n        select an optimal chunk to send to a peer.\n\n        @return: int(chunkNumber), str(chunkData) if there is data to be sent,\n        otherwise None, None\n        \"\"\"",
        "code": "def selectOptimalChunk(self, peer):\n        \"\"\"\n        select an optimal chunk to send to a peer.\n\n        @return: int(chunkNumber), str(chunkData) if there is data to be sent,\n        otherwise None, None\n        \"\"\"\n        for chunkNumber, chunkData in self.chunks.items():\n            if chunkNumber not in peer.receivedChunks:\n                return chunkNumber, chunkData\n        return None, None",
        "writer": "machine",
        "rewrite": [
            "def selectOptimalChunk(self, peer):\n    try:\n        return next(((chunkNumber, chunkData) for chunkNumber, chunkData in self.chunks.items() if chunkNumber not in peer.receivedChunks))\n    except StopIteration:\n        return None, None",
            "def selectOptimalChunk(self, peer):\n    for chunkNumber in self.chunks:\n        if chunkNumber not in peer.receivedChunks:\n            return chunkNumber, self.chunks[chunkNumber]\n    return None, None",
            "def selectOptimalChunk(self, peer):\n    try:\n        return next((cn, self.chunks[cn]) for cn in self.chunks if cn not in peer.receivedChunks)\n    except StopIteration:\n        return None, None"
        ]
    },
    {
        "id": 3,
        "prompt": "def replace_built(self, built_packages):\n    \"\"\"Return a copy of this resolvable set but with built packages.\n\n    :param dict built_packages: A mapping from a resolved package to its locally built package.\n    :returns: A new resolvable set with built package replacements made.\n    \"\"\"",
        "code": "def replace_built(self, built_packages):\n    \"\"\"Return a copy of this resolvable set but with built packages.\n\n    :param dict built_packages: A mapping from a resolved package to its locally built package.\n    :returns: A new resolvable set with built package replacements made.\n    \"\"\"\n    def map_packages(resolved_packages):\n      packages = OrderedSet(built_packages.get(p, p) for p in resolved_packages.packages)\n      return _ResolvedPackages(resolved_packages.resolvable, packages,\n                               resolved_packages.parent, resolved_packages.constraint_only)\n\n    return _ResolvableSet([map_packages(rp) for rp in self.__tuples])",
        "writer": "human",
        "rewrite": [
            "def replace_built(self, built_packages):\n    return _ResolvableSet([_ResolvedPackages(rp.resolvable, OrderedSet(built_packages.get(p, p) for p in rp.packages), rp.parent, rp.constraint_only) for rp in self.__tuples])",
            "def replace_built(self, built_packages):\n    return _ResolvableSet(_ResolvedPackages(rp.resolvable, OrderedSet(built_packages.get(p,p) for p in rp.packages), rp.parent, rp.constraint_only) for rp in self.__tuples)",
            "def replace_built(self, built_packages):\n    return _ResolvableSet(map(lambda rp: _ResolvedPackages(rp.resolvable, OrderedSet(built_packages.get(p, p) for p in rp.packages), rp.parent, rp.constraint_only), self.__tuples))"
        ]
    },
    {
        "id": 3,
        "prompt": "def replace_built(self, built_packages):\n    \"\"\"Return a copy of this resolvable set but with built packages.\n\n    :param dict built_packages: A mapping from a resolved package to its locally built package.\n    :returns: A new resolvable set with built package replacements made.\n    \"\"\"",
        "code": "def replace_built(self, built_packages):\n    \"\"\"Return a copy of this resolvable set but with built packages.\n\n    :param dict built_packages: A mapping from a resolved package to its locally built package.\n    :returns: A new resolvable set with built package replacements made.\n    \"\"\"\n    new_resolvable_set = self.copy()\n    for resolved_package, built_package in built_packages.items():\n        if resolved_package in new_resolvable_set:\n            new_resolvable_set[built_package] = new_resolvable_set.pop(resolved_package)\n    return new_resolvable_set",
        "writer": "machine",
        "rewrite": [
            "def replace_built(self, built_packages):\n    new_resolvable_set = self.copy()\n    new_resolvable_set.update({built_package: new_resolvable_set.pop(resolved_package) for resolved_package, built_package in built_packages.items() if resolved_package in new_resolvable_set})\n    return new_resolvable_set",
            "def replace_built(self, built_packages):\n    new_resolvable_set = self.copy()\n    for resolved, built in built_packages.items():\n        if resolved in new_resolvable_set:\n            new_resolvable_set[built] = new_resolvable_set.pop(resolved)\n    return new_resolvable_set",
            "def replace_built(self, built_packages):\n  return {built_package if resolved_package in self else resolved_package: value for resolved_package, value in self.items() for built_package, _ in built_packages.items() if resolved_package == built_package}"
        ]
    },
    {
        "id": 4,
        "prompt": "def create_uo(self, configuration=None, tpl=None, keys=None, obj_type=None):\n        \"\"\"\n        Create a new UserObject from the given template.\n\n        :param configuration:  EB configuration to use\n        :param tpl: CreateUserObject template, contain misc settings\n        :param keys: dictionary of keys, create_uo.KeyTypes. Communication keys, application key (if applicable).\n        :param obj_type: optional field for easy object type entry - required flags are computed from keys dict and tpl.\n        :return: UO - user object ready to use\n        \"\"\"",
        "code": "def create_uo(self, configuration=None, tpl=None, keys=None, obj_type=None):\n        \"\"\"\n        Create a new UserObject from the given template.\n\n        :param configuration:  EB configuration to use\n        :param tpl: CreateUserObject template, contain misc settings\n        :param keys: dictionary of keys, create_uo.KeyTypes. Communication keys, application key (if applicable).\n        :param obj_type: optional field for easy object type entry - required flags are computed from keys dict and tpl.\n        :return: UO - user object ready to use\n        \"\"\"\n        if configuration is None:\n            configuration = self.configuration\n        if tpl is None:\n            tpl = self.tpl\n        if keys is None:\n            keys = self.keys\n        if obj_type is None:\n            obj_type = self.obj_type\n        uo = self.create_uo_from_template(tpl, keys, obj_type)\n        if configuration is not None:\n            uo.set_configuration(configuration)\n        return uo",
        "writer": "machine",
        "rewrite": [
            "def create_uo(self, configuration=None, tpl=None, keys=None, obj_type=None):\n    configuration = configuration or self.configuration\n    tpl = tpl or self.tpl\n    keys = keys or self.keys\n    obj_type = obj_type or self.obj_type\n    uo = self.create_uo_from_template(tpl, keys, obj_type)\n    if configuration:\n        uo.set_configuration(configuration)\n    return uo",
            "def create_uo(self, configuration=self.configuration, tpl=self.tpl, keys=self.keys, obj_type=self.obj_type):\n    uo = self.create_uo_from_template(tpl, keys, obj_type)\n    if configuration:\n        uo.set_configuration(configuration)\n    return uo",
            "def create_uo(self, configuration=None, tpl=None, keys=None, obj_type=None):\n    uo = self.create_uo_from_template(tpl or self.tpl, keys or self.keys, obj_type or self.obj_type)\n    if configuration:\n        uo.set_configuration(configuration or self.configuration)\n    return uo"
        ]
    },
    {
        "id": 1,
        "prompt": "def hydrophobic_interactions(atom_set_a, atom_set_b):\n    \"\"\"Detection of hydrophobic pliprofiler between atom_set_a (binding site) and atom_set_b (ligand).\n    Definition: All pairs of qualified carbon atoms within a distance of HYDROPH_DIST_MAX\n    \"\"\"",
        "code": "def hydrophobic_interactions(atom_set_a, atom_set_b):\n    \"\"\"Detection of hydrophobic pliprofiler between atom_set_a (binding site) and atom_set_b (ligand).\n    Definition: All pairs of qualified carbon atoms within a distance of HYDROPH_DIST_MAX\n    \"\"\"\n    HYDROPH_DIST_MAX = 4.5\n    hydrophobic_interactions = []\n    for atom_a in atom_set_a:\n        if atom_a.element == 'C':\n            for atom_b in atom_set_b:\n                if atom_b.element == 'C':\n                    distance = atom_a.distance(atom_b)\n                    if distance <= HYDROPH_DIST_MAX:\n                        hydrophobic_interactions.append((atom_a, atom_b))\n    return hydrophobic_interactions",
        "writer": "machine",
        "rewrite": [
            "def hydrophobic_interactions(atom_set_a, atom_set_b):\n    HYDROPH_DIST_MAX = 4.5\n    return [(atom_a, atom_b) for atom_a in atom_set_a for atom_b in atom_set_b if atom_a.element == 'C' and atom_b.element == 'C' and atom_a.distance(atom_b) <= HYDROPH_DIST_MAX]",
            "def hydrophobic_interactions(atom_set_a, atom_set_b):\n    HYDROPH_DIST_MAX = 4.5\n    return [(a, b) for a in atom_set_a for b in atom_set_b if a.element == 'C' and b.element == 'C' and a.distance(b) <= HYDROPH_DIST_MAX]",
            "def hydrophobic_interactions(atom_set_a, atom_set_b):\n    max_dist = 4.5\n    return [(a, b) for a in atom_set_a for b in atom_set_b if a.element == 'C' and b.element == 'C' and a.distance(b) <= max_dist]"
        ]
    },
    {
        "id": 5,
        "prompt": "def password_entropy(length: int, chars: str) -> float:\n    \"\"\"Calculate the entropy of a password with given length and chars.\"\"\"",
        "code": "def password_entropy(length: int, chars: str) -> float:\n    \"\"\"Calculate the entropy of a password with given length and chars.\"\"\"\n    import math\n    return length * math.log2(len(chars))",
        "writer": "machine",
        "rewrite": [
            "def password_entropy(length: int, chars: str) -> float:\n    return length * math.log2(len(chars))",
            "import math\n\ndef password_entropy(length, chars):\n  return length * math.log2(len(chars))",
            "import math\n\ndef calculate_password_entropy(password_length: int, character_set: str) -> float:\n    return password_length * math.log2(len(character_set))"
        ]
    },
    {
        "id": 2,
        "prompt": "def selectOptimalChunk(self, peer):\n        \"\"\"\n        select an optimal chunk to send to a peer.\n\n        @return: int(chunkNumber), str(chunkData) if there is data to be sent,\n        otherwise None, None\n        \"\"\"",
        "code": "def selectOptimalChunk(self, peer):\n        \"\"\"\n        select an optimal chunk to send to a peer.\n\n        @return: int(chunkNumber), str(chunkData) if there is data to be sent,\n        otherwise None, None\n        \"\"\"\n\n        # stuff I have\n        have = sets.Set(self.mask.positions(1))\n        # stuff that this peer wants\n        want = sets.Set(self.peers[peer].mask.positions(0))\n        exchangeable = have.intersection(want)\n        finalSet = dict.fromkeys(exchangeable, 0)\n\n        # taking a page from bittorrent, rarest-first\n        for chunkNumber in exchangeable:\n            for otherPeer in self.peers.itervalues():\n                finalSet[chunkNumber] += not otherPeer.mask[chunkNumber]\n        rarityList = [(rarity, random.random(), chunkNumber)\n                      for (chunkNumber, rarity)\n                      in finalSet.iteritems()]\n        if not rarityList:\n            return None, None\n        rarityList.sort()\n        chunkNumber = rarityList[-1][-1] # sorted in ascending order of rarity\n\n        # sanity check\n        assert self.mask[chunkNumber], \"I wanted to send a chunk I didn't have\"\n\n        self.file.seek(chunkNumber * CHUNK_SIZE)\n        chunkData = self.file.read(CHUNK_SIZE)\n        self.sha1sums[chunkNumber] = sha.new(chunkData).digest()\n        return chunkNumber, chunkData",
        "writer": "human",
        "rewrite": [
            "def selectOptimalChunk(self, peer):\n    have = set(self.mask.positions(1))\n    want = set(self.peers[peer].mask.positions(0))\n    exchangeable = have & want\n    finalSet = {chunk: 0 for chunk in exchangeable}\n    for chunkNumber in exchangeable:\n        for otherPeer in self.peers.values():\n            finalSet[chunkNumber] += not otherPeer.mask[chunkNumber]\n    rarityList = sorted([(rarity, random.random(), chunkNumber) for chunkNumber, rarity in finalSet.items()])\n    if not rarityList:\n        return None, None\n    chunkNumber = rarityList[-1][-1]\n    assert self.mask[chunkNumber]\n    self.file.seek(chunkNumber * CHUNK_SIZE)\n    chunkData = self.file.read(CHUNK_SIZE)\n    self.sha1sums[chunkNumber] = hashlib.sha1(chunkData).digest()\n    return chunkNumber, chunkData",
            "def selectOptimalChunk(self, peer):\n    have = set(self.mask.positions(1))\n    want = set(self.peers[peer].mask.positions(0))\n    exchangeable = have.intersection(want)\n    if not exchangeable:\n        return None, None\n    finalSet = {chunk: sum(not p.mask[chunk] for p in self.peers.values()) for chunk in exchangeable}\n    chunkNumber = max(finalSet, key=finalSet.get)\n    assert self.mask[chunkNumber]\n    self.file.seek(chunkNumber * CHUNK_SIZE)\n    chunkData = self.file.read(CHUNK_SIZE)\n    self.sha1sums[chunkNumber] = hashlib.sha1(chunkData).digest()\n    return chunkNumber, chunkData",
            "def selectOptimalChunk(self, peer):\n    have = set(self.mask.positions(1))\n    want = set(self.peers[peer].mask.positions(0))\n    exchangeable = have & want\n    if not exchangeable:\n        return None, None\n    rarities = {chunk: sum(1 for p in self.peers.values() if not p.mask[chunk]) for chunk in exchangeable}\n    chunk_number = max(rarities, key=rarities.get)\n    assert self.mask[chunk_number]\n    self.file.seek(chunk_number * CHUNK_SIZE)\n    chunk_data = self.file.read(CHUNK_SIZE)\n    self.sha1sums[chunk_number] = hashlib.sha1(chunk_data).digest()\n    return chunk_number, chunk_data"
        ]
    },
    {
        "id": 5,
        "prompt": "def password_entropy(length: int, chars: str) -> float:\n    \"\"\"Calculate the entropy of a password with given length and chars.\"\"\"",
        "code": "def password_entropy(length: int, chars: str) -> float:\n    \"\"\"Calculate the entropy of a password with given length and chars.\"\"\"\n    if not isinstance(length, int):\n        raise TypeError('length can only be int')\n    if length < 0:\n        raise ValueError('length should be greater than 0')\n    if not isinstance(chars, str):\n        raise TypeError('chars can only be string')\n    if not chars:\n        raise ValueError(\"chars can't be null\")\n\n    if length == 0:\n        return 0.0\n\n    entropy_c = entropy_bits(list(chars))\n    return float(length * entropy_c)",
        "writer": "human",
        "rewrite": [
            "def password_entropy(length: int, chars: str) -> float:\n    if not isinstance(length, int) or length < 0:\n        raise ValueError(\"length must be a positive integer\")\n    if not isinstance(chars, str) or not chars:\n        raise ValueError(\"chars must be a non-empty string\")\n    \n    entropy_c = len(set(chars)) * 1.0\n    return length * math.log2(entropy_c)\nimport math",
            "import math\n\ndef password_entropy(length: int, chars: str) -> float:\n    if not isinstance(length, int) or length < 0:\n        raise ValueError(\"Length must be a non-negative integer.\")\n    if not isinstance(chars, str) or not chars:\n        raise ValueError(\"Chars must be a non-empty string.\")\n    \n    num_unique_chars = len(set(chars))\n    return length * math.log2(num_unique_chars)",
            "import math\n\ndef password_entropy(length: int, chars: str) -> float:\n    if not isinstance(length, int) or length < 0:\n        raise ValueError(\"Invalid length\")\n    if not isinstance(chars, str) or not chars:\n        raise ValueError(\"Invalid chars\")\n    return length * math.log2(len(set(chars)))"
        ]
    },
    {
        "id": 6,
        "prompt": "def actualize_source_type (self, sources, prop_set):\n        \"\"\" Helper for 'actualize_sources'.\n            For each passed source, actualizes it with the appropriate scanner.\n            Returns the actualized virtual targets.\n        \"\"\"",
        "code": "def actualize_source_type(self, sources, prop_set):\n        \"\"\" Helper for 'actualize_sources'.\n            For each passed source, actualizes it with the appropriate scanner.\n            Returns the actualized virtual targets.\n        \"\"\"\n        virtual_targets = []\n        for source in sources:\n            scanner = self.get_scanner(source, prop_set)\n            if scanner:\n                virtual_targets.extend(scanner.actualize_source(source, prop_set))\n        return virtual_targets",
        "writer": "machine",
        "rewrite": [
            "def actualize_source_type(self, sources, prop_set):\n    return [scanner.actualize_source(source, prop_set) for source in sources if (scanner := self.get_scanner(source, prop_set))]",
            "def actualize_source_type(self, sources, prop_set):\n    return sum((scanner.actualize_source(source, prop_set) for source in sources if (scanner := self.get_scanner(source, prop_set))), [])",
            "def actualize_source_type(self, sources, prop_set):\n    return [target for source in sources for target in (self.get_scanner(source, prop_set).actualize_source(source, prop_set) if self.get_scanner(source, prop_set) else [])]"
        ]
    },
    {
        "id": 6,
        "prompt": "def actualize_source_type (self, sources, prop_set):\n        \"\"\" Helper for 'actualize_sources'.\n            For each passed source, actualizes it with the appropriate scanner.\n            Returns the actualized virtual targets.\n        \"\"\"",
        "code": "def actualize_source_type (self, sources, prop_set):\n        \"\"\" Helper for 'actualize_sources'.\n            For each passed source, actualizes it with the appropriate scanner.\n            Returns the actualized virtual targets.\n        \"\"\"\n        assert is_iterable_typed(sources, VirtualTarget)\n        assert isinstance(prop_set, property_set.PropertySet)\n        result = []\n        for i in sources:\n            scanner = None\n\n# FIXME: what's this?\n#            if isinstance (i, str):\n#                i = self.manager_.get_object (i)\n\n            if i.type ():\n                scanner = b2.build.type.get_scanner (i.type (), prop_set)\n\n            r = i.actualize (scanner)\n            result.append (r)\n\n        return result",
        "writer": "human",
        "rewrite": [
            "def actualize_source_type(self, sources, prop_set):\n    assert is_iterable_typed(sources, VirtualTarget)\n    assert isinstance(prop_set, property_set.PropertySet)\n    return [i.actualize(b2.build.type.get_scanner(i.type(), prop_set) if i.type() else None) for i in sources]",
            "def actualize_source_type(self, sources, prop_set):\n    assert is_iterable_typed(sources, VirtualTarget)\n    assert isinstance(prop_set, property_set.PropertySet)\n    return [i.actualize(b2.build.type.get_scanner(i.type(), prop_set) if i.type() else None) for i in sources]",
            "def actualize_source_type(self, sources, prop_set):\n    assert is_iterable_typed(sources, VirtualTarget)\n    assert isinstance(prop_set, property_set.PropertySet)\n    return [s.actualize(b2.build.type.get_scanner(s.type(), prop_set) if s.type() else None) for s in sources]"
        ]
    },
    {
        "id": 7,
        "prompt": "def _HuntObjectFromRow(self, row):\n    \"\"\"Generates a flow object from a database row.\"\"\"",
        "code": "def _HuntObjectFromRow(self, row):\n    \"\"\"Generates a flow object from a database row.\"\"\"\n    return self.flow_class(\n        id=row[\"id\"],\n        name=row[\"name\"],\n        description=row[\"description\"],\n        created_at=row[\"created_at\"],\n        updated_at=row[\"updated_at\"],\n        deleted_at=row[\"deleted_at\"],\n    )",
        "writer": "machine",
        "rewrite": [
            "def _HuntObjectFromRow(self, row):\n    return self.flow_class(**row)",
            "def _HuntObjectFromRow(self, row):\n    kwargs = {k: v for k, v in row.items() if k in ['id', 'name', 'description', 'created_at', 'updated_at', 'deleted_at']}\n    return self.flow_class(**kwargs)",
            "def _HuntObjectFromRow(self, row):\n    return self.flow_class(**{k: row[k] for k in ['id', 'name', 'description', 'created_at', 'updated_at', 'deleted_at']})"
        ]
    },
    {
        "id": 4,
        "prompt": "def create_uo(self, configuration=None, tpl=None, keys=None, obj_type=None):\n        \"\"\"\n        Create a new UserObject from the given template.\n\n        :param configuration:  EB configuration to use\n        :param tpl: CreateUserObject template, contain misc settings\n        :param keys: dictionary of keys, create_uo.KeyTypes. Communication keys, application key (if applicable).\n        :param obj_type: optional field for easy object type entry - required flags are computed from keys dict and tpl.\n        :return: UO - user object ready to use\n        \"\"\"",
        "code": "def create_uo(self, configuration=None, tpl=None, keys=None, obj_type=None):\n        \"\"\"\n        Create a new UserObject from the given template.\n\n        :param configuration:  EB configuration to use\n        :param tpl: CreateUserObject template, contain misc settings\n        :param keys: dictionary of keys, create_uo.KeyTypes. Communication keys, application key (if applicable).\n        :param obj_type: optional field for easy object type entry - required flags are computed from keys dict and tpl.\n        :return: UO - user object ready to use\n        \"\"\"\n        if configuration is not None:\n            self.configuration = configuration\n        if tpl is not None:\n            self.tpl = tpl\n        if keys is not None:\n            self.keys = keys\n        if self.keys is None:\n            self.keys = dict()\n\n        # generate comm keys if not present\n        TemplateProcessor.generate_comm_keys_if_not_present(self.keys)\n\n        # obj_type infer\n        if obj_type is not None:\n            tpl_type = CreateUO.get_uo_type(obj_type, KeyTypes.COMM_ENC in self.keys, KeyTypes.APP_KEY in self.keys)\n            self.tpl = CreateUO.set_type(self.tpl if self.tpl is not None else dict(), tpl_type)\n\n        # Create template specifications, using local config and defaults.\n        spec = CreateUO.get_template_request_spec(self.configuration)\n        if self.tpl is not None:\n            if isinstance(self.tpl, dict):\n                spec = EBUtils.update(spec, self.tpl)\n            else:\n                raise ValueError('Unknown tpl format')\n\n        # Fetch template for new UO.\n        tpl_resp = CreateUO.template_request(self.configuration, spec)\n\n        # Process the template, fill in the keys, do the crypto\n        tpl_processor = TemplateProcessor(configuration=self.configuration, keys=self.keys, tpl_response=tpl_resp)\n        tpl_req = tpl_processor.process()\n\n        # Import the initialized UO\n        self.import_resp = CreateUO.import_object(configuration=self.configuration, tpl=tpl_req)\n\n        # Build UO\n        uo = CreateUO.build_imported_object(configuration=self.configuration, tpl_import_req=tpl_req,\n                                            import_resp=self.import_resp)\n        return uo",
        "writer": "human",
        "rewrite": [
            "def create_uo(self, configuration=None, tpl=None, keys=None, obj_type=None):\n    self.configuration = configuration or self.configuration\n    self.tpl = tpl or self.tpl\n    self.keys = keys or self.keys or {}\n    TemplateProcessor.generate_comm_keys_if_not_present(self.keys)\n    if obj_type:\n        tpl_type = CreateUO.get_uo_type(obj_type, KeyTypes.COMM_ENC in self.keys, KeyTypes.APP_KEY in self.keys)\n        self.tpl = CreateUO.set_type(self.tpl or {}, tpl_type)\n    spec = CreateUO.get_template_request_spec(self.configuration)\n    if self.tpl:\n        if isinstance(self.tpl, dict):\n            spec.update(self.tpl)\n        else:\n            raise ValueError('Unknown tpl format')\n    tpl_resp = CreateUO.template_request(self.configuration, spec)\n    tpl_processor = TemplateProcessor(configuration=self.configuration, keys=self.keys, tpl_response=tpl_resp)\n    tpl_req = tpl_processor.process()\n    self.import_resp = CreateUO.import_object(configuration=self.configuration, tpl=tpl_req)\n    uo = CreateUO.build_imported_object(configuration=self.configuration, tpl_import_req=tpl_req, import_resp=self.import_resp)\n    return uo",
            "def create_uo(self, configuration=None, tpl=None, keys=None, obj_type=None):\n    self.configuration = configuration or self.configuration\n    self.tpl = tpl or self.tpl\n    self.keys = keys or self.keys or {}\n    TemplateProcessor.generate_comm_keys_if_not_present(self.keys)\n    if obj_type:\n        tpl_type = CreateUO.get_uo_type(obj_type, KeyTypes.COMM_ENC in self.keys, KeyTypes.APP_KEY in self.keys)\n        self.tpl = CreateUO.set_type(self.tpl or {}, tpl_type)\n    spec = CreateUO.get_template_request_spec(self.configuration)\n    if self.tpl:\n        if isinstance(self.tpl, dict):\n            spec.update(self.tpl)\n        else:\n            raise ValueError('Unknown tpl format')\n    tpl_resp = CreateUO.template_request(self.configuration, spec)\n    tpl_processor = TemplateProcessor(configuration=self.configuration, keys=self.keys, tpl_response=tpl_resp)\n    tpl_req = tpl_processor.process()\n    self.import_resp = CreateUO.import_object(configuration=self.configuration, tpl=tpl_req)\n    uo = CreateUO.build_imported_object(configuration=self.configuration, tpl_import_req=tpl_req, import_resp=self.import_resp)\n    return uo",
            "def create_uo(self, configuration=None, tpl=None, keys=None, obj_type=None):\n    self.configuration = configuration or self.configuration\n    self.tpl = tpl or self.tpl\n    self.keys = keys or self.keys or {}\n    TemplateProcessor.generate_comm_keys_if_not_present(self.keys)\n    if obj_type is not None:\n        tpl_type = CreateUO.get_uo_type(obj_type, KeyTypes.COMM_ENC in self.keys, KeyTypes.APP_KEY in self.keys)\n        self.tpl = CreateUO.set_type(self.tpl or {}, tpl_type)\n    spec = CreateUO.get_template_request_spec(self.configuration)\n    if self.tpl:\n        if isinstance(self.tpl, dict):\n            spec.update(self.tpl)\n        else:\n            raise ValueError(\"Unknown tpl format\")\n    tpl_resp = CreateUO.template_request(self.configuration, spec)\n    tpl_processor = TemplateProcessor(configuration=self.configuration, keys=self.keys, tpl_response=tpl_resp)\n    tpl_req = tpl_processor.process()\n    self.import_resp = CreateUO.import_object(configuration=self.configuration, tpl=tpl_req)\n    uo = CreateUO.build_imported_object(configuration=self.configuration, tpl_import_req=tpl_req, import_resp=self.import_resp)\n    return uo"
        ]
    },
    {
        "id": 9,
        "prompt": "def compress(self, image_path):\n    \"\"\"compress will (properly) compress an image\"\"\"",
        "code": "def compress(self, image_path):\n    \"\"\"compress will (properly) compress an image\"\"\"\n    from PIL import Image\n    image = Image.open(image_path)\n    image.save(image_path, optimize=True, quality=85)",
        "writer": "machine",
        "rewrite": [
            "def compress(self, image_path):\n    from PIL import Image\n    img = Image.open(image_path)\n    img.save(image_path, optimize=True, quality=85)",
            "def compress_image(image_path):\n    from PIL import Image\n    try:\n        with Image.open(image_path) as img:\n            img.save(image_path, optimize=True, quality=85)\n    except FileNotFoundError:\n        return False\n    except Exception as e:\n        return e",
            "def compress_image(image_path):\n    try:\n        Image.open(image_path).save(image_path, optimize=True, quality=85)\n    except (FileNotFoundError,IOError,ImportError) as e:\n        print(f\"Error processing image: {e}\")"
        ]
    },
    {
        "id": 1,
        "prompt": "def hydrophobic_interactions(atom_set_a, atom_set_b):\n    \"\"\"Detection of hydrophobic pliprofiler between atom_set_a (binding site) and atom_set_b (ligand).\n    Definition: All pairs of qualified carbon atoms within a distance of HYDROPH_DIST_MAX\n    \"\"\"",
        "code": "def hydrophobic_interactions(atom_set_a, atom_set_b):\n    \"\"\"Detection of hydrophobic pliprofiler between atom_set_a (binding site) and atom_set_b (ligand).\n    Definition: All pairs of qualified carbon atoms within a distance of HYDROPH_DIST_MAX\n    \"\"\"\n    data = namedtuple('hydroph_interaction', 'bsatom bsatom_orig_idx ligatom ligatom_orig_idx '\n                                             'distance restype resnr reschain restype_l, resnr_l, reschain_l')\n    pairings = []\n    for a, b in itertools.product(atom_set_a, atom_set_b):\n        if a.orig_idx == b.orig_idx:\n            continue\n        e = euclidean3d(a.atom.coords, b.atom.coords)\n        if not config.MIN_DIST < e < config.HYDROPH_DIST_MAX:\n            continue\n        restype, resnr, reschain = whichrestype(a.atom), whichresnumber(a.atom), whichchain(a.atom)\n        restype_l, resnr_l, reschain_l = whichrestype(b.orig_atom), whichresnumber(b.orig_atom), whichchain(b.orig_atom)\n        contact = data(bsatom=a.atom, bsatom_orig_idx=a.orig_idx, ligatom=b.atom, ligatom_orig_idx=b.orig_idx,\n                       distance=e, restype=restype, resnr=resnr,\n                       reschain=reschain, restype_l=restype_l,\n                       resnr_l=resnr_l, reschain_l=reschain_l)\n        pairings.append(contact)\n    return filter_contacts(pairings)",
        "writer": "human",
        "rewrite": [
            "from scipy.spatial.distance import euclidean as euclidean3d\nfrom collections import namedtuple\nimport itertools\n\ndef hydrophobic_interactions(atom_set_a, atom_set_b):\n    data = namedtuple('hydroph_interaction', 'bsatom bsatom_orig_idx ligatom ligatom_orig_idx distance restype resnr reschain restype_l resnr_l reschain_l')\n    pairings = []\n    for a, b in itertools.product(atom_set_a, atom_set_b):\n        if a.orig_idx == b.orig_idx:\n            continue\n        dist = euclidean3d(a.atom.coords, b.atom.coords)\n        if not config.MIN_DIST < dist < config.HYDROPH_DIST_MAX:\n            continue\n        restype, resnr, reschain = whichrestype(a.atom), whichresnumber(a.atom), whichchain(a.atom)\n        restype_l, resnr_l, reschain_l = whichrestype(b.orig_atom), whichresnumber(b.orig_atom), whichchain(b.orig_atom)\n        pairings.append(data(bsatom=a.atom, bsatom_orig_idx=a.orig_idx, ligatom=b.atom, ligatom_orig_idx=b.orig_idx, distance=dist, restype=restype, resnr=resnr, reschain=reschain, restype_l=restype_l, resnr_l=resnr_l, reschain_l=reschain_l))\n    return pairings",
            "from collections import namedtuple\nimport itertools\nfrom scipy.spatial.distance import euclidean as euclidean3d\n\ndef hydrophobic_interactions(atom_set_a, atom_set_b):\n    HydrophInteraction = namedtuple('HydrophInteraction', ['bsatom', 'bsatom_orig_idx', 'ligatom', 'ligatom_orig_idx', 'distance', 'restype', 'resnr', 'reschain', 'restype_l', 'resnr_l', 'reschain_l'])\n    interactions = []\n    for atom_a, atom_b in itertools.product(atom_set_a, atom_set_b):\n        if atom_a.orig_idx == atom_b.orig_idx:\n            continue\n        distance = euclidean3d(atom_a.atom.coords, atom_b.atom.coords)\n        if not config.MIN_DIST < distance < config.HYDROPH_DIST_MAX:\n            continue\n        restype_a, resnr_a, reschain_a = whichrestype(atom_a.atom), whichresnumber(atom_a.atom), whichchain(atom_a.atom)\n        restype_b, resnr_b, reschain_b = whichrestype(atom_b.orig_atom), whichresnumber(atom_b.orig_atom), whichchain(atom_b.orig_atom)\n        interaction = HydrophInteraction(atom_a.atom, atom_a.orig_idx, atom_b.atom, atom_b.orig_idx, distance, restype_a, resnr_a, reschain_a, restype_b, resnr_b, reschain_b)\n        interactions.append(interaction)\n    return filter_contacts(interactions)",
            "from collections import namedtuple\nimport itertools\nfrom scipy.spatial.distance import euclidean as euclidean3d\n\ndef hydrophobic_interactions(atom_set_a, atom_set_b):\n    Interaction = namedtuple('Interaction', ['bsatom', 'bsatom_orig_idx', 'ligatom', 'ligatom_orig_idx', 'distance', 'restype', 'resnr', 'reschain', 'restype_l', 'resnr_l', 'reschain_l'])\n    interactions = []\n    for a, b in itertools.product(atom_set_a, atom_set_b):\n        if a.orig_idx == b.orig_idx:\n            continue\n        dist = euclidean3d(a.atom.coords, b.atom.coords)\n        if not config.MIN_DIST < dist < config.HYDROPH_DIST_MAX:\n            continue\n        r_a, n_a, c_a = whichrestype(a.atom), whichresnumber(a.atom), whichchain(a.atom)\n        r_b, n_b, c_b = whichrestype(b.orig_atom), whichresnumber(b.orig_atom), whichchain(b.orig_atom)\n        interaction = Interaction(a.atom, a.orig_idx, b.atom, b.orig_idx, dist, r_a, n_a, c_a, r_b, n_b, c_b)\n        interactions.append(interaction)\n    return filter_contacts(interactions)"
        ]
    },
    {
        "id": 8,
        "prompt": "def ff(items, targets):\n    \"\"\"First-Fit\n\n    This is perhaps the simplest packing heuristic;\n    it simply packs items in the next available bin.\n\n    Complexity O(n^2)\n    \"\"\"",
        "code": "def ff(items, targets):\n    \"\"\"First-Fit\n\n    This is perhaps the simplest packing heuristic;\n    it simply packs items in the next available bin.\n\n    Complexity O(n^2)\n    \"\"\"\n    bins = []\n    for item in items:\n        placed = False\n        for bin in bins:\n            if bin[1] + item <= targets:\n                bin[1] += item\n                placed = True\n                break\n        if not placed:\n            bins.append([item, item])\n    return bins",
        "writer": "machine",
        "rewrite": [
            "def ff(items, targets):\n    bins = []\n    for item in items:\n        placed = False\n        for i, bin in enumerate(bins):\n            if bin[1] + item <= targets:\n                bins[i][1] += item\n                placed = True\n                break\n        if not placed:\n            bins.append([item, item])\n    return bins",
            "def ff(items, capacity):\n    bins = []\n    for item in items:\n        added = False\n        for i, (first, total) in enumerate(bins):\n            if total + item <= capacity:\n                bins[i] = (first, total + item)\n                added = True\n                break\n        if not added:\n            bins.append((item, item))\n    return bins",
            "def first_fit(items, capacity):\n    bins = []\n    for item in items:\n        assigned = False\n        for i in range(len(bins)):\n            if bins[i][1] + item <= capacity:\n                bins[i][1] += item\n                assigned = True\n                break\n        if not assigned:\n            bins.append([item, item])\n    return bins"
        ]
    },
    {
        "id": 8,
        "prompt": "def ff(items, targets):\n    \"\"\"First-Fit\n\n    This is perhaps the simplest packing heuristic;\n    it simply packs items in the next available bin.\n\n    Complexity O(n^2)\n    \"\"\"",
        "code": "def ff(items, targets):\n    \"\"\"First-Fit\n\n    This is perhaps the simplest packing heuristic;\n    it simply packs items in the next available bin.\n\n    Complexity O(n^2)\n    \"\"\"\n    bins = [(target, []) for target in targets]\n    skip = []\n\n    for item in items:\n        for target, content in bins:\n            if item <= (target - sum(content)):\n                content.append(item)\n                break\n        else:\n            skip.append(item)\n    return bins, skip",
        "writer": "human",
        "rewrite": [
            "def ff(items, targets):\n    bins = [[target] for target in targets]\n    skip = []\n    for item in items:\n        placed = False\n        for i, bin in enumerate(bins):\n            if item <= bin[0] - sum(bin[1:]):\n                bins[i].append(item)\n                placed = True\n                break\n        if not placed:\n            skip.append(item)\n    return [[bin[0], bin[1:]] for bin in bins], skip",
            "def ff(items, targets):\n    bins = [([target], []) for target in targets]\n    skipped = []\n    for item in items:\n        added = False\n        for bin_data in bins:\n            if item <= bin_data[0][0] - sum(bin_data[1]):\n                bin_data[1].append(item)\n                added = True\n                break\n        if not added:\n            skipped.append(item)\n    return [[bin_data[0][0], bin_data[1]] for bin_data in bins], skipped",
            "def first_fit(items, capacities):\n    bins = [[capacity, []] for capacity in capacities]\n    unfitted_items = []\n    for item in items:\n        placed = False\n        for bin_capacity, bin_contents in bins:\n            remaining_capacity = bin_capacity - sum(bin_contents)\n            if item <= remaining_capacity:\n                bin_contents.append(item)\n                placed = True\n                break\n        if not placed:\n            unfitted_items.append(item)\n    return bins, unfitted_items"
        ]
    },
    {
        "id": 9,
        "prompt": "def compress(self, image_path):\n    \"\"\"compress will (properly) compress an image\"\"\"",
        "code": "def compress(self, image_path):\n    \"\"\"compress will (properly) compress an image\"\"\"\n    if os.path.exists(image_path):\n        compressed_image = \"%s.gz\" %image_path\n        os.system('gzip -c -6 %s > %s' %(image_path, compressed_image))\n        return compressed_image\n\n    bot.exit(\"Cannot find image %s\" %image_path)",
        "writer": "human",
        "rewrite": [
            "def compress(self, image_path):\n    import gzip\n    import os\n    if os.path.exists(image_path):\n        with open(image_path, 'rb') as f_in:\n            with gzip.open(image_path + '.gz', 'wb') as f_out:\n                f_out.writelines(f_in)\n        return image_path + '.gz'\n    else:\n        return None",
            "import gzip\nimport os\n\ndef compress(self, image_path):\n    if os.path.exists(image_path):\n        with open(image_path, 'rb') as f_in:\n            with gzip.open(image_path + '.gz', 'wb') as f_out:\n                f_out.writelines(f_in)\n        return image_path + '.gz'\n    return None",
            "import gzip\nimport os\n\ndef compress(self, image_path):\n    if os.path.exists(image_path):\n        with open(image_path, 'rb') as f_in:\n            with gzip.open(image_path + '.gz', 'wb') as f_out:\n                f_out.writelines(f_in)\n        return image_path + '.gz'\n    return None"
        ]
    },
    {
        "id": 10,
        "prompt": "def c_getprocs(self):\n        \"\"\"Get the getprocs of the module.\"\"\"",
        "code": "def c_getprocs(self):\n        \"\"\"Get the getprocs of the module.\"\"\"\n        return self._module.getprocs()",
        "writer": "machine",
        "rewrite": [
            "def getprocs(self):\n    return self._module.getprocs()",
            "def getprocs(self):\n    return self._module.getprocs()",
            "def getprocs(self):\n    return self._module.getprocs()"
        ]
    },
    {
        "id": 11,
        "prompt": "def SLICE(array, n, position=None):\n    \"\"\"\n    Returns a subset of an array.\n    See https://docs.mongodb.com/manual/reference/operator/aggregation/slice/\n    for more details\n    :param array: Any valid expression as long as it resolves to an array.\n    :param n: Any valid expression as long as it resolves to an integer.\n    :param position: Optional. Any valid expression as long as it resolves to an integer.\n    :return: Aggregation operator\n    \"\"\"",
        "code": "def SLICE(array, n, position=None):\n    \"\"\"\n    Returns a subset of an array.\n    See https://docs.mongodb.com/manual/reference/operator/aggregation/slice/\n    for more details\n    :param array: Any valid expression as long as it resolves to an array.\n    :param n: Any valid expression as long as it resolves to an integer.\n    :param position: Optional. Any valid expression as long as it resolves to an integer.\n    :return: Aggregation operator\n    \"\"\"\n    if position is None:\n        return array[0:n]\n    else:\n        return array[position:position+n]",
        "writer": "machine",
        "rewrite": [
            "def SLICE(array, n, position=None):\n    if position is None:\n        return array[:n]\n    return array[position:position + n]",
            "def SLICE(array, n, position=0):\n    return array[position:position + n]",
            "def SLICE(array, n, position=0):\n    return array[position:min(position + n, len(array))]"
        ]
    },
    {
        "id": 7,
        "prompt": "def _HuntObjectFromRow(self, row):\n    \"\"\"Generates a flow object from a database row.\"\"\"",
        "code": "def _HuntObjectFromRow(self, row):\n    \"\"\"Generates a flow object from a database row.\"\"\"\n    (\n        create_time,\n        last_update_time,\n        creator,\n        duration_micros,\n        client_rate,\n        client_limit,\n        hunt_state,\n        hunt_state_comment,\n        init_start_time,\n        last_start_time,\n        num_clients_at_start_time,\n        description,\n        body,\n    ) = row\n    hunt_obj = rdf_hunt_objects.Hunt.FromSerializedString(body)\n    hunt_obj.duration = rdfvalue.Duration.FromMicroseconds(duration_micros)\n    hunt_obj.create_time = mysql_utils.TimestampToRDFDatetime(create_time)\n    hunt_obj.last_update_time = mysql_utils.TimestampToRDFDatetime(\n        last_update_time)\n\n    # Checks below are needed for hunts that were written to the database before\n    # respective fields became part of F1 schema.\n    if creator is not None:\n      hunt_obj.creator = creator\n\n    if client_rate is not None:\n      hunt_obj.client_rate = client_rate\n\n    if client_limit is not None:\n      hunt_obj.client_limit = client_limit\n\n    if hunt_state is not None:\n      hunt_obj.hunt_state = hunt_state\n\n    if hunt_state_comment is not None:\n      hunt_obj.hunt_state_comment = hunt_state_comment\n\n    if init_start_time is not None:\n      hunt_obj.init_start_time = mysql_utils.TimestampToRDFDatetime(\n          init_start_time)\n\n    if last_start_time is not None:\n      hunt_obj.last_start_time = mysql_utils.TimestampToRDFDatetime(\n          last_start_time)\n\n    if num_clients_at_start_time is not None:\n      hunt_obj.num_clients_at_start_time = num_clients_at_start_time\n\n    if description is not None:\n      hunt_obj.description = description\n\n    return hunt_obj",
        "writer": "human",
        "rewrite": [
            "def _HuntObjectFromRow(self, row):\n    (create_time, last_update_time, creator, duration_micros, client_rate, client_limit, hunt_state, hunt_state_comment, init_start_time, last_start_time, num_clients_at_start_time, description, body) = row\n    hunt_obj = rdf_hunt_objects.Hunt.FromSerializedString(body)\n    hunt_obj.duration = rdfvalue.Duration.FromMicroseconds(duration_micros)\n    hunt_obj.create_time = mysql_utils.TimestampToRDFDatetime(create_time)\n    hunt_obj.last_update_time = mysql_utils.TimestampToRDFDatetime(last_update_time)\n    for attr, value in zip([\"creator\", \"client_rate\", \"client_limit\", \"hunt_state\", \"hunt_state_comment\", \"init_start_time\", \"last_start_time\", \"num_clients_at_start_time\", \"description\"], [creator, client_rate, client_limit, hunt_state, hunt_state_comment, init_start_time, last_start_time, num_clients_at_start_time, description]):\n        if value is not None:\n            setattr(hunt_obj, attr, mysql_utils.TimestampToRDFDatetime(value) if isinstance(value, (int, float)) else value)\n    return hunt_obj",
            "def _HuntObjectFromRow(self, row):\n    hunt_fields = [\"create_time\", \"last_update_time\", \"creator\", \"duration_micros\", \"client_rate\", \"client_limit\", \"hunt_state\", \"hunt_state_comment\", \"init_start_time\", \"last_start_time\", \"num_clients_at_start_time\", \"description\", \"body\"]\n    hunt_obj = rdf_hunt_objects.Hunt.FromSerializedString(row[-1])\n    hunt_obj.duration = rdfvalue.Duration.FromMicroseconds(row[3])\n    hunt_obj.create_time = mysql_utils.TimestampToRDFDatetime(row[0])\n    hunt_obj.last_update_time = mysql_utils.TimestampToRDFDatetime(row[1])\n    for i, field in enumerate(hunt_fields[:-1]):\n        if row[i] is not None:\n            value = mysql_utils.TimestampToRDFDatetime(row[i]) if i in (0,1,8,9) else row[i]\n            setattr(hunt_obj, field, value)\n    return hunt_obj",
            "def _HuntObjectFromRow(self, row):\n    kwargs = dict(zip([\"create_time\", \"last_update_time\", \"creator\", \"duration_micros\", \"client_rate\", \"client_limit\", \"hunt_state\", \"hunt_state_comment\", \"init_start_time\", \"last_start_time\", \"num_clients_at_start_time\", \"description\", \"body\"], row))\n    hunt_obj = rdf_hunt_objects.Hunt.FromSerializedString(kwargs[\"body\"])\n    hunt_obj.duration = rdfvalue.Duration.FromMicroseconds(kwargs[\"duration_micros\"])\n    hunt_obj.create_time = mysql_utils.TimestampToRDFDatetime(kwargs[\"create_time\"])\n    hunt_obj.last_update_time = mysql_utils.TimestampToRDFDatetime(kwargs[\"last_update_time\"])\n    for k, v in kwargs.items():\n        if v is not None and k not in [\"body\", \"duration_micros\", \"create_time\", \"last_update_time\"]:\n            if k in [\"init_start_time\", \"last_start_time\"]:\n                v = mysql_utils.TimestampToRDFDatetime(v)\n            setattr(hunt_obj, k, v)\n\n    return hunt_obj"
        ]
    },
    {
        "id": 11,
        "prompt": "def SLICE(array, n, position=None):\n    \"\"\"\n    Returns a subset of an array.\n    See https://docs.mongodb.com/manual/reference/operator/aggregation/slice/\n    for more details\n    :param array: Any valid expression as long as it resolves to an array.\n    :param n: Any valid expression as long as it resolves to an integer.\n    :param position: Optional. Any valid expression as long as it resolves to an integer.\n    :return: Aggregation operator\n    \"\"\"",
        "code": "def SLICE(array, n, position=None):\n    \"\"\"\n    Returns a subset of an array.\n    See https://docs.mongodb.com/manual/reference/operator/aggregation/slice/\n    for more details\n    :param array: Any valid expression as long as it resolves to an array.\n    :param n: Any valid expression as long as it resolves to an integer.\n    :param position: Optional. Any valid expression as long as it resolves to an integer.\n    :return: Aggregation operator\n    \"\"\"\n    return {'$slice': [array, position, n]} if position is not None else {'$slice': [array, n]}",
        "writer": "human",
        "rewrite": [
            "def SLICE(array, n, position=None):\n    if position is not None:\n        return {\"$slice\": [array, position, n]}\n    return {\"$slice\": [array, n]}",
            "def SLICE(array, n, position=None):\n  return {\"$slice\": [array, position, n] if position is not None else [array, n]}",
            "def SLICE(array, n, position=None):\n    params = [array, n]\n    if position is not None:\n        params.insert(1, position)\n    return {'$slice': params}"
        ]
    },
    {
        "id": 13,
        "prompt": "def raise_302(instance, location):\n    \"\"\"Abort the current request with a 302 (Found) response code. Sets the\n    Location header correctly. If the location does not start with a slash,\n    the path of the current request is prepended.\n\n    :param instance: Resource instance (used to access the response)\n    :type instance: :class:`webob.resource.Resource`\n    :raises: :class:`webob.exceptions.ResponseException` of status 302\n    \"\"\"",
        "code": "def raise_302(instance, location):\n    \"\"\"Abort the current request with a 302 (Found) response code. Sets the\n    Location header correctly. If the location does not start with a slash,\n    the path of the current request is prepended.\n\n    :param instance: Resource instance (used to access the response)\n    :type instance: :class:`webob.resource.Resource`\n    :raises: :class:`webob.exceptions.ResponseException` of status 302\n    \"\"\"\n    _set_location(instance, location)\n    instance.response.status = 302\n    raise ResponseException(instance.response)",
        "writer": "human",
        "rewrite": [
            "def raise_302(instance, location):\n    _set_location(instance, location)\n    instance.response.status = 302\n    raise webob.exc.HTTPFound(location=instance.request.path_url + location if not location.startswith('/') else location)",
            "def raise_302(instance, location):\n    if not location.startswith('/'):\n        location = instance.request.path_url + location\n    raise webob.exc.HTTPFound(location=location)",
            "def raise_302(instance, location):\n    location = location if location.startswith('/') else instance.request.path_url + location\n    raise webob.exc.HTTPFound(location=location)"
        ]
    },
    {
        "id": 10,
        "prompt": "def c_getprocs(self):\n        \"\"\"Get the getprocs of the module.\"\"\"",
        "code": "def c_getprocs(self):\n        \"\"\"Get the getprocs of the module.\"\"\"\n        getprocs = []\n        for name, dummy_args in self.funcs:\n            if name == 'GetProcAddress':\n                if callable(self.opts.hash_func):\n                    continue\n                getter = 'get_proc_by_string'\n            elif self.opts.no_structs:\n                getter = 'GetProcAddress'\n            else:\n                getter = 'windll->kernel32.GetProcAddress'\n            if callable(self.opts.hash_func):\n                getter = 'get_proc_by_hash'\n            if self.opts.no_structs:\n                var = name\n            else:\n                var = 'windll->{}.{}'.format(self.name, name)\n            getproc = '{} = ({} *){}({}, {}{});\\n'.format(\n                var,\n                self._c_type_name(name),\n                getter,\n                self._c_base_var(),\n                self.opts.prefix, name\n            )\n            getprocs.append(getproc + self._c_null_check(var))\n        return getprocs",
        "writer": "human",
        "rewrite": [
            "def c_getprocs(self):\n    getprocs = []\n    for name, dummy_args in self.funcs:\n        getter = 'get_proc_by_string' if name == 'GetProcAddress' and not callable(self.opts.hash_func) else ('GetProcAddress' if self.opts.no_structs else 'windll->kernel32.GetProcAddress')\n        getter = 'get_proc_by_hash' if callable(self.opts.hash_func) else getter\n        var = name if self.opts.no_structs else f'windll->{{}}.{{}}'.format(self.name, name)\n        getproc = f'{var} = ({self._c_type_name(name)} *){{getter}}({self._c_base_var()}, {self.opts.prefix}{name});\\n'\n        getprocs.append(getproc + self._c_null_check(var))\n    return getprocs",
            "def c_getprocs(self):\n    return [f'{var} = ({self._c_type_name(name)} *){getter}({self._c_base_var()}, {self.opts.prefix}{name});\\n' + self._c_null_check(var)\n            for name, _ in self.funcs\n            for getter in ([('get_proc_by_string' if name != 'GetProcAddress' or callable(self.opts.hash_func) else 'GetProcAddress') if self.opts.no_structs else ('get_proc_by_hash' if callable(self.opts.hash_func) else 'windll->kernel32.GetProcAddress')]\n            for var in ([name] if self.opts.no_structs else [f'windll->{{}}.{{}}'.format(self.name, name)])]",
            "def c_getprocs(self):\n    getprocs = []\n    for name, _ in self.funcs:\n        if name == 'GetProcAddress' and callable(self.opts.hash_func):\n            continue\n        getter = 'get_proc_by_hash' if callable(self.opts.hash_func) else ('get_proc_by_string' if name == 'GetProcAddress' else ('GetProcAddress' if self.opts.no_structs else 'windll->kernel32.GetProcAddress'))\n        var = name if self.opts.no_structs else f'windll->{{}}.{{}}'.format(self.name, name)\n        getproc = f'{var} = ({self._c_type_name(name)} *){getter}({self._c_base_var()}, {self.opts.prefix}{name});\\n'\n        getprocs.append(getproc + self._c_null_check(var))\n    return getprocs"
        ]
    },
    {
        "id": 13,
        "prompt": "def raise_302(instance, location):\n    \"\"\"Abort the current request with a 302 (Found) response code. Sets the\n    Location header correctly. If the location does not start with a slash,\n    the path of the current request is prepended.\n\n    :param instance: Resource instance (used to access the response)\n    :type instance: :class:`webob.resource.Resource`\n    :raises: :class:`webob.exceptions.ResponseException` of status 302\n    \"\"\"",
        "code": "def raise_302(instance, location):\n    \"\"\"Abort the current request with a 302 (Found) response code. Sets the\n    Location header correctly. If the location does not start with a slash,\n    the path of the current request is prepended.\n\n    :param instance: Resource instance (used to access the response)\n    :type instance: :class:`webob.resource.Resource`\n    :raises: :class:`webob.exceptions.ResponseException` of status 302\n    \"\"\"\n    from webob.exc import HTTPFound\n    if not location.startswith('/'):\n        location = instance.request.path_info + location\n    raise HTTPFound(location=location)",
        "writer": "machine",
        "rewrite": [
            "from webob.exc import HTTPFound\n\ndef raise_302(instance, location):\n    location = instance.request.path_info + location if not location.startswith('/') else location\n    raise HTTPFound(location=location)",
            "from webob.exc import HTTPFound\n\ndef raise_302(instance, location):\n    raise HTTPFound(location=instance.request.path_info + location if not location.startswith('/') else location)",
            "from webob.exc import HTTPFound\n\ndef raise_302(instance, location):\n    location = location if location.startswith('/') else instance.request.path_info + location\n    raise HTTPFound(location=location)"
        ]
    },
    {
        "id": 14,
        "prompt": "def _count_extra_actions(self, game_image):\n        \"\"\"Count the number of extra actions for player in this turn.\"\"\"",
        "code": "def _count_extra_actions(self, game_image):\n        \"\"\"Count the number of extra actions for player in this turn.\"\"\"\n        extra_actions = 0\n        # Implement logic to count extra actions based on game_image\n        # For example:\n        # if some condition based on game_image:\n        #     extra_actions += 1\n        return extra_actions",
        "writer": "machine",
        "rewrite": [
            "def _count_extra_actions(self, game_image):\n    extra_actions = 0\n    return extra_actions",
            "def _count_extra_actions(self, game_image):\n    return 0",
            "def _count_extra_actions(self, game_image):\n    try:\n        if game_image.count('special_item') > 0:\n            return game_image.count('special_item')\n        else: return 0\n    except AttributeError:\n        return 0"
        ]
    },
    {
        "id": 17,
        "prompt": "def add(self, **kwargs):\n        \"\"\"Returns a new MayaDT object with the given offsets.\"\"\"",
        "code": "def add(self, **kwargs):\n        \"\"\"Returns a new MayaDT object with the given offsets.\"\"\"\n        return self.from_datetime(\n            pendulum.instance(self.datetime()).add(**kwargs)\n        )",
        "writer": "human",
        "rewrite": [
            "def add(self, **kwargs):\n    return type(self)(self.datetime().add(**kwargs))",
            "def add(self, **kwargs):\n    return self.from_datetime(self.to_datetime().add(**kwargs))",
            "def add(self, **kwargs):\n    dt = self.to_datetime()\n    dt = dt.add(**kwargs)\n    return type(self).from_datetime(dt)"
        ]
    },
    {
        "id": 18,
        "prompt": "def _periodic_callback(self):\n        \"\"\" Will be started on first emit \"\"\"",
        "code": "def _periodic_callback(self):\n        \"\"\" Will be started on first emit \"\"\"\n        if self._timer is not None:\n            self._timer.cancel()\n        self._timer = Timer(self._interval, self._callback)\n        self._timer.start()",
        "writer": "machine",
        "rewrite": [
            "def _periodic_callback(self):\n    self._timer = Timer(self._interval, self._callback) if self._timer is None else self._timer.cancel() or Timer(self._interval, self._callback)\n    self._timer.start()",
            "def _periodic_callback(self):\n    if self._timer: self._timer.cancel()\n    self._timer = Timer(self._interval, self._callback).start()",
            "def _periodic_callback(self):\n    self._timer = self._timer and self._timer.cancel() or None\n    self._timer = Timer(self._interval, self._callback)\n    self._timer.start()"
        ]
    },
    {
        "id": 14,
        "prompt": "def _count_extra_actions(self, game_image):\n        \"\"\"Count the number of extra actions for player in this turn.\"\"\"",
        "code": "def _count_extra_actions(self, game_image):\n        \"\"\"Count the number of extra actions for player in this turn.\"\"\"\n        proportional = self._bonus_tools['extra_action_region']\n        # Use ProportionalRegion to isolate the extra actions area\n        t, l, b, r = proportional.region_in(game_image)\n        token_region = game_image[t:b, l:r]\n        # Use TemplateFinder (multiple) to check for extra actions\n        game_h, game_w = game_image.shape[0:2]\n        token_h = int(round(game_h * 27.0 / 960))\n        token_w = int(round(game_w * 22.0 / 1280))\n        sizes = (token_h, token_w),\n        # sizes change every time so just remake it.\n        # thresholds are tight since need to count conservatively\n        finder = v.TemplateFinder(pq_data.extra_action_template,\n                                  sizes=sizes,\n                                  acceptable_threshold=0.1,\n                                  immediate_threshold=0.1)\n        found_tokens = finder.locate_multiple_in(token_region)\n        return len(found_tokens)",
        "writer": "human",
        "rewrite": [
            "def _count_extra_actions(self, game_image):\n    t, l, b, r = self._bonus_tools['extra_action_region'].region_in(game_image)\n    token_region = game_image[t:b, l:r]\n    game_h, game_w = game_image.shape[:2]\n    token_h = int(round(game_h * 27 / 960))\n    token_w = int(round(game_w * 22 / 1280))\n    finder = v.TemplateFinder(pq_data.extra_action_template, sizes=((token_h, token_w),), acceptable_threshold=0.1, immediate_threshold=0.1)\n    return len(finder.locate_multiple_in(token_region))",
            "def _count_extra_actions(self, game_image):\n    region = self._bonus_tools['extra_action_region'].region_in(game_image)\n    token_region = game_image[region[0]:region[2], region[1]:region[3]]\n    game_h, game_w = game_image.shape[:2]\n    token_h = int(round(game_h * 27 / 960))\n    token_w = int(round(game_w * 22 / 1280))\n    finder = v.TemplateFinder(pq_data.extra_action_template, sizes=((token_h, token_w),), acceptable_threshold=0.1, immediate_threshold=0.1)\n    return len(finder.locate_multiple_in(token_region))",
            "def _count_extra_actions(self, game_image):\n    top, left, bottom, right = self._bonus_tools['extra_action_region'].region_in(game_image)\n    region = game_image[top:bottom, left:right]\n    height, width = game_image.shape[:2]\n    token_size = (int(round(height * 27 / 960)), int(round(width * 22 / 1280)))\n    finder = v.TemplateFinder(pq_data.extra_action_template, sizes=(token_size,), acceptable_threshold=0.1, immediate_threshold=0.1)\n    return len(finder.locate_multiple_in(region))"
        ]
    },
    {
        "id": 19,
        "prompt": "def add_node(self, node):\n        \"\"\"Add an agent, connecting it to the previous node.\"\"\"",
        "code": "def add_node(self, node):\n        \"\"\"Add an agent, connecting it to the previous node.\"\"\"\n        other_nodes = [n for n in self.nodes() if n.id != node.id]\n\n        if isinstance(node, Source) and other_nodes:\n            raise(Exception(\"Chain network already has a nodes, \"\n                            \"can't add a source.\"))\n\n        if other_nodes:\n            parent = max(other_nodes, key=attrgetter('creation_time'))\n            parent.connect(whom=node)",
        "writer": "human",
        "rewrite": [
            "def add_node(self, node):\n    if isinstance(node, Source) and any(self.nodes()):\n        raise Exception(\"Chain network already has nodes, can't add a source.\")\n    if any(self.nodes()):\n        max_node = max(self.nodes(), key=lambda n: n.creation_time)\n        max_node.connect(whom=node)",
            "def add_node(self, node):\n    if isinstance(node, Source) and list(self.nodes()):\n        raise Exception(\"Chain network already has nodes, can't add a source.\")\n    if self.nodes():\n        parent = max(self.nodes(), key=lambda n: n.creation_time)\n        parent.connect(whom=node)",
            "def add_node(self, node):\n    if isinstance(node, Source) and self.nodes():\n        raise Exception(\"Chain network already has nodes, can't add a source.\")\n    if self.nodes():\n        self.nodes()[-1].connect(whom=node)"
        ]
    },
    {
        "id": 17,
        "prompt": "def add(self, **kwargs):\n        \"\"\"Returns a new MayaDT object with the given offsets.\"\"\"",
        "code": "def add(self, **kwargs):\n        \"\"\"Returns a new MayaDT object with the given offsets.\"\"\"\n        new_dt = self.copy()\n        for key, value in kwargs.items():\n            if key == 'years':\n                new_dt.year += value\n            elif key == 'months':\n                new_dt.month += value\n            elif key == 'days':\n                new_dt.day += value\n            elif key == 'hours':\n                new_dt.hour += value\n            elif key == 'minutes':\n                new_dt.minute += value\n            elif key == 'seconds':\n                new_dt.second += value\n            elif key == 'microseconds':\n                new_dt.microsecond += value\n            else:\n                raise ValueError(f\"Invalid keyword argument: {key}\")\n        return new_dt",
        "writer": "machine",
        "rewrite": [
            "def add(self, **kwargs):\n    new_dt = self.copy()\n    for key, value in kwargs.items():\n        try:\n            setattr(new_dt, key, getattr(new_dt, key) + value)\n        except AttributeError:\n            raise ValueError(f\"Invalid keyword argument: {key}\")\n    return new_dt",
            "def add(self, **kwargs):\n    new_dt = self.copy()\n    attrs = {'years': 'year', 'months': 'month', 'days': 'day', 'hours': 'hour', 'minutes': 'minute', 'seconds': 'second', 'microseconds': 'microsecond'}\n    for key, value in kwargs.items():\n        if key in attrs:\n            setattr(new_dt, attrs[key], getattr(new_dt, attrs[key]) + value)\n        else:\n            raise ValueError(f\"Invalid keyword argument: {key}\")\n    return new_dt",
            "def add(self, **kwargs):\n    new_dt = self.copy()\n    time_attributes = [\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\", \"microsecond\"]\n    for key, value in kwargs.items():\n        try:\n            attribute = key.rstrip('s') #remove plural 's'\n            if attribute in time_attributes:\n                setattr(new_dt, attribute, getattr(new_dt, attribute) + value)\n            else:\n                raise ValueError(f\"Invalid keyword argument: {key}\")\n        except AttributeError:\n            raise ValueError(f\"Invalid keyword argument: {key}\")\n\n    return new_dt"
        ]
    },
    {
        "id": 22,
        "prompt": "def append(self, decoration):\n        \"\"\"\n        Adds a text decoration on a CodeEdit instance\n\n        :param decoration: Text decoration to add\n        :type decoration: pyqode.core.api.TextDecoration\n        \"\"\"",
        "code": "def append(self, decoration):\n        \"\"\"\n        Adds a text decoration on a CodeEdit instance\n\n        :param decoration: Text decoration to add\n        :type decoration: pyqode.core.api.TextDecoration\n        \"\"\"\n        self._editor.decorations.append(decoration)",
        "writer": "machine",
        "rewrite": [
            "def append_decoration(editor, decoration):\n    editor.decorations.append(decoration)",
            "def add_text_decoration(editor, decoration):\n    editor.decorations.append(decoration)",
            "def decorate_text(code_editor, decoration):\n    code_editor.decorations.append(decoration)"
        ]
    },
    {
        "id": 20,
        "prompt": "def predict(self, X, **kwargs):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            New data to predict.\n\n        Returns\n        -------\n        labels : array, shape [n_samples,]\n            Index of the cluster each sample belongs to.\n        \"\"\"",
        "code": "def predict(self, X, **kwargs):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            New data to predict.\n\n        Returns\n        -------\n        labels : array, shape [n_samples,]\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        return [self._closest(x) for x in X]",
        "writer": "machine",
        "rewrite": [
            "def predict(self, X, **kwargs):\n    return np.argmin(np.sum((X[:, np.newaxis, :] - self.cluster_centers_) ** 2, axis=2), axis=1)",
            "def predict(self, X, **kwargs):\n    distances = cdist(X, self.cluster_centers_)\n    return np.argmin(distances, axis=1)",
            "def predict(self, X, **kwargs):\n    return np.apply_along_axis(self._closest, axis=1, arr=X)"
        ]
    },
    {
        "id": 19,
        "prompt": "def add_node(self, node):\n        \"\"\"Add an agent, connecting it to the previous node.\"\"\"",
        "code": "def add_node(self, node):\n        \"\"\"Add an agent, connecting it to the previous node.\"\"\"\n        if self.head is None:\n            self.head = node\n            self.tail = node\n        else:\n            self.tail.next = node\n            self.tail = node",
        "writer": "machine",
        "rewrite": [
            "def add_node(self, node):\n    if not self.head:\n        self.head = self.tail = node\n    else:\n        self.tail.next = node\n        self.tail = node",
            "def add_node(self, node):\n    if self.head is None:\n        self.head = self.tail = node\n    else:\n        self.tail.next = node\n        self.tail = node",
            "def add_node(self, node):\n    if not self.head:\n        self.head = node\n        self.tail = node\n    else:\n        self.tail.next = node\n        self.tail = node"
        ]
    },
    {
        "id": 22,
        "prompt": "def append(self, decoration):\n        \"\"\"\n        Adds a text decoration on a CodeEdit instance\n\n        :param decoration: Text decoration to add\n        :type decoration: pyqode.core.api.TextDecoration\n        \"\"\"",
        "code": "def append(self, decoration):\n        \"\"\"\n        Adds a text decoration on a CodeEdit instance\n\n        :param decoration: Text decoration to add\n        :type decoration: pyqode.core.api.TextDecoration\n        \"\"\"\n        if decoration not in self._decorations:\n            self._decorations.append(decoration)\n            self._decorations = sorted(\n                self._decorations, key=lambda sel: sel.draw_order)\n            self.editor.setExtraSelections(self._decorations)\n            return True\n        return False",
        "writer": "human",
        "rewrite": [
            "def append(self, decoration):\n    if decoration not in self._decorations:\n        self._decorations.append(decoration)\n        self._decorations.sort(key=lambda deco: deco.draw_order)\n        self.editor.setExtraSelections(self._decorations)\n        return True\n    return False",
            "def append(self, decoration):\n    if decoration not in self._decorations:\n        self._decorations.append(decoration)\n        self._decorations.sort(key=lambda d: d.draw_order)\n        self.editor.setExtraSelections(self._decorations)\n        return True\n    return False",
            "def append(self, decoration):\n    try:\n        self._decorations.remove(decoration)\n    except ValueError:\n        pass\n    self._decorations.append(decoration)\n    self._decorations.sort(key=lambda d: d.draw_order)\n    self.editor.setExtraSelections(self._decorations)\n    return True"
        ]
    },
    {
        "id": 23,
        "prompt": "def transliterate(self, target_language=\"en\"):\n    \"\"\"Transliterate the string to the target language.\"\"\"",
        "code": "def transliterate(self, target_language=\"en\"):\n    \"\"\"Transliterate the string to the target language.\"\"\"\n    t = Transliterator(source_lang=self.language,\n                       target_lang=target_language)\n    return t.transliterate(self.string)",
        "writer": "human",
        "rewrite": [
            "def transliterate(self, target_language=\"en\"):\n    return Transliterator(self.language, target_language).transliterate(self.string)",
            "def transliterate(self, target_language=\"en\"):\n    return Transliterator(source_lang=self.language, target_lang=target_language).transliterate(self.string)",
            "def transliterate(self, target_language=\"en\"):\n    return Transliterator(self.language, target_language).transliterate(self.string)"
        ]
    },
    {
        "id": 20,
        "prompt": "def predict(self, X, **kwargs):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            New data to predict.\n\n        Returns\n        -------\n        labels : array, shape [n_samples,]\n            Index of the cluster each sample belongs to.\n        \"\"\"",
        "code": "def predict(self, X, **kwargs):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            New data to predict.\n\n        Returns\n        -------\n        labels : array, shape [n_samples,]\n            Index of the cluster each sample belongs to.\n        \"\"\"\n\n        assert hasattr(self, '_enc_cluster_centroids'), \"Model not yet fitted.\"\n\n        if self.verbose and self.cat_dissim == ng_dissim:\n            print(\"Ng's dissimilarity measure was used to train this model, \"\n                  \"but now that it is predicting the model will fall back to \"\n                  \"using simple matching dissimilarity.\")\n\n        X = pandas_to_numpy(X)\n        X = check_array(X, dtype=None)\n        X, _ = encode_features(X, enc_map=self._enc_map)\n        return _labels_cost(X, self._enc_cluster_centroids, self.cat_dissim)[0]",
        "writer": "human",
        "rewrite": [
            "def predict(self, X, **kwargs):\n    assert hasattr(self, '_enc_cluster_centroids')\n    if self.verbose and self.cat_dissim == ng_dissim:\n        print(\"Ng's dissimilarity measure was used to train this model, but now that it is predicting the model will fall back to using simple matching dissimilarity.\")\n    X = pandas_to_numpy(X)\n    X = check_array(X, dtype=None)\n    X, _ = encode_features(X, enc_map=self._enc_map)\n    return _labels_cost(X, self._enc_cluster_centroids, self.cat_dissim)[0]",
            "def predict(self, X, **kwargs):\n    if not hasattr(self, '_enc_cluster_centroids'):\n        raise Exception(\"Model not yet fitted.\")\n    if self.verbose and self.cat_dissim is ng_dissim:\n        print(\"Ng's dissimilarity measure was used to train this model, but now that it is predicting the model will fall back to using simple matching dissimilarity.\")\n    X = np.array(X) if isinstance(X, pd.DataFrame) else X\n    X = check_array(X, dtype=None)\n    X, _ = encode_features(X, enc_map=self._enc_map)\n    return _labels_cost(X, self._enc_cluster_centroids, self.cat_dissim)[0]",
            "def predict(self, X, **kwargs):\n    if not hasattr(self, '_enc_cluster_centroids'): raise RuntimeError(\"Model not fitted\")\n    if self.verbose and self.cat_dissim is ng_dissim: print(\"Warning: Falling back to simple matching dissimilarity for prediction.\")\n    X = np.asarray(X)\n    X = check_array(X)\n    X, _ = encode_features(X, self._enc_map)\n    return _labels_cost(X, self._enc_cluster_centroids, self.cat_dissim)[0]"
        ]
    },
    {
        "id": 23,
        "prompt": "def transliterate(self, target_language=\"en\"):\n    \"\"\"Transliterate the string to the target language.\"\"\"",
        "code": "def transliterate(self, target_language=\"en\"):\n    \"\"\"Transliterate the string to the target language.\"\"\"\n    if target_language == \"en\":\n        return self\n    else:\n        raise NotImplementedError(f\"Transliteration to {target_language} is not supported yet.\")",
        "writer": "machine",
        "rewrite": [
            "def transliterate(self, target_language=\"en\"):\n    if target_language != \"en\":\n        raise NotImplementedError(f\"Transliteration to {target_language} is not implemented.\")\n    return self",
            "def transliterate(self, target_language=\"en\"):\n  return self if target_language == \"en\" else NotImplementedError(f\"Transliteration to {target_language} not supported.\")",
            "def transliterate(self, target_language='en'):\n    if target_language != 'en':\n        raise NotImplementedError(f\"Transliteration to {target_language} is not supported.\")\n    return self"
        ]
    },
    {
        "id": 18,
        "prompt": "def _periodic_callback(self):\n        \"\"\" Will be started on first emit \"\"\"",
        "code": "def _periodic_callback(self):\n        \"\"\" Will be started on first emit \"\"\"\n        try:\n            self.notify(self._state)  # emit to all subscribers\n        except Exception:  # pylint: disable=broad-except\n            self._error_callback(*sys.exc_info())\n\n        if self._subscriptions:\n            # if there are still subscriptions register next _periodic callback\n            self._call_later_handle = \\\n                self._loop.call_later(self._interval, self._periodic_callback)\n        else:\n            self._state = NONE\n            self._call_later_handle = None",
        "writer": "human",
        "rewrite": [
            "def _periodic_callback(self):\n    try:\n        self.notify(self._state)\n    except Exception as e:\n        self._error_callback(type(e), e, e.__traceback__)\n\n    if self._subscriptions:\n        self._call_later_handle = self._loop.call_later(self._interval, self._periodic_callback)\n    else:\n        self._state = None\n        self._call_later_handle = None",
            "def _periodic_callback(self):\n    try:\n        self.notify(self._state)\n    except Exception as e:\n        self._error_callback(type(e), e, e.__traceback__)\n\n    if self._subscriptions:\n        self._call_later_handle = self._loop.call_later(self._interval, self._periodic_callback)\n    else:\n        self._state = None\n        self._call_later_handle = None",
            "def _periodic_callback(self):\n    try:\n        self.notify(self._state)\n    except Exception as e:\n        self._error_callback(e)\n    \n    if self._subscriptions:\n        self._call_later_handle = self._loop.call_later(self._interval, self._periodic_callback)\n    else:\n        self._state = None\n        self._call_later_handle = None"
        ]
    },
    {
        "id": 27,
        "prompt": "def clean(self, value):\n\t\t\"\"\"Clean\n\n\t\tUses the valid method to check which type the value is, and then calls\n\t\tthe correct version of clean on that node\n\n\t\tArguments:\n\t\t\tvalue {mixed} -- The value to clean\n\n\t\tReturns:\n\t\t\tmixed\n\t\t\"\"\"",
        "code": "def clean(self, value):\n\t\t\"\"\"Clean\n\n\t\tUses the valid method to check which type the value is, and then calls\n\t\tthe correct version of clean on that node\n\n\t\tArguments:\n\t\t\tvalue {mixed} -- The value to clean\n\n\t\tReturns:\n\t\t\tmixed\n\t\t\"\"\"\n\t\tif self.valid(value):\n\t\t\treturn self.clean_valid(value)\n\t\telse:\n\t\t\treturn self.clean_invalid(value)",
        "writer": "machine",
        "rewrite": [
            "def clean(self, value):\n    return self.clean_valid(value) if self.valid(value) else self.clean_invalid(value)",
            "def clean(self, value):\n    return self.clean_valid(value) if self.valid(value) else self.clean_invalid(value)",
            "def clean(self, value):\n    return self.clean_valid(value) if self.valid(value) else self.clean_invalid(value)"
        ]
    },
    {
        "id": 27,
        "prompt": "def clean(self, value):\n\t\t\"\"\"Clean\n\n\t\tUses the valid method to check which type the value is, and then calls\n\t\tthe correct version of clean on that node\n\n\t\tArguments:\n\t\t\tvalue {mixed} -- The value to clean\n\n\t\tReturns:\n\t\t\tmixed\n\t\t\"\"\"",
        "code": "def clean(self, value):\n\t\t\"\"\"Clean\n\n\t\tUses the valid method to check which type the value is, and then calls\n\t\tthe correct version of clean on that node\n\n\t\tArguments:\n\t\t\tvalue {mixed} -- The value to clean\n\n\t\tReturns:\n\t\t\tmixed\n\t\t\"\"\"\n\n\t\t# If the value is None and it's optional, return as is\n\t\tif value is None and self._optional:\n\t\t\treturn None\n\n\t\t# Go through each of the nodes\n\t\tfor i in range(len(self._nodes)):\n\n\t\t\t# If it's valid\n\t\t\tif self._nodes[i].valid(value):\n\n\t\t\t\t# Use it's clean\n\t\t\t\treturn self._nodes[i].clean(value)\n\n\t\t# Something went wrong\n\t\traise ValueError('value', value)",
        "writer": "human",
        "rewrite": [
            "def clean(self, value):\n    if value is None and self._optional:\n        return None\n    for node in self._nodes:\n        if node.valid(value):\n            return node.clean(value)\n    raise ValueError('value', value)",
            "def clean(self, value):\n    if value is None and self._optional: return None\n    for node in self._nodes:\n        if node.valid(value): return node.clean(value)\n    raise ValueError('value', value)",
            "def clean(self, value):\n    if value is None and self._optional:\n        return None\n    try:\n        return next(node.clean(value) for node in self._nodes if node.valid(value))\n    except StopIteration:\n        raise ValueError('value', value)"
        ]
    },
    {
        "id": 26,
        "prompt": "def flatten(obj, isinstance=isinstance, StringTypes=StringTypes,\n            SequenceTypes=SequenceTypes, do_flatten=do_flatten):\n    \"\"\"Flatten a sequence to a non-nested list.\n\n    Flatten() converts either a single scalar or a nested sequence\n    to a non-nested list. Note that flatten() considers strings\n    to be scalars instead of sequences like Python would.\n    \"\"\"",
        "code": "def flatten(obj, isinstance=isinstance, StringTypes=StringTypes,\n            SequenceTypes=SequenceTypes, do_flatten=do_flatten):\n    \"\"\"Flatten a sequence to a non-nested list.\n\n    Flatten() converts either a single scalar or a nested sequence\n    to a non-nested list. Note that flatten() considers strings\n    to be scalars instead of sequences like Python would.\n    \"\"\"\n    if isinstance(obj, StringTypes) or not isinstance(obj, SequenceTypes):\n        return [obj]\n    result = []\n    for item in obj:\n        if isinstance(item, StringTypes) or not isinstance(item, SequenceTypes):\n            result.append(item)\n        else:\n            do_flatten(item, result)\n    return result",
        "writer": "human",
        "rewrite": [
            "def flatten(obj):\n    if isinstance(obj, str) or not isinstance(obj, (list, tuple)):\n        return [obj]\n    result = []\n    for item in obj:\n        if isinstance(item, str) or not isinstance(item, (list, tuple)):\n            result.append(item)\n        else:\n            result.extend(flatten(item))\n    return result",
            "def flatten(obj):\n    if isinstance(obj, str) or not isinstance(obj, (list, tuple)):\n        return [obj]\n    flat_list = []\n    for sublist in obj:\n        if isinstance(sublist, str) or not isinstance(sublist, (list, tuple)):\n            flat_list.append(sublist)\n        else:\n            flat_list.extend(flatten(sublist))\n    return flat_list",
            "def flatten(obj):\n    if isinstance(obj, str) or not isinstance(obj, (list, tuple)):\n        return [obj]\n    flat_list = []\n    for item in obj:\n        flat_list.extend(flatten(item) if isinstance(item, (list, tuple)) else [item])\n    return flat_list"
        ]
    },
    {
        "id": 26,
        "prompt": "def flatten(obj, isinstance=isinstance, StringTypes=StringTypes,\n            SequenceTypes=SequenceTypes, do_flatten=do_flatten):\n    \"\"\"Flatten a sequence to a non-nested list.\n\n    Flatten() converts either a single scalar or a nested sequence\n    to a non-nested list. Note that flatten() considers strings\n    to be scalars instead of sequences like Python would.\n    \"\"\"",
        "code": "def flatten(obj, isinstance=isinstance, StringTypes=str,\n            SequenceTypes=(list, tuple, set), do_flatten=True):\n    \"\"\"Flatten a sequence to a non-nested list.\n\n    Flatten() converts either a single scalar or a nested sequence\n    to a non-nested list. Note that flatten() considers strings\n    to be scalars instead of sequences like Python would.\n    \"\"\"\n    if not do_flatten:\n        return obj\n    if isinstance(obj, StringTypes):\n        return [obj]\n    if isinstance(obj, SequenceTypes):\n        return [item for sublist in obj for item in flatten(sublist, do_flatten=do_flatten)]\n    return [obj]",
        "writer": "machine",
        "rewrite": [
            "def flatten(obj):\n    if isinstance(obj, str):\n        return [obj]\n    if isinstance(obj, (list, tuple, set)):\n        return [item for sublist in obj for item in flatten(sublist)]\n    return [obj]",
            "def flatten(obj):\n    if isinstance(obj, str):\n        return [obj]\n    if isinstance(obj, (list, tuple, set)):\n        flat_list = []\n        for sublist in obj:\n            flat_list.extend(flatten(sublist))\n        return flat_list\n    return [obj]",
            "def flatten(obj):\n    if isinstance(obj, str):\n        return [obj]\n    if isinstance(obj, (list, tuple, set)):\n        result = []\n        for element in obj:\n            result.extend(flatten(element))\n        return result\n    return [obj]"
        ]
    },
    {
        "id": 28,
        "prompt": "def _co_names_from_config(self):\n        \"\"\"\n        Parse the configuration for the names of the COs for which to\n        construct virtual IdPs.\n\n        :rtype: [str]\n\n        :return: list of CO names\n        \"\"\"",
        "code": "def _co_names_from_config(self):\n        \"\"\"\n        Parse the configuration for the names of the COs for which to\n        construct virtual IdPs.\n\n        :rtype: [str]\n\n        :return: list of CO names\n        \"\"\"\n        co_names = []\n        for co in self.config.get('virtual_idps', []):\n            co_names.append(co['co_name'])\n        return co_names",
        "writer": "machine",
        "rewrite": [
            "def _co_names_from_config(self):\n    return [co['co_name'] for co in self.config.get('virtual_idps', [])]",
            "def _co_names_from_config(self):\n    try:\n        return [co['co_name'] for co in self.config['virtual_idps']]\n    except (KeyError, TypeError):\n        return []",
            "def _co_names_from_config(self):\n    return [co.get('co_name') for co in self.config.get('virtual_idps', []) if isinstance(co, dict) and 'co_name' in co]"
        ]
    },
    {
        "id": 28,
        "prompt": "def _co_names_from_config(self):\n        \"\"\"\n        Parse the configuration for the names of the COs for which to\n        construct virtual IdPs.\n\n        :rtype: [str]\n\n        :return: list of CO names\n        \"\"\"",
        "code": "def _co_names_from_config(self):\n        \"\"\"\n        Parse the configuration for the names of the COs for which to\n        construct virtual IdPs.\n\n        :rtype: [str]\n\n        :return: list of CO names\n        \"\"\"\n        co_names = [co[self.KEY_ENCODEABLE_NAME] for\n                    co in self.config[self.KEY_CO]]\n\n        return co_names",
        "writer": "human",
        "rewrite": [
            "def _co_names_from_config(self):\n    return [co[self.KEY_ENCODEABLE_NAME] for co in self.config.get(self.KEY_CO, [])]",
            "def _co_names_from_config(self):\n    try:\n        return [co[self.KEY_ENCODEABLE_NAME] for co in self.config[self.KEY_CO]]\n    except (KeyError, TypeError, IndexError):\n        return []",
            "def _co_names_from_config(self):\n    return [co.get(self.KEY_ENCODEABLE_NAME) for co in self.config.get(self.KEY_CO, [])]"
        ]
    },
    {
        "id": 30,
        "prompt": "def add_router(self, path, router):\n        \"\"\"\n        Add a (regex, router) pair to this router. Any req.path that\n        matches the regex will pass the request/response objects to\n        that router.\n        \"\"\"",
        "code": "def add_router(self, path, router):\n        \"\"\"\n        Add a (regex, router) pair to this router. Any req.path that\n        matches the regex will pass the request/response objects to\n        that router.\n        \"\"\"\n        self.routers.append((path, router))",
        "writer": "machine",
        "rewrite": [
            "def add_router(self, path, router):\n    self.routers.append((path,router))",
            "def add_router(self, path, router):\n    self.routers.append((path, router))",
            "def add_router(self, path, router):\n    self.routers.append((path, router))"
        ]
    },
    {
        "id": 30,
        "prompt": "def add_router(self, path, router):\n        \"\"\"\n        Add a (regex, router) pair to this router. Any req.path that\n        matches the regex will pass the request/response objects to\n        that router.\n        \"\"\"",
        "code": "def add_router(self, path, router):\n        \"\"\"\n        Add a (regex, router) pair to this router. Any req.path that\n        matches the regex will pass the request/response objects to\n        that router.\n        \"\"\"\n        self.add(HTTPMethod.ALL, path, router)\n        return self",
        "writer": "human",
        "rewrite": [
            "def add_router(self, path, router):\n    self.add(HTTPMethod.ALL, path, router)\n    return self",
            "def add_router(self, path, router):\n    self._routes.append((path, router))\n    return self",
            "def add_router(self, path, router):\n    self.routes.setdefault(path, []).append(router)\n    return self"
        ]
    },
    {
        "id": 31,
        "prompt": "def http_stream(url, chunk=4096):\n    \"\"\"Generate stream for a given record video.\n\n    :param chunk: chunk bytes to read per time\n    :returns generator object\n    \"\"\"",
        "code": "def http_stream(url, chunk=4096):\n    \"\"\"Generate stream for a given record video.\n\n    :param chunk: chunk bytes to read per time\n    :returns generator object\n    \"\"\"\n    import requests\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        for chunk in r.iter_content(chunk_size=chunk):\n            yield chunk",
        "writer": "machine",
        "rewrite": [
            "def http_stream(url, chunk=4096):\n    import requests\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        yield from r.iter_content(chunk_size=chunk)",
            "def http_stream(url, chunk_size=4096):\n    import requests\n    with requests.get(url, stream=True) as response:\n        response.raise_for_status()\n        yield from response.iter_content(chunk_size=chunk_size)",
            "def http_stream(url, chunk_size=4096):\n    import requests\n    with requests.get(url, stream=True) as response:\n        response.raise_for_status()\n        for chunk in response.iter_content(chunk_size=chunk_size):\n            yield chunk"
        ]
    },
    {
        "id": 31,
        "prompt": "def http_stream(url, chunk=4096):\n    \"\"\"Generate stream for a given record video.\n\n    :param chunk: chunk bytes to read per time\n    :returns generator object\n    \"\"\"",
        "code": "def http_stream(url, chunk=4096):\n    \"\"\"Generate stream for a given record video.\n\n    :param chunk: chunk bytes to read per time\n    :returns generator object\n    \"\"\"\n    ret = requests.get(url, stream=True)\n    ret.raise_for_status()\n    for data in ret.iter_content(chunk):\n        yield data",
        "writer": "human",
        "rewrite": [
            "def http_stream(url, chunk=4096):\n    with requests.get(url, stream=True) as response:\n        response.raise_for_status()\n        for chunk in response.iter_content(chunk_size=chunk):\n            yield chunk",
            "import requests\n\ndef http_stream(url, chunk_size=4096):\n    with requests.get(url, stream=True) as response:\n        response.raise_for_status()\n        for chunk in response.iter_content(chunk_size=chunk_size):\n            yield chunk",
            "import requests\n\ndef http_stream(url, chunk_size=4096):\n    try:\n        with requests.get(url, stream=True) as response:\n            response.raise_for_status()\n            yield from response.iter_content(chunk_size=chunk_size)\n    except requests.exceptions.RequestException as e:\n        yield from (e,)"
        ]
    },
    {
        "id": 33,
        "prompt": "def _to_edit(self, infoid):\n        \"\"\"\n        render the HTML page for post editing.\n        \"\"\"",
        "code": "def _to_edit(self, infoid):\n        \"\"\"\n        render the HTML page for post editing.\n        \"\"\"\n        post = self.get_post(infoid)\n        if not post:\n            return self.render_error(404, \"Post not found\")\n        return self.render_template(\"edit.html\", post=post)",
        "writer": "machine",
        "rewrite": [
            "def _to_edit(self, infoid):\n    post = self.get_post(infoid)\n    return self.render_template(\"edit.html\", post=post) if post else self.render_error(404, \"Post not found\")",
            "def _to_edit(self, infoid):\n    try:\n        post = self.get_post(infoid)\n        return self.render_template(\"edit.html\", post=post)\n    except Exception as e:\n        return self.render_error(404, f\"Post not found: {e}\")",
            "def _to_edit(self, infoid):\n    post = self.get_post(infoid)\n    return self.render_error(404, \"Post not found\") if post is None else self.render_template(\"edit.html\", post=post)"
        ]
    },
    {
        "id": 24,
        "prompt": "def to_example(dictionary):\n  \"\"\"Helper: build tf.Example from (string -> int/float/str list) dictionary.\"\"\"",
        "code": "def to_example(dictionary):\n  \"\"\"Helper: build tf.Example from (string -> int/float/str list) dictionary.\"\"\"\n  features = {}\n  for (k, v) in six.iteritems(dictionary):\n    if not v:\n      raise ValueError(\"Empty generated field: %s\" % str((k, v)))\n    if isinstance(v[0], six.integer_types):\n      features[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n    elif isinstance(v[0], float):\n      features[k] = tf.train.Feature(float_list=tf.train.FloatList(value=v))\n    elif isinstance(v[0], six.string_types):\n      if not six.PY2:  # Convert in python 3.\n        v = [bytes(x, \"utf-8\") for x in v]\n      features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n    elif isinstance(v[0], bytes):\n      features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n    else:\n      raise ValueError(\"Value for %s is not a recognized type; v: %s type: %s\" %\n                       (k, str(v[0]), str(type(v[0]))))\n  return tf.train.Example(features=tf.train.Features(feature=features))",
        "writer": "human",
        "rewrite": [
            "def to_example(dictionary):\n  features = {}\n  for k, v in dictionary.items():\n    if not v:\n      raise ValueError(f\"Empty generated field: {(k, v)}\")\n    if all(isinstance(x, int) for x in v):\n      features[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n    elif all(isinstance(x, float) for x in v):\n      features[k] = tf.train.Feature(float_list=tf.train.FloatList(value=v))\n    elif all(isinstance(x, str) for x in v):\n      v = [x.encode(\"utf-8\") for x in v]\n      features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n    elif all(isinstance(x, bytes) for x in v):\n      features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n    else:\n      raise ValueError(f\"Value for {k} is not a recognized type; v: {v[0]} type: {type(v[0])}\")\n  return tf.train.Example(features=tf.train.Features(feature=features))",
            "import tensorflow as tf\n\ndef to_example(dictionary):\n  features = {}\n  for k, v in dictionary.items():\n    if not v:\n      raise ValueError(f\"Empty generated field: {k}\")\n    if all(isinstance(x, int) for x in v):\n      features[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n    elif all(isinstance(x, float) for x in v):\n      features[k] = tf.train.Feature(float_list=tf.train.FloatList(value=v))\n    elif all(isinstance(x, str) for x in v):\n      features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[x.encode('utf-8') for x in v]))\n    elif all(isinstance(x, bytes) for x in v):\n      features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n    else:\n      raise ValueError(f\"Value for {k} is not a recognized type: {type(v[0])}\")\n  return tf.train.Example(features=tf.train.Features(feature=features))",
            "import tensorflow as tf\n\ndef to_example(dictionary):\n    features = {}\n    for k, v in dictionary.items():\n        if not v:\n            raise ValueError(f\"Empty field: {k}\")\n        if all(isinstance(x, int) for x in v):\n            features[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n        elif all(isinstance(x, float) for x in v):\n            features[k] = tf.train.Feature(float_list=tf.train.FloatList(value=v))\n        elif all(isinstance(x, str) for x in v):\n            features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[x.encode('utf-8') for x in v]))\n        elif all(isinstance(x, bytes) for x in v):\n            features[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n        else:\n            raise ValueError(f\"Unsupported type for field {k}: {type(v[0])}\")\n    return tf.train.Example(features=tf.train.Features(feature=features))"
        ]
    },
    {
        "id": 24,
        "prompt": "def to_example(dictionary):\n  \"\"\"Helper: build tf.Example from (string -> int/float/str list) dictionary.\"\"\"",
        "code": "def to_example(dictionary):\n  \"\"\"Helper: build tf.Example from (string -> int/float/str list) dictionary.\"\"\"\n  feature = {}\n  for k, v in dictionary.items():\n    if isinstance(v, list):\n      feature[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n    elif isinstance(v, int):\n      feature[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=[v]))\n    elif isinstance(v, float):\n      feature[k] = tf.train.Feature(float_list=tf.train.FloatList(value=[v]))\n    elif isinstance(v, str):\n      feature[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[v.encode('utf-8')]))\n    else:\n      raise ValueError('Value type not supported: %s' % type(v))\n  return tf.train.Example(features=tf.train.Features(feature=feature))",
        "writer": "machine",
        "rewrite": [
            "def to_example(dictionary):\n  feature = {}\n  for k, v in dictionary.items():\n    if isinstance(v, list):\n      if all(isinstance(x, int) for x in v):\n        feature[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n      elif all(isinstance(x, float) for x in v):\n        feature[k] = tf.train.Feature(float_list=tf.train.FloatList(value=v))\n      elif all(isinstance(x, str) for x in v):\n        feature[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[x.encode('utf-8') for x in v]))\n      else:\n        raise ValueError('List elements must be of same numeric or string type')\n    elif isinstance(v, int):\n      feature[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=[v]))\n    elif isinstance(v, float):\n      feature[k] = tf.train.Feature(float_list=tf.train.FloatList(value=[v]))\n    elif isinstance(v, str):\n      feature[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[v.encode('utf-8')]))\n    else:\n      raise ValueError('Value type not supported: %s' % type(v))\n  return tf.train.Example(features=tf.train.Features(feature=feature))",
            "def to_example(dictionary):\n    features = {k: tf.train.Feature(int64_list=tf.train.Int64List(value=[v])) if isinstance(v, int)\n                else tf.train.Feature(float_list=tf.train.FloatList(value=[v])) if isinstance(v, float)\n                else tf.train.Feature(bytes_list=tf.train.BytesList(value=[v.encode('utf-8')])) if isinstance(v, str)\n                else tf.train.Feature(int64_list=tf.train.Int64List(value=v)) if isinstance(v,list) and all(isinstance(i,int) for i in v)\n                else tf.train.Feature(float_list=tf.train.FloatList(value=v)) if isinstance(v,list) and all(isinstance(i,float) for i in v)\n                else tf.train.Feature(bytes_list=tf.train.BytesList(value=[i.encode('utf-8') for i in v])) if isinstance(v,list) and all(isinstance(i,str) for i in v)\n                else None for k, v in dictionary.items()}\n    if None in features.values():\n        raise ValueError('Value type not supported')\n    return tf.train.Example(features=tf.train.Features(feature=features))",
            "def to_example(d):\n    f = {}\n    for k,v in d.items():\n        if isinstance(v,list):\n            if all(isinstance(x,int) for x in v):\n                f[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n            elif all(isinstance(x,float) for x in v):\n                f[k] = tf.train.Feature(float_list=tf.train.FloatList(value=v))\n            elif all(isinstance(x,str) for x in v):\n                f[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[x.encode() for x in v]))\n            else: raise ValueError\n        elif isinstance(v,int): f[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=[v]))\n        elif isinstance(v,float): f[k] = tf.train.Feature(float_list=tf.train.FloatList(value=[v]))\n        elif isinstance(v,str): f[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[v.encode()]))\n        else: raise ValueError\n    return tf.train.Example(features=tf.train.Features(feature=f))"
        ]
    },
    {
        "id": 36,
        "prompt": "def _evaluate(self, val_iter: data_io.BaseParallelSampleIter, val_metric: mx.metric.EvalMetric):\n        \"\"\"\n        Evaluates the model on the validation data and updates the validation metric(s).\n        \"\"\"",
        "code": "def _evaluate(self, val_iter: data_io.BaseParallelSampleIter, val_metric: mx.metric.EvalMetric):\n        \"\"\"\n        Evaluates the model on the validation data and updates the validation metric(s).\n        \"\"\"\n        val_iter.reset()\n        val_metric.reset()\n        self.model.evaluate(val_iter, val_metric)",
        "writer": "human",
        "rewrite": [
            "def _evaluate(self, val_iter, val_metric):\n    val_iter.reset()\n    val_metric.reset()\n    self.model.evaluate(val_iter, val_metric)",
            "def evaluate_model(model, val_iter, val_metric):\n    val_iter.reset()\n    val_metric.reset()\n    model.evaluate(val_iter, val_metric)",
            "def evaluate(model, val_iter, val_metric):\n    val_iter.reset()\n    val_metric.reset()\n    return model.evaluate(val_iter, val_metric)"
        ]
    },
    {
        "id": 34,
        "prompt": "def on_binlog(event, stream):\n    \"\"\" Process on a binlog event\n\n    1. Convert event instance into a dict\n    2. Send corresponding schema/table/signals\n\n    Args:\n        event (pymysqlreplication.row_event.RowsEvent): the event\n    \"\"\"",
        "code": "def on_binlog(event, stream):\n    \"\"\" Process on a binlog event\n\n    1. Convert event instance into a dict\n    2. Send corresponding schema/table/signals\n\n    Args:\n        event (pymysqlreplication.row_event.RowsEvent): the event\n    \"\"\"\n    event_dict = {\n        'header': {\n            'timestamp': event.timestamp,\n            'server_id': event.server_id,\n            'event_type': event.event_type,\n            'schema': event.schema,\n            'table': event.table,\n            'log_pos': event.log_pos,\n            'flags': event.flags,\n        },\n        'data': [],\n    }\n    for row in event.rows:\n        row_dict = {}\n        for i, value in enumerate(row):\n            row_dict[event.columns[i]] = value\n        event_dict['data'].append(row_dict)\n    # Send corresponding schema/table/signals\n    # ...",
        "writer": "machine",
        "rewrite": [
            "def on_binlog(event, stream):\n    event_dict = {'header': {'timestamp': event.timestamp, 'server_id': event.server_id, 'event_type': event.event_type, 'schema': event.schema, 'table': event.table, 'log_pos': event.log_pos, 'flags': event.flags}, 'data': [{event.columns[i]: value for i, value in enumerate(row)} for row in event.rows]}",
            "def on_binlog(event, stream):\n    event_dict = {'header': {'timestamp': event.timestamp, 'server_id': event.server_id, 'event_type': event.event_type, 'schema': event.schema, 'table': event.table, 'log_pos': event.log_pos, 'flags': event.flags}, 'data': [{k: v for k, v in zip(event.columns, row)} for row in event.rows]}",
            "def on_binlog(event, stream):\n    header = {'timestamp': event.timestamp, 'server_id': event.server_id, 'event_type': event.event_type, 'schema': event.schema, 'table': event.table, 'log_pos': event.log_pos, 'flags': event.flags}\n    data = [{k: v for k, v in zip(event.columns, row)} for row in event.rows]\n    event_dict = {'header': header, 'data': data}"
        ]
    },
    {
        "id": 34,
        "prompt": "def on_binlog(event, stream):\n    \"\"\" Process on a binlog event\n\n    1. Convert event instance into a dict\n    2. Send corresponding schema/table/signals\n\n    Args:\n        event (pymysqlreplication.row_event.RowsEvent): the event\n    \"\"\"",
        "code": "def on_binlog(event, stream):\n    \"\"\" Process on a binlog event\n\n    1. Convert event instance into a dict\n    2. Send corresponding schema/table/signals\n\n    Args:\n        event (pymysqlreplication.row_event.RowsEvent): the event\n    \"\"\"\n    rows, meta = _rows_event_to_dict(event, stream)\n\n    table_name = '%s.%s' % (meta['schema'], meta['table'])\n\n    if meta['action'] == 'insert':\n        sig = signals.rows_inserted\n    elif meta['action'] == 'update':\n        sig = signals.rows_updated\n    elif meta['action'] == 'delete':\n        sig = signals.rows_deleted\n    else:\n        raise RuntimeError('Invalid action \"%s\"' % meta['action'])\n\n    sig.send(table_name, rows=rows, meta=meta)",
        "writer": "human",
        "rewrite": [
            "def on_binlog(event, stream):\n    rows, meta = _rows_event_to_dict(event, stream)\n    table_name = f\"{meta['schema']}.{meta['table']}\"\n    sig = {\n        'insert': signals.rows_inserted,\n        'update': signals.rows_updated,\n        'delete': signals.rows_deleted\n    }.get(meta['action'])\n    if not sig:\n        raise RuntimeError(f'Invalid action \"{meta[\"action\"]}\"')\n    sig.send(table_name, rows=rows, meta=meta)",
            "def on_binlog(event, stream):\n    rows, meta = _rows_event_to_dict(event, stream)\n    table_name = f\"{meta['schema']}.{meta['table']}\"\n    actions = {\n        'insert': signals.rows_inserted,\n        'update': signals.rows_updated,\n        'delete': signals.rows_deleted\n    }\n    sig = actions.get(meta['action'])\n    if not sig:\n        raise RuntimeError(f'Invalid action: {meta[\"action\"]}')\n    sig.send(table_name, rows=rows, meta=meta)",
            "def on_binlog(event, stream):\n    rows, meta = _rows_event_to_dict(event, stream)\n    table_name = f\"{meta['schema']}.{meta['table']}\"\n    try:\n        sig = getattr(signals, f\"rows_{meta['action']}\")\n        sig.send(table_name, rows=rows, meta=meta)\n    except AttributeError:\n        raise RuntimeError(f\"Invalid action: {meta['action']}\")"
        ]
    },
    {
        "id": 37,
        "prompt": "def save(self, filething=None, padding=None):\n        \"\"\"save(filething=None, padding=None)\n\n        Save a tag to a file.\n\n        If no filename is given, the one most recently loaded is used.\n\n        Args:\n            filething (filething)\n            padding (:obj:`mutagen.PaddingFunction`)\n        Raises:\n            mutagen.MutagenError\n        \"\"\"",
        "code": "def save(self, filething=None, padding=None):\n        \"\"\"save(filething=None, padding=None)\n\n        Save a tag to a file.\n\n        If no filename is given, the one most recently loaded is used.\n\n        Args:\n            filething (filething)\n            padding (:obj:`mutagen.PaddingFunction`)\n        Raises:\n            mutagen.MutagenError\n        \"\"\"\n\n        try:\n            self.tags._inject(filething.fileobj, padding)\n        except (IOError, error) as e:\n            reraise(self._Error, e, sys.exc_info()[2])\n        except EOFError:\n            raise self._Error(\"no appropriate stream found\")",
        "writer": "human",
        "rewrite": [
            "def save(self, filething=None, padding=None):\n    try:\n        self.tags._inject(filething.fileobj, padding)\n    except (IOError, error) as e:\n        raise self._Error from e\n    except EOFError:\n        raise self._Error(\"no appropriate stream found\")",
            "def save(self, filething=None, padding=None):\n    try:\n        self.tags._inject(filething.fileobj, padding)\n    except (IOError, Exception) as e:\n        raise self._Error from e\n    except EOFError:\n        raise self._Error(\"no appropriate stream found\")",
            "def save(self, filething=None, padding=None):\n    try:\n        self.tags._inject(filething.fileobj, padding)\n    except OSError as e:\n        raise self._Error from e\n    except EOFError:\n        raise self._Error(\"no appropriate stream found\")"
        ]
    },
    {
        "id": 40,
        "prompt": "def interpret(self, msg):\n        \"\"\" Load input \"\"\"",
        "code": "def interpret(self, msg):\n        \"\"\" Load input \"\"\"\n        self.msg = msg",
        "writer": "machine",
        "rewrite": [
            "def interpret(self, msg):\n    self.msg = msg",
            "def interpret(self, msg):\n    self._msg = msg",
            "def set_message(self, msg):\n    self.msg = msg"
        ]
    },
    {
        "id": 37,
        "prompt": "def save(self, filething=None, padding=None):\n        \"\"\"save(filething=None, padding=None)\n\n        Save a tag to a file.\n\n        If no filename is given, the one most recently loaded is used.\n\n        Args:\n            filething (filething)\n            padding (:obj:`mutagen.PaddingFunction`)\n        Raises:\n            mutagen.MutagenError\n        \"\"\"",
        "code": "def save(self, filething=None, padding=None):\n        \"\"\"save(filething=None, padding=None)\n\n        Save a tag to a file.\n\n        If no filename is given, the one most recently loaded is used.\n\n        Args:\n            filething (filething)\n            padding (:obj:`mutagen.PaddingFunction`)\n        Raises:\n            mutagen.MutagenError\n        \"\"\"\n        if filething is None:\n            filething = self._filename\n        if filething is None:\n            raise MutagenError('No filename was specified and none was loaded.')\n        if padding is None:\n            padding = self.padding\n        self.write(filething, padding)",
        "writer": "machine",
        "rewrite": [
            "def save(self, filething=None, padding=None):\n    filething = filething or self._filename\n    if filething is None:\n        raise MutagenError('No filename was specified and none was loaded.')\n    padding = padding or self.padding\n    self.write(filething, padding)",
            "def save(self, filething=None, padding=None):\n    filething = self._filename if filething is None else filething\n    if filething is None:\n        raise MutagenError('No filename was specified and none was loaded.')\n    padding = self.padding if padding is None else padding\n    self.write(filething, padding)",
            "def save(self, filething=self._filename, padding=None):\n    if filething is None:\n        raise MutagenError('No filename was specified and none was loaded.')\n    padding = self.padding if padding is None else padding\n    self.write(filething, padding)"
        ]
    },
    {
        "id": 40,
        "prompt": "def interpret(self, msg):\n        \"\"\" Load input \"\"\"",
        "code": "def interpret(self, msg):\n        \"\"\" Load input \"\"\"\n        slides = msg.get('slides', [])\n\n        result = []\n        for slide in slides:\n            image = self.layout(slide)\n\n            result.append(image)\n\n        return result",
        "writer": "human",
        "rewrite": [
            "def interpret(self, msg):\n    return [self.layout(slide) for slide in msg.get('slides', [])]",
            "def interpret(self, msg):\n    return [self.layout(slide) for slide in msg.get('slides', [])]",
            "def interpret(self, msg):\n    return [self.layout(s) for s in msg.get('slides', [])]"
        ]
    },
    {
        "id": 36,
        "prompt": "def _evaluate(self, val_iter: data_io.BaseParallelSampleIter, val_metric: mx.metric.EvalMetric):\n        \"\"\"\n        Evaluates the model on the validation data and updates the validation metric(s).\n        \"\"\"",
        "code": "def _evaluate(self, val_iter: data_io.BaseParallelSampleIter, val_metric: mx.metric.EvalMetric):\n        \"\"\"\n        Evaluates the model on the validation data and updates the validation metric(s).\n        \"\"\"\n        val_metric.reset()\n        for batch in val_iter:\n            data = gluon.utils.split_and_load(batch.data[0], ctx_list=self.ctx, batch_axis=0, even_split=False)\n            label = gluon.utils.split_and_load(batch.label[0], ctx_list=self.ctx, batch_axis=0, even_split=False)\n            outputs = [self(X) for X in data]\n            val_metric.update(label, outputs)\n        return val_metric.get()",
        "writer": "machine",
        "rewrite": [
            "def _evaluate(self, val_iter, val_metric):\n    val_metric.reset()\n    for batch in val_iter:\n        data = gluon.utils.split_and_load(batch.data[0], ctx_list=self.ctx, batch_axis=0, even_split=False)\n        label = gluon.utils.split_and_load(batch.label[0], ctx_list=self.ctx, batch_axis=0, even_split=False)\n        outputs = [self(X) for X in data]\n        val_metric.update(label, outputs)\n    return val_metric.get()",
            "def _evaluate(self, val_iter, val_metric):\n    val_metric.reset()\n    for batch in val_iter:\n        data = gluon.utils.split_and_load(batch.data[0], ctx_list=self.ctx, batch_axis=0, even_split=False)\n        label = gluon.utils.split_and_load(batch.label[0], ctx_list=self.ctx, batch_axis=0, even_split=False)\n        outputs = [self(d) for d in data]\n        val_metric.update(label, outputs)\n    return val_metric.get()",
            "def _evaluate(self, val_iter, val_metric):\n    val_metric.reset()\n    for batch in val_iter:\n        data, label = batch.data[0], batch.label[0]\n        data = gluon.utils.split_and_load(data, self.ctx, 0, False)\n        label = gluon.utils.split_and_load(label, self.ctx, 0, False)\n        outputs = [self(x) for x in data]\n        val_metric.update(label, outputs)\n    return val_metric.get()"
        ]
    },
    {
        "id": 43,
        "prompt": "def getAnalysisRequests(self):\n        \"\"\" Return all the Analysis Request brains linked to the Sampling Round\n        \"\"\"",
        "code": "def getAnalysisRequests(self):\n        \"\"\" Return all the Analysis Request brains linked to the Sampling Round\n        \"\"\"\n        return [brain for brain in self.getBrains() if brain.is_analysis_request]",
        "writer": "machine",
        "rewrite": [
            "def getAnalysisRequests(self):\n    return [brain for brain in self.getBrains() if hasattr(brain, 'is_analysis_request') and brain.is_analysis_request]",
            "def getAnalysisRequests(self):\n    analysis_requests = []\n    for brain in self.getBrains():\n        if brain.is_analysis_request:\n            analysis_requests.append(brain)\n    return analysis_requests",
            "def getAnalysisRequests(self):\n    return filter(lambda brain: brain.is_analysis_request, self.getBrains())"
        ]
    },
    {
        "id": 43,
        "prompt": "def getAnalysisRequests(self):\n        \"\"\" Return all the Analysis Request brains linked to the Sampling Round\n        \"\"\"",
        "code": "def getAnalysisRequests(self):\n        \"\"\" Return all the Analysis Request brains linked to the Sampling Round\n        \"\"\"\n        # I have to get the catalog in this way because I can't do it with 'self'...\n        pc = getToolByName(api.portal.get(), 'portal_catalog')\n        contentFilter = {'portal_type': 'AnalysisRequest',\n                         'is_active': True,\n                         'SamplingRoundUID': self.UID()}\n        return pc(contentFilter)",
        "writer": "human",
        "rewrite": [
            "def getAnalysisRequests(self):\n    pc = getToolByName(api.portal.get(), 'portal_catalog')\n    return pc(portal_type='AnalysisRequest', is_active=True, SamplingRoundUID=self.UID())",
            "def getAnalysisRequests(self):\n    pc = getToolByName(api.portal.get(), 'portal_catalog')\n    return pc.searchResults(portal_type='AnalysisRequest', is_active=True, SamplingRoundUID=self.UID())",
            "def getAnalysisRequests(self):\n    catalog = getToolByName(api.portal.get(), 'portal_catalog')\n    return catalog.queryCatalog(portal_type='AnalysisRequest', is_active=True, SamplingRoundUID=self.UID())"
        ]
    },
    {
        "id": 46,
        "prompt": "def describe_edge(self, edge):\n        \"\"\"\n        return edge, edge data, head, tail for edge\n        \"\"\"",
        "code": "def describe_edge(self, edge):\n        \"\"\"\n        return edge, edge data, head, tail for edge\n        \"\"\"\n        head, tail, data = self.edges[edge]\n        return edge, data, head, tail",
        "writer": "human",
        "rewrite": [
            "def describe_edge(self, edge):\n        return edge, self.edges[edge][1], self.edges[edge][0], self.edges[edge][2]",
            "def describe_edge(self, edge):\n    head, tail, data = self.edges[edge]\n    return edge, data, head, tail",
            "def describe_edge(self, edge):\n    return edge, *self.edges[edge]"
        ]
    },
    {
        "id": 42,
        "prompt": "def convert_wav(org_wav_fn: Path, tgt_wav_fn: Path) -> None:\n    \"\"\" Converts the wav into a 16bit mono 16000Hz wav.\n\n        Args:\n            org_wav_fn: A `Path` to the original wave file\n            tgt_wav_fn: The `Path` to output the processed wave file\n    \"\"\"",
        "code": "from pathlib import Path\nimport soundfile as sf\n\ndef convert_wav(org_wav_fn: Path, tgt_wav_fn: Path) -> None:\n    \"\"\" Converts the wav into a 16bit mono 16000Hz wav.\n\n        Args:\n            org_wav_fn: A `Path` to the original wave file\n            tgt_wav_fn: The `Path` to output the processed wave file\n    \"\"\"\n    data, samplerate = sf.read(org_wav_fn)\n    sf.write(tgt_wav_fn, data, 16000, subtype='PCM_16')",
        "writer": "machine",
        "rewrite": [
            "import soundfile as sf\nfrom pathlib import Path\n\ndef convert_wav(org_wav_fn: Path, tgt_wav_fn: Path) -> None:\n    data, samplerate = sf.read(org_wav_fn)\n    if len(data.shape) > 1:\n        data = data.mean(axis=1)\n    sf.write(tgt_wav_fn, data, 16000, subtype='PCM_16')",
            "import soundfile as sf\nfrom pathlib import Path\n\ndef convert_wav(org_wav_fn: Path, tgt_wav_fn: Path):\n    data, samplerate = sf.read(org_wav_fn)\n    if len(data.shape) > 1:\n        data = data.mean(axis=1)\n    sf.write(tgt_wav_fn, data, 16000, subtype='PCM_16')",
            "import soundfile as sf\nfrom pathlib import Path\n\ndef convert_wav(org_wav_fn: Path, tgt_wav_fn: Path) -> None:\n    try:\n        data, samplerate = sf.read(org_wav_fn)\n        sf.write(tgt_wav_fn, data, 16000, subtype='PCM_16')\n    except Exception as e:\n        print(f\"Error converting {org_wav_fn}: {e}\")"
        ]
    },
    {
        "id": 42,
        "prompt": "def convert_wav(org_wav_fn: Path, tgt_wav_fn: Path) -> None:\n    \"\"\" Converts the wav into a 16bit mono 16000Hz wav.\n\n        Args:\n            org_wav_fn: A `Path` to the original wave file\n            tgt_wav_fn: The `Path` to output the processed wave file\n    \"\"\"",
        "code": "def convert_wav(org_wav_fn: Path, tgt_wav_fn: Path) -> None:\n    \"\"\" Converts the wav into a 16bit mono 16000Hz wav.\n\n        Args:\n            org_wav_fn: A `Path` to the original wave file\n            tgt_wav_fn: The `Path` to output the processed wave file\n    \"\"\"\n    if not org_wav_fn.exists():\n        raise FileNotFoundError\n    args = [config.FFMPEG_PATH,\n            \"-i\", str(org_wav_fn), \"-ac\", \"1\", \"-ar\", \"16000\", str(tgt_wav_fn)]\n    subprocess.run(args)",
        "writer": "human",
        "rewrite": [
            "import subprocess\nfrom pathlib import Path\n\ndef convert_wav(org_wav_fn: Path, tgt_wav_fn: Path) -> None:\n    if not org_wav_fn.is_file():\n        raise FileNotFoundError\n    subprocess.run([\n        \"ffmpeg\",\n        \"-y\",\n        \"-i\", str(org_wav_fn),\n        \"-acodec\", \"pcm_s16le\",\n        \"-ac\", \"1\",\n        \"-ar\", \"16000\",\n        str(tgt_wav_fn)\n    ], check=True)",
            "import subprocess\nfrom pathlib import Path\n\ndef convert_wav(org_wav_fn: Path, tgt_wav_fn: Path) -> None:\n    if not org_wav_fn.is_file():\n        raise FileNotFoundError\n    try:\n        subprocess.run([\"ffmpeg\", \"-y\", \"-i\", str(org_wav_fn), \"-ac\", \"1\", \"-ar\", \"16000\", \"-acodec\", \"pcm_s16le\", str(tgt_wav_fn)], check=True, stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError as e:\n        print(f\"FFmpeg error: {e.stderr.decode()}\")\n        raise",
            "import subprocess\nfrom pathlib import Path\n\ndef convert_wav(org_wav_fn: Path, tgt_wav_fn: Path) -> None:\n    if not org_wav_fn.exists():\n        raise FileNotFoundError\n    subprocess.run(['ffmpeg', '-i', str(org_wav_fn), '-ac', '1', '-ar', '16000', '-acodec', 'pcm_s16le', str(tgt_wav_fn)], check=True, stderr=subprocess.PIPE)"
        ]
    },
    {
        "id": 44,
        "prompt": "def _load_aux_image(self, image, auxfile):\n        \"\"\"\n        Load a fits file (bkg/rms/curve) and make sure that\n        it is the same shape as the main image.\n\n        Parameters\n        ----------\n        image : :class:`AegeanTools.fits_image.FitsImage`\n            The main image that has already been loaded.\n\n        auxfile : str or HDUList\n            The auxiliary file to be loaded.\n\n        Returns\n        -------\n        aux : :class:`AegeanTools.fits_image.FitsImage`\n            The loaded image.\n        \"\"\"",
        "code": "def _load_aux_image(self, image, auxfile):\n        \"\"\"\n        Load a fits file (bkg/rms/curve) and make sure that\n        it is the same shape as the main image.\n\n        Parameters\n        ----------\n        image : :class:`AegeanTools.fits_image.FitsImage`\n            The main image that has already been loaded.\n\n        auxfile : str or HDUList\n            The auxiliary file to be loaded.\n\n        Returns\n        -------\n        aux : :class:`AegeanTools.fits_image.FitsImage`\n            The loaded image.\n        \"\"\"\n        from astropy.io import fits\n        if isinstance(auxfile, str):\n            aux = fits.open(auxfile)\n        else:\n            aux = auxfile\n        if aux[0].shape != image.data.shape:\n            raise ValueError(\"Auxiliary image shape ({}) does not match main image shape ({})\".format(aux[0].shape, image.data.shape))\n        return self.FitsImage(aux[0].data, aux[0].header)",
        "writer": "machine",
        "rewrite": [
            "def _load_aux_image(self, image, auxfile):\n    from astropy.io import fits\n    aux = fits.open(auxfile) if isinstance(auxfile, str) else auxfile\n    if aux[0].shape != image.data.shape:\n        raise ValueError(f\"Auxiliary image shape ({aux[0].shape}) does not match main image shape ({image.data.shape})\")\n    return self.FitsImage(aux[0].data, aux[0].header)",
            "def _load_aux_image(self, image, auxfile):\n    from astropy.io import fits\n    aux = fits.open(auxfile) if isinstance(auxfile, str) else auxfile\n    if aux[0].shape != image.data.shape:\n        raise ValueError(f\"Auxiliary image shape ({aux[0].shape}) does not match main image shape ({image.data.shape})\")\n    return self.FitsImage(aux[0].data, aux[0].header)",
            "def _load_aux_image(self, image, auxfile):\n    from astropy.io import fits\n    try:\n        aux = fits.open(auxfile) if isinstance(auxfile, str) else auxfile\n        if aux[0].shape != image.data.shape:\n            raise ValueError(f\"Auxiliary image shape {aux[0].shape} != main image shape {image.data.shape}\")\n        return self.FitsImage(aux[0].data, aux[0].header)\n    except (FileNotFoundError, OSError, ValueError) as e:\n        raise type(e)(f\"Error loading auxiliary image: {e}\") from None"
        ]
    },
    {
        "id": 44,
        "prompt": "def _load_aux_image(self, image, auxfile):\n        \"\"\"\n        Load a fits file (bkg/rms/curve) and make sure that\n        it is the same shape as the main image.\n\n        Parameters\n        ----------\n        image : :class:`AegeanTools.fits_image.FitsImage`\n            The main image that has already been loaded.\n\n        auxfile : str or HDUList\n            The auxiliary file to be loaded.\n\n        Returns\n        -------\n        aux : :class:`AegeanTools.fits_image.FitsImage`\n            The loaded image.\n        \"\"\"",
        "code": "def _load_aux_image(self, image, auxfile):\n        \"\"\"\n        Load a fits file (bkg/rms/curve) and make sure that\n        it is the same shape as the main image.\n\n        Parameters\n        ----------\n        image : :class:`AegeanTools.fits_image.FitsImage`\n            The main image that has already been loaded.\n\n        auxfile : str or HDUList\n            The auxiliary file to be loaded.\n\n        Returns\n        -------\n        aux : :class:`AegeanTools.fits_image.FitsImage`\n            The loaded image.\n        \"\"\"\n        auximg = FitsImage(auxfile, beam=self.global_data.beam).get_pixels()\n        if auximg.shape != image.get_pixels().shape:\n            self.log.error(\"file {0} is not the same size as the image map\".format(auxfile))\n            self.log.error(\"{0}= {1}, image = {2}\".format(auxfile, auximg.shape, image.get_pixels().shape))\n            sys.exit(1)\n        return auximg",
        "writer": "human",
        "rewrite": [
            "def _load_aux_image(self, image, auxfile):\n    auximg = FitsImage(auxfile, beam=self.global_data.beam).get_pixels()\n    if auximg.shape != image.get_pixels().shape:\n        raise ValueError(f\"file {auxfile} is not the same size as the image map. {auxfile} shape: {auximg.shape}, image shape: {image.get_pixels().shape}\")\n    return auximg",
            "def _load_aux_image(self, image, auxfile):\n    try:\n        auximg = FitsImage(auxfile, beam=self.global_data.beam).get_pixels()\n        if auximg.shape != image.get_pixels().shape:\n            raise ValueError(f\"Auxiliary file {auxfile} shape mismatch: {auximg.shape} != {image.get_pixels().shape}\")\n        return auximg\n    except Exception as e:\n        self.log.exception(f\"Error loading auxiliary image {auxfile}: {e}\")\n        raise",
            "def _load_aux_image(self, image, auxfile):\n    auximg = FitsImage(auxfile, beam=self.global_data.beam).get_pixels()\n    if auximg.shape != image.get_pixels().shape:\n        raise ValueError(f\"Shape mismatch: {auxfile} shape {auximg.shape} != image shape {image.get_pixels().shape}\")\n    return auximg"
        ]
    },
    {
        "id": 49,
        "prompt": "def new(cls, nsptagname, val):\n        \"\"\"\n        Return a new ``CT_DecimalNumber`` element having tagname *nsptagname*\n        and ``val`` attribute set to *val*.\n        \"\"\"",
        "code": "def new(cls, nsptagname, val):\n        \"\"\"\n        Return a new ``CT_DecimalNumber`` element having tagname *nsptagname*\n        and ``val`` attribute set to *val*.\n        \"\"\"\n        return cls(nsptagname, val=val)",
        "writer": "machine",
        "rewrite": [
            "def new(cls, nsptagname, val):\n    return cls(nsptagname, val=val)",
            "def new(cls, nsptagname, val):\n    return cls(nsptagname, val=val)",
            "def new(cls, nsptagname, val):\n    return cls(nsptagname, val=val)"
        ]
    },
    {
        "id": 49,
        "prompt": "def new(cls, nsptagname, val):\n        \"\"\"\n        Return a new ``CT_DecimalNumber`` element having tagname *nsptagname*\n        and ``val`` attribute set to *val*.\n        \"\"\"",
        "code": "def new(cls, nsptagname, val):\n        \"\"\"\n        Return a new ``CT_DecimalNumber`` element having tagname *nsptagname*\n        and ``val`` attribute set to *val*.\n        \"\"\"\n        return OxmlElement(nsptagname, attrs={qn('w:val'): str(val)})",
        "writer": "human",
        "rewrite": [
            "def new(cls, nsptagname, val):\n    return OxmlElement(nsptagname, {qn('w:val'): str(val)})",
            "def new(cls, nsptagname, val):\n    return OxmlElement(nsptagname, {qn('w:val'): str(val)})",
            "def new(cls, nsptagname, val):\n    return OxmlElement(nsptagname, **{qn('w:val'): str(val)})"
        ]
    },
    {
        "id": 46,
        "prompt": "def describe_edge(self, edge):\n        \"\"\"\n        return edge, edge data, head, tail for edge\n        \"\"\"",
        "code": "def describe_edge(self, edge):\n        \"\"\"\n        return edge, edge data, head, tail for edge\n        \"\"\"\n        return edge, self.edge_data[edge], self.get_head(edge), self.get_tail(edge)",
        "writer": "machine",
        "rewrite": [
            "def describe_edge(self, edge):\n        return edge, self.edge_data.get(edge), self.get_head(edge), self.get_tail(edge)",
            "def describe_edge(self, edge):\n    try:\n        return edge, self.edge_data[edge], self.get_head(edge), self.get_tail(edge)\n    except KeyError:\n        return edge, None, None, None",
            "def describe_edge(self, edge):\n    head, tail = self.get_head(edge), self.get_tail(edge)\n    return edge, self.edge_data.get(edge, None), head, tail"
        ]
    },
    {
        "id": 33,
        "prompt": "def _to_edit(self, infoid):\n        \"\"\"\n        render the HTML page for post editing.\n        \"\"\"",
        "code": "def _to_edit(self, infoid):\n        \"\"\"\n        render the HTML page for post editing.\n        \"\"\"\n\n        postinfo = MPost.get_by_uid(infoid)\n\n        if postinfo:\n            pass\n        else:\n            return self.show404()\n\n        if 'def_cat_uid' in postinfo.extinfo:\n            catid = postinfo.extinfo['def_cat_uid']\n        elif 'gcat0' in postinfo.extinfo:\n            catid = postinfo.extinfo['gcat0']\n        else:\n            catid = ''\n\n        if len(catid) == 4:\n            pass\n        else:\n            catid = ''\n\n        catinfo = None\n        p_catinfo = None\n\n        post2catinfo = MPost2Catalog.get_first_category(postinfo.uid)\n        if post2catinfo:\n            catid = post2catinfo.tag_id\n            catinfo = MCategory.get_by_uid(catid)\n            if catinfo:\n                p_catinfo = MCategory.get_by_uid(catinfo.pid)\n\n        kwd = {\n            'gcat0': catid,\n            'parentname': '',\n            'catname': '',\n            'parentlist': MCategory.get_parent_list(),\n            'userip': self.request.remote_ip,\n            'extinfo': json.dumps(postinfo.extinfo, indent=2, ensure_ascii=False),\n        }\n\n        if self.filter_view:\n            tmpl = 'autogen/edit/edit_{0}.html'.format(catid)\n        else:\n            tmpl = 'post_{0}/post_edit.html'.format(self.kind)\n\n        logger.info('Meta template: {0}'.format(tmpl))\n\n        self.render(\n            tmpl,\n            kwd=kwd,\n            postinfo=postinfo,\n            catinfo=catinfo,\n            pcatinfo=p_catinfo,\n            userinfo=self.userinfo,\n            cat_enum=MCategory.get_qian2(catid[:2]),\n            tag_infos=MCategory.query_all(by_order=True, kind=self.kind),\n            tag_infos2=MCategory.query_all(by_order=True, kind=self.kind),\n            app2tag_info=MPost2Catalog.query_by_entity_uid(infoid, kind=self.kind).objects(),\n            app2label_info=MPost2Label.get_by_uid(infoid).objects()\n        )",
        "writer": "human",
        "rewrite": [
            "def _to_edit(self, infoid):\n    postinfo = MPost.get_by_uid(infoid)\n    if not postinfo:\n        return self.show404()\n\n    catid = postinfo.extinfo.get('def_cat_uid') or postinfo.extinfo.get('gcat0', '')\n    if len(catid) != 4:\n        catid = ''\n\n    post2catinfo = MPost2Catalog.get_first_category(postinfo.uid)\n    if post2catinfo:\n        catid = post2catinfo.tag_id\n        catinfo = MCategory.get_by_uid(catid)\n        p_catinfo = MCategory.get_by_uid(catinfo.pid) if catinfo else None\n    else:\n        catinfo = None\n        p_catinfo = None\n\n    kwd = {\n        'gcat0': catid,\n        'parentname': '',\n        'catname': '',\n        'parentlist': MCategory.get_parent_list(),\n        'userip': self.request.remote_ip,\n        'extinfo': json.dumps(postinfo.extinfo, indent=2, ensure_ascii=False),\n    }\n\n    tmpl = 'autogen/edit/edit_{0}.html'.format(catid) if self.filter_view else 'post_{0}/post_edit.html'.format(self.kind)\n    logger.info('Meta template: {0}'.format(tmpl))\n\n    self.render(\n        tmpl,\n        kwd=kwd,\n        postinfo=postinfo,\n        catinfo=catinfo,\n        pcatinfo=p_catinfo,\n        userinfo=self.userinfo,\n        cat_enum=MCategory.get_qian2(catid[:2]),\n        tag_infos=MCategory.query_all(by_order=True, kind=self.kind),\n        tag_infos2=MCategory.query_all(by_order=True, kind=self.kind),\n        app2tag_info=MPost2Catalog.query_by_entity_uid(infoid, kind=self.kind).objects(),\n        app2label_info=MPost2Label.get_by_uid(infoid).objects()\n    )",
            "def _to_edit(self, infoid):\n    postinfo = MPost.get_by_uid(infoid)\n    if not postinfo:\n        return self.show404()\n\n    catid = postinfo.extinfo.get('def_cat_uid') or postinfo.extinfo.get('gcat0', '')\n    catid = catid if len(catid) == 4 else ''\n\n    post2catinfo = MPost2Catalog.get_first_category(postinfo.uid)\n    if post2catinfo:\n        catid = post2catinfo.tag_id\n        catinfo = MCategory.get_by_uid(catid)\n        p_catinfo = MCategory.get_by_uid(catinfo.pid) if catinfo else None\n    else:\n        catinfo = p_catinfo = None\n\n    kwd = {\n        'gcat0': catid,\n        'parentname': '',\n        'catname': '',\n        'parentlist': MCategory.get_parent_list(),\n        'userip': self.request.remote_ip,\n        'extinfo': json.dumps(postinfo.extinfo, indent=2, ensure_ascii=False),\n    }\n\n    tmpl = 'autogen/edit/edit_{}.html'.format(catid) if self.filter_view else 'post_{}/post_edit.html'.format(self.kind)\n    logger.info(f'Meta template: {tmpl}')\n\n    self.render(tmpl, kwd=kwd, postinfo=postinfo, catinfo=catinfo, pcatinfo=p_catinfo, userinfo=self.userinfo,\n                cat_enum=MCategory.get_qian2(catid[:2]), tag_infos=MCategory.query_all(by_order=True, kind=self.kind),\n                tag_infos2=MCategory.query_all(by_order=True, kind=self.kind),\n                app2tag_info=MPost2Catalog.query_by_entity_uid(infoid, kind=self.kind).objects(),\n                app2label_info=MPost2Label.get_by_uid(infoid).objects())",
            "def _to_edit(self, infoid):\n    postinfo = MPost.get_by_uid(infoid)\n    if not postinfo:\n        return self.show404()\n\n    catid = postinfo.extinfo.get('def_cat_uid') or postinfo.extinfo.get('gcat0', '')\n    catid = catid if len(catid) == 4 else \"\"\n\n    post2catinfo = MPost2Catalog.get_first_category(postinfo.uid)\n    if post2catinfo:\n        catid = post2catinfo.tag_id\n        catinfo = MCategory.get_by_uid(catid)\n        p_catinfo = MCategory.get_by_uid(catinfo.pid) if catinfo else None\n    else:\n        catinfo = p_catinfo = None\n\n    kwd = {\n        'gcat0': catid,\n        'parentname': '',\n        'catname': '',\n        'parentlist': MCategory.get_parent_list(),\n        'userip': self.request.remote_ip,\n        'extinfo': json.dumps(postinfo.extinfo, indent=2, ensure_ascii=False),\n    }\n\n    tmpl = 'autogen/edit/edit_{}.html'.format(catid) if self.filter_view else 'post_{}/post_edit.html'.format(self.kind)\n    logger.info(f'Meta template: {tmpl}')\n\n    self.render(tmpl, kwd=kwd, postinfo=postinfo, catinfo=catinfo, pcatinfo=p_catinfo, userinfo=self.userinfo,\n                cat_enum=MCategory.get_qian2(catid[:2]), tag_infos=MCategory.query_all(by_order=True, kind=self.kind),\n                tag_infos2=MCategory.query_all(by_order=True, kind=self.kind),\n                app2tag_info=MPost2Catalog.query_by_entity_uid(infoid, kind=self.kind).objects(),\n                app2label_info=MPost2Label.get_by_uid(infoid).objects())"
        ]
    },
    {
        "id": 51,
        "prompt": "def create_api_equipment(self):\n        \"\"\"Get an instance of Api Equipment services facade.\"\"\"",
        "code": "def create_api_equipment(self):\n        \"\"\"Get an instance of Api Equipment services facade.\"\"\"\n        return self.get_service(\"equipment\")",
        "writer": "machine",
        "rewrite": [
            "def get_equipment_api(self):\n    return self.get_service(\"equipment\")",
            "def get_equipment_service(self):\n    return self.get_service(\"equipment\")",
            "def equipment_api(self):\n    return self.get_service('equipment')"
        ]
    },
    {
        "id": 51,
        "prompt": "def create_api_equipment(self):\n        \"\"\"Get an instance of Api Equipment services facade.\"\"\"",
        "code": "def create_api_equipment(self):\n        \"\"\"Get an instance of Api Equipment services facade.\"\"\"\n        return ApiEquipment(\n            self.networkapi_url,\n            self.user,\n            self.password,\n            self.user_ldap)",
        "writer": "human",
        "rewrite": [
            "def create_api_equipment(networkapi_url, user, password, user_ldap):\n    return ApiEquipment(networkapi_url, user, password, user_ldap)",
            "def create_api_equipment(networkapi_url, user, password, user_ldap):\n    return ApiEquipment(networkapi_url, user, password, user_ldap)",
            "def create_api_equipment(self):\n    return ApiEquipment(*self.get_credentials())"
        ]
    },
    {
        "id": 52,
        "prompt": "def clean_series_name(seriesname):\n    \"\"\"Cleans up series name.\n\n    By removing any . and _ characters, along with any trailing hyphens.\n\n    Is basically equivalent to replacing all _ and . with a\n    space, but handles decimal numbers in string, for example:\n\n    >>> _clean_series_name(\"an.example.1.0.test\")\n    'an example 1.0 test'\n    >>> _clean_series_name(\"an_example_1.0_test\")\n    'an example 1.0 test'\n    \"\"\"",
        "code": "def clean_series_name(seriesname):\n    \"\"\"Cleans up series name.\n\n    By removing any . and _ characters, along with any trailing hyphens.\n\n    Is basically equivalent to replacing all _ and . with a\n    space, but handles decimal numbers in string, for example:\n\n    >>> _clean_series_name(\"an.example.1.0.test\")\n    'an example 1.0 test'\n    >>> _clean_series_name(\"an_example_1.0_test\")\n    'an example 1.0 test'\n    \"\"\"\n    seriesname = seriesname.replace('.', ' ').replace('_', ' ')\n    return seriesname.strip('-')",
        "writer": "machine",
        "rewrite": [
            "def clean_series_name(seriesname):\n    seriesname = seriesname.replace('.', ' ').replace('_', ' ')\n    return seriesname.rstrip('-')",
            "import re\n\ndef clean_series_name(seriesname):\n  return re.sub(r'[._]', ' ', seriesname).rstrip('-')",
            "def clean_series_name(seriesname):\n    seriesname = seriesname.translate(str.maketrans({'.': ' ', '_': ' '}))\n    return seriesname.rstrip('-')"
        ]
    },
    {
        "id": 53,
        "prompt": "def egress(self, envelope, http_headers, operation, binding_options):\n    \"\"\"Overrides the egress function ror request logging.\n\n    Args:\n      envelope: An Element with the SOAP request data.\n      http_headers: A dict of the current http headers.\n      operation: The SoapOperation instance.\n      binding_options: An options dict for the SOAP binding.\n\n    Returns:\n      A tuple of the envelope and headers.\n    \"\"\"",
        "code": "def egress(self, envelope, http_headers, operation, binding_options):\n    \"\"\"Overrides the egress function ror request logging.\n\n    Args:\n      envelope: An Element with the SOAP request data.\n      http_headers: A dict of the current http headers.\n      operation: The SoapOperation instance.\n      binding_options: An options dict for the SOAP binding.\n\n    Returns:\n      A tuple of the envelope and headers.\n    \"\"\"\n    # Log the request here\n    # ...\n    return envelope, http_headers",
        "writer": "machine",
        "rewrite": [
            "def egress(self, envelope, http_headers, operation, binding_options):\n    return envelope, http_headers",
            "import logging\n\ndef egress(self, envelope, http_headers, operation, binding_options):\n    logging.info(f\"SOAP Request:\\nEnvelope: {envelope}\\nHeaders: {http_headers}\\nOperation: {operation}\\nBinding Options: {binding_options}\")\n    return envelope, http_headers",
            "import json\n\ndef egress(self, envelope, http_headers, operation, binding_options):\n    log_data = {\n        'envelope': str(envelope),\n        'http_headers': http_headers,\n        'operation': str(operation),\n        'binding_options': binding_options\n    }\n    print(json.dumps(log_data, indent=2))\n    return envelope, http_headers"
        ]
    },
    {
        "id": 54,
        "prompt": "def list_nodes_min(call=None):\n    \"\"\"\n    Return a list of the instances that are on the provider. Only a list of\n    instances names, and their state, is returned.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_nodes_min my-qingcloud\n    \"\"\"",
        "code": "def list_nodes_min(call=None):\n    \"\"\"\n    Return a list of the instances that are on the provider. Only a list of\n    instances names, and their state, is returned.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_nodes_min my-qingcloud\n    \"\"\"\n    ret = {}\n    if call == 'action':\n        return ret\n    nodes = __salt__['cloud.list_nodes'](call)\n    for node in nodes:\n        ret[node] = nodes[node]['state']\n    return ret",
        "writer": "machine",
        "rewrite": [
            "def list_nodes_min(call=None):\n    ret = {}\n    if call == 'action':\n        return ret\n    nodes = __salt__['cloud.list_nodes'](call)\n    ret = {node: nodes[node]['state'] for node in nodes}\n    return ret",
            "def list_nodes_min(call=None):\n    if call == 'action':\n        return {}\n    return {node: data['state'] for node, data in __salt__['cloud.list_nodes'](call).items()}",
            "def list_nodes_min(call=None):\n    return {} if call == 'action' else {k: v['state'] for k, v in __salt__['cloud.list_nodes'](call).items()}"
        ]
    },
    {
        "id": 52,
        "prompt": "def clean_series_name(seriesname):\n    \"\"\"Cleans up series name.\n\n    By removing any . and _ characters, along with any trailing hyphens.\n\n    Is basically equivalent to replacing all _ and . with a\n    space, but handles decimal numbers in string, for example:\n\n    >>> _clean_series_name(\"an.example.1.0.test\")\n    'an example 1.0 test'\n    >>> _clean_series_name(\"an_example_1.0_test\")\n    'an example 1.0 test'\n    \"\"\"",
        "code": "def clean_series_name(seriesname):\n    \"\"\"Cleans up series name.\n\n    By removing any . and _ characters, along with any trailing hyphens.\n\n    Is basically equivalent to replacing all _ and . with a\n    space, but handles decimal numbers in string, for example:\n\n    >>> _clean_series_name(\"an.example.1.0.test\")\n    'an example 1.0 test'\n    >>> _clean_series_name(\"an_example_1.0_test\")\n    'an example 1.0 test'\n    \"\"\"\n    if not seriesname:\n        return seriesname\n    # TODO(xxxx): Could this be made to clean \"Hawaii.Five-0.2010\" into\n    # \"Hawaii Five-0 2010\"?\n    seriesname = re.sub(r'(\\D)[.](\\D)', '\\\\1 \\\\2', seriesname)\n    seriesname = re.sub(r'(\\D)[.]', '\\\\1 ', seriesname)\n    seriesname = re.sub(r'[.](\\D)', ' \\\\1', seriesname)\n    seriesname = seriesname.replace('_', ' ')\n    seriesname = re.sub('-$', '', seriesname)\n    return _replace_series_name(seriesname.strip(),\n                                cfg.CONF.input_series_replacements)",
        "writer": "human",
        "rewrite": [
            "def clean_series_name(seriesname):\n    if not seriesname:\n        return seriesname\n    seriesname = seriesname.replace('_', ' ').replace('.', ' ')\n    seriesname = re.sub(r'\\s+$', '', seriesname)\n    return _replace_series_name(seriesname, cfg.CONF.input_series_replacements)",
            "def clean_series_name(seriesname):\n    if not seriesname:\n        return seriesname\n    seriesname = seriesname.replace('_', ' ')\n    seriesname = re.sub(r'\\.(\\d+\\.\\d+)', r' \\1', seriesname)\n    seriesname = re.sub(r'(\\D)\\.(\\D)', r'\\1 \\2', seriesname)\n    seriesname = re.sub(r'[.]', ' ', seriesname)\n    seriesname = seriesname.rstrip('-').strip()\n    return _replace_series_name(seriesname, cfg.CONF.input_series_replacements)",
            "def clean_series_name(seriesname):\n    if not seriesname: return seriesname\n    seriesname = seriesname.replace('_', ' ')\n    seriesname = re.sub(r'\\.(\\d+\\.?\\d*)', r' \\1', seriesname)\n    seriesname = re.sub(r'(\\D)\\.(\\D)', r'\\1 \\2', seriesname)\n    seriesname = re.sub(r'\\.(\\w)',r' \\1', seriesname)\n    seriesname = re.sub(r'^\\s+|\\s+$', '', seriesname)\n    seriesname = re.sub(r'\\s+', ' ', seriesname)\n    seriesname = seriesname.rstrip('-')\n    return _replace_series_name(seriesname, cfg.CONF.input_series_replacements)"
        ]
    },
    {
        "id": 54,
        "prompt": "def list_nodes_min(call=None):\n    \"\"\"\n    Return a list of the instances that are on the provider. Only a list of\n    instances names, and their state, is returned.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_nodes_min my-qingcloud\n    \"\"\"",
        "code": "def list_nodes_min(call=None):\n    \"\"\"\n    Return a list of the instances that are on the provider. Only a list of\n    instances names, and their state, is returned.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_nodes_min my-qingcloud\n    \"\"\"\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The list_nodes_min function must be called with -f or --function.'\n        )\n\n    nodes = list_nodes_full()\n\n    result = {}\n    for instance_id, full_node in nodes.items():\n        result[instance_id] = {\n            'name': full_node['instance_name'],\n            'status': full_node['status'],\n        }\n\n    return result",
        "writer": "human",
        "rewrite": [
            "def list_nodes_min(call=None):\n    if call != 'function':\n        raise SaltCloudSystemExit('The list_nodes_min function must be called with -f or --function.')\n    nodes = list_nodes_full()\n    return {instance_id: {'name': node['instance_name'], 'status': node['status']} for instance_id, node in nodes.items()}",
            "def list_nodes_min(call=None):\n    if call != 'function':\n        raise SaltCloudSystemExit('The list_nodes_min function must be called with -f or --function.')\n    return {instance_id: {'name': node['instance_name'], 'status': node['status']} for instance_id, node in list_nodes_full().items()}",
            "def list_nodes_min(call=None):\n    if call != 'function':\n        raise SaltCloudSystemExit('The list_nodes_min function must be called with -f or --function.')\n    return dict((instance_id, {'name': node['instance_name'], 'status': node['status']}) for instance_id, node in list_nodes_full().items())"
        ]
    },
    {
        "id": 50,
        "prompt": "def to_bytes(self, transparent=True, thumbnail_size=None, resampling=None, in_range='dtype', out_range='dtype',\n                 format=\"png\"):\n        \"\"\"\n        Convert to selected format (discarding geo).\n\n        Optionally also resizes.\n        Note: for color images returns interlaced.\n        :param transparent: if True - sets alpha channel for nodata pixels\n        :param thumbnail_size: if not None - resize to thumbnail size, e.g. 512\n        :param in_range: input intensity range\n        :param out_range: output intensity range\n        :param format : str, image format, default \"png\"\n        :param resampling: one of Resampling enums\n\n        :return bytes\n        \"\"\"",
        "code": "def to_bytes(self, transparent=True, thumbnail_size=None, resampling=None, in_range='dtype', out_range='dtype',\n                 format=\"png\"):\n        \"\"\"\n        Convert to selected format (discarding geo).\n\n        Optionally also resizes.\n        Note: for color images returns interlaced.\n        :param transparent: if True - sets alpha channel for nodata pixels\n        :param thumbnail_size: if not None - resize to thumbnail size, e.g. 512\n        :param in_range: input intensity range\n        :param out_range: output intensity range\n        :param format : str, image format, default \"png\"\n        :param resampling: one of Resampling enums\n\n        :return bytes\n        \"\"\"\n        from PIL import Image\n        from rio_tiler.utils import _get_resampling\n        from rio_tiler.errors import InvalidResampling\n        import numpy as np\n\n        arr = self.read(1, out_shape=thumbnail_size, resampling=_get_resampling(resampling))\n\n        if transparent:\n            mask = self.mask(1)",
        "writer": "machine",
        "rewrite": [
            "def to_bytes(self, transparent=True, thumbnail_size=None, resampling=None, in_range='dtype', out_range='dtype', format=\"png\"):\n    from PIL import Image\n    from rio_tiler.utils import _get_resampling\n    from rio_tiler.errors import InvalidResampling\n    import numpy as np\n\n    arr = self.read(1, out_shape=thumbnail_size, resampling=_get_resampling(resampling))\n\n    if transparent:\n        mask = self.mask(1)\n        if len(arr.shape) == 3:\n            alpha = np.where(mask, 255, 0).astype(np.uint8)\n            arr = np.concatenate((arr, alpha[:, :, None]), axis=2)\n        else:\n            alpha = np.where(mask, 255, 0).astype(np.uint8)\n            arr = np.stack((arr, alpha), axis=-1)\n\n    img = Image.fromarray(arr)\n    with io.BytesIO() as f:\n        img.save(f, format)\n        return f.getvalue()",
            "def to_bytes(self, transparent=True, thumbnail_size=None, resampling=None, in_range='dtype', out_range='dtype', format=\"png\"):\n    import io\n    from PIL import Image\n    from rio_tiler.utils import _get_resampling\n    import numpy as np\n\n    arr = self.read(1, out_shape=thumbnail_size, resampling=_get_resampling(resampling))\n\n    if transparent:\n        mask = self.mask(1)\n        if len(arr.shape) == 2:\n            arr = np.stack([arr, np.where(mask, 255, 0)], axis=-1)\n        else:\n            arr = np.concatenate([arr, np.where(mask[..., None], 255, 0)], axis=-1)\n\n    img = Image.fromarray(arr.astype(np.uint8))\n    with io.BytesIO() as f:\n        img.save(f, format)\n        return f.getvalue()",
            "def to_bytes(self, transparent=True, thumbnail_size=None, resampling=None, in_range='dtype', out_range='dtype', format=\"png\"):\n    import io\n    from PIL import Image\n    from rio_tiler.utils import _get_resampling\n    import numpy as np\n\n    arr = self.read(1, out_shape=thumbnail_size, resampling=_get_resampling(resampling))\n\n    if transparent:\n        mask = self.mask(1)\n        alpha = np.where(mask, 255, 0).astype(np.uint8)\n        if len(arr.shape) == 2:\n            arr = np.stack((arr, alpha), axis=-1)\n        else:\n            arr = np.concatenate((arr, alpha[..., None]), axis=-1)\n\n    img = Image.fromarray(arr)\n    with io.BytesIO() as buffer:\n        img.save(buffer, format)\n        return buffer.getvalue()"
        ]
    },
    {
        "id": 55,
        "prompt": "def init2(\n        self, \n        input_tube,      # Read task from the input tube.\n        output_tubes,    # Send result on all the output tubes.\n        num_workers,     # Total number of workers in the stage.\n        disable_result,  # Whether to override any result with None.\n        do_stop_task,    # Whether to call doTask() on \"stop\" request.\n        ):\n        \"\"\"Create *num_workers* worker objects with *input_tube* and \n        an iterable of *output_tubes*. The worker reads a task from *input_tube* \n        and writes the result to *output_tubes*.\"\"\"",
        "code": "def init2(\n        self, \n        input_tube,      # Read task from the input tube.\n        output_tubes,    # Send result on all the output tubes.\n        num_workers,     # Total number of workers in the stage.\n        disable_result,  # Whether to override any result with None.\n        do_stop_task,    # Whether to call doTask() on \"stop\" request.\n        ):\n        \"\"\"Create *num_workers* worker objects with *input_tube* and \n        an iterable of *output_tubes*. The worker reads a task from *input_tube* \n        and writes the result to *output_tubes*.\"\"\"\n\n        super(UnorderedWorker, self).__init__()\n        self._tube_task_input = input_tube\n        self._tubes_result_output = output_tubes\n        self._num_workers = num_workers\n        self._disable_result = disable_result\n        self._do_stop_task = do_stop_task",
        "writer": "human",
        "rewrite": [
            "def init2(self, input_tube, output_tubes, num_workers, disable_result, do_stop_task):\n    super().__init__()\n    self._tube_task_input = input_tube\n    self._tubes_result_output = output_tubes\n    self._num_workers = num_workers\n    self._disable_result = disable_result\n    self._do_stop_task = do_stop_task",
            "def __init__(self, input_tube, output_tubes, num_workers, disable_result=False, do_stop_task=False):\n    super().__init__()\n    self.input_tube = input_tube\n    self.output_tubes = output_tubes\n    self.num_workers = num_workers\n    self.disable_result = disable_result\n    self.do_stop_task = do_stop_task",
            "def __init__(self, input_tube, output_tubes, num_workers, disable_result=False, do_stop_task=False):\n    super().__init__()\n    self.input = input_tube\n    self.outputs = output_tubes\n    self.num_workers = num_workers\n    self.disable_result = disable_result\n    self.do_stop_task = do_stop_task"
        ]
    },
    {
        "id": 53,
        "prompt": "def egress(self, envelope, http_headers, operation, binding_options):\n    \"\"\"Overrides the egress function ror request logging.\n\n    Args:\n      envelope: An Element with the SOAP request data.\n      http_headers: A dict of the current http headers.\n      operation: The SoapOperation instance.\n      binding_options: An options dict for the SOAP binding.\n\n    Returns:\n      A tuple of the envelope and headers.\n    \"\"\"",
        "code": "def egress(self, envelope, http_headers, operation, binding_options):\n    \"\"\"Overrides the egress function ror request logging.\n\n    Args:\n      envelope: An Element with the SOAP request data.\n      http_headers: A dict of the current http headers.\n      operation: The SoapOperation instance.\n      binding_options: An options dict for the SOAP binding.\n\n    Returns:\n      A tuple of the envelope and headers.\n    \"\"\"\n    if self._logger.isEnabledFor(logging.INFO):\n      service_name = operation.binding.wsdl.services.keys()[0]\n      self._logger.info(_REQUEST_LOG_LINE, service_name, operation.name,\n                        binding_options['address'])\n\n    if self._logger.isEnabledFor(logging.DEBUG):\n      http_headers_safe = http_headers.copy()\n      if self._AUTHORIZATION_HEADER in http_headers_safe:\n        http_headers_safe[self._AUTHORIZATION_HEADER] = self._REDACTED\n\n      request_string = etree.tostring(envelope, pretty_print=True)\n      safe_request = self._DEVELOPER_TOKEN_SUB.sub(\n          self._REDACTED, request_string.decode('utf-8'))\n      self._logger.debug(\n          _REQUEST_XML_LOG_LINE, http_headers_safe, safe_request)\n\n    return envelope, http_headers",
        "writer": "human",
        "rewrite": [
            "def egress(self, envelope, http_headers, operation, binding_options):\n    log = self._logger\n    if log.isEnabledFor(logging.INFO):\n        service_name = next(iter(operation.binding.wsdl.services))\n        log.info(_REQUEST_LOG_LINE, service_name, operation.name, binding_options['address'])\n    if log.isEnabledFor(logging.DEBUG):\n        headers = http_headers.copy()\n        headers[self._AUTHORIZATION_HEADER] = self._REDACTED if self._AUTHORIZATION_HEADER in headers else headers[self._AUTHORIZATION_HEADER]\n        request = etree.tostring(envelope, pretty_print=True).decode('utf-8')\n        safe_request = self._DEVELOPER_TOKEN_SUB.sub(self._REDACTED, request)\n        log.debug(_REQUEST_XML_LOG_LINE, headers, safe_request)\n    return envelope, http_headers",
            "def egress(self, envelope, http_headers, operation, binding_options):\n    logger = self._logger\n    if logger.isEnabledFor(logging.INFO):\n        service_name = list(operation.binding.wsdl.services.keys())[0]\n        logger.info(_REQUEST_LOG_LINE, service_name, operation.name, binding_options['address'])\n    if logger.isEnabledFor(logging.DEBUG):\n        headers = http_headers.copy()\n        headers[self._AUTHORIZATION_HEADER] = self._REDACTED if self._AUTHORIZATION_HEADER in headers else headers.get(self._AUTHORIZATION_HEADER)\n        request = etree.tostring(envelope, pretty_print=True).decode()\n        safe_request = self._DEVELOPER_TOKEN_SUB.sub(self._REDACTED, request)\n        logger.debug(_REQUEST_XML_LOG_LINE, headers, safe_request)\n    return envelope, http_headers",
            "def egress(self, envelope, http_headers, operation, binding_options):\n    log_level = self._logger.getEffectiveLevel()\n    if log_level <= logging.INFO:\n        service_name = next(iter(operation.binding.wsdl.services))\n        self._logger.info(_REQUEST_LOG_LINE, service_name, operation.name, binding_options[\"address\"])\n    if log_level <= logging.DEBUG:\n        headers = http_headers.copy()\n        headers[self._AUTHORIZATION_HEADER] = self._REDACTED\n        xml_request = etree.tostring(envelope, pretty_print=True).decode()\n        safe_xml = self._DEVELOPER_TOKEN_SUB.sub(self._REDACTED, xml_request)\n        self._logger.debug(_REQUEST_XML_LOG_LINE, headers, safe_xml)\n    return envelope, http_headers"
        ]
    },
    {
        "id": 57,
        "prompt": "def L1(layer=\"input\", constant=0, batch=None):\n  \"\"\"L1 norm of layer. Generally used as penalty.\"\"\"",
        "code": "def L1(layer=\"input\", constant=0, batch=None):\n  \"\"\"L1 norm of layer. Generally used as penalty.\"\"\"\n  import tensorflow as tf\n  if batch is None:\n    return tf.reduce_sum(tf.abs(layer)) * constant\n  else:\n    return tf.reduce_sum(tf.abs(layer), axis=batch) * constant",
        "writer": "machine",
        "rewrite": [
            "import tensorflow as tf\n\ndef L1(layer=\"input\", constant=0, batch=None):\n  return tf.reduce_sum(tf.abs(layer), axis=batch) * constant",
            "import tensorflow as tf\n\ndef L1(layer, constant=0, batch=None):\n  return tf.reduce_sum(tf.abs(layer), axis=batch) * constant",
            "import tensorflow as tf\n\ndef l1_norm(tensor, constant=1.0, axis=None):\n  return tf.reduce_sum(tf.abs(tensor), axis=axis) * constant"
        ]
    },
    {
        "id": 55,
        "prompt": "def init2(\n        self, \n        input_tube,      # Read task from the input tube.\n        output_tubes,    # Send result on all the output tubes.\n        num_workers,     # Total number of workers in the stage.\n        disable_result,  # Whether to override any result with None.\n        do_stop_task,    # Whether to call doTask() on \"stop\" request.\n        ):\n        \"\"\"Create *num_workers* worker objects with *input_tube* and \n        an iterable of *output_tubes*. The worker reads a task from *input_tube* \n        and writes the result to *output_tubes*.\"\"\"",
        "code": "def init2(\n        self, \n        input_tube,      # Read task from the input tube.\n        output_tubes,    # Send result on all the output tubes.\n        num_workers,     # Total number of workers in the stage.\n        disable_result,  # Whether to override any result with None.\n        do_stop_task,    # Whether to call doTask() on \"stop\" request.\n        ):\n        \"\"\"Create *num_workers* worker objects with *input_tube* and \n        an iterable of *output_tubes*. The worker reads a task from *input_tube* \n        and writes the result to *output_tubes*.\"\"\"\n        self.workers = [self.Worker(input_tube, output_tubes, disable_result, do_stop_task) for _ in range(num_workers)]",
        "writer": "machine",
        "rewrite": [
            "def init2(self, input_tube, output_tubes, num_workers, disable_result, do_stop_task):\n    self.workers = [self.Worker(input_tube, output_tubes, disable_result, do_stop_task) for _ in range(num_workers)]",
            "def init2(self, input_tube, output_tubes, num_workers, disable_result, do_stop_task):\n    self.workers = [self.Worker(input_tube, output_tubes, disable_result, do_stop_task) for _ in range(num_workers)]",
            "def init2(self, input_tube, output_tubes, num_workers, disable_result, do_stop_task):\n    self.workers = [self.Worker(input_tube, output_tubes, disable_result, do_stop_task) for i in range(num_workers)]"
        ]
    },
    {
        "id": 57,
        "prompt": "def L1(layer=\"input\", constant=0, batch=None):\n  \"\"\"L1 norm of layer. Generally used as penalty.\"\"\"",
        "code": "def L1(layer=\"input\", constant=0, batch=None):\n  \"\"\"L1 norm of layer. Generally used as penalty.\"\"\"\n  if batch is None:\n    return lambda T: tf.reduce_sum(tf.abs(T(layer) - constant))\n  else:\n    return lambda T: tf.reduce_sum(tf.abs(T(layer)[batch] - constant))",
        "writer": "human",
        "rewrite": [
            "def L1(layer=\"input\", constant=0, batch=None):\n  def l1_norm(T):\n    tensor = T(layer)\n    if batch is not None:\n      tensor = tensor[batch]\n    return tf.reduce_sum(tf.abs(tensor - constant))\n  return l1_norm",
            "def L1(layer=\"input\", constant=0, batch=None):\n  return lambda T: tf.reduce_sum(tf.abs(T(layer)[batch] if batch is not None else T(layer) - constant))",
            "import tensorflow as tf\n\ndef L1(layer=\"input\", constant=0, batch=None):\n  def l1_calculator(T):\n    values = T(layer)\n    if batch is not None:\n      values = values[batch]\n    return tf.reduce_sum(tf.abs(values - constant))\n  return l1_calculator"
        ]
    },
    {
        "id": 59,
        "prompt": "def _validate_condition_keys(self, field, value, error):\n        \"\"\"\n        Validates that all of the keys in one of the sets of keys are defined\n        as keys of ``value``.\n        \"\"\"",
        "code": "def _validate_condition_keys(self, field, value, error):\n        \"\"\"\n        Validates that all of the keys in one of the sets of keys are defined\n        as keys of ``value``.\n        \"\"\"\n        if not isinstance(value, dict):\n            return\n        for key in field.keys:\n            if key not in value:\n                error(f\"The condition key '{key}' is not defined \"\n                      f\"in the provided dictionary.\")",
        "writer": "machine",
        "rewrite": [
            "def _validate_condition_keys(self, field, value, error):\n    if isinstance(value, dict):\n        for key in field.keys:\n            if key not in value:\n                error(f\"The condition key '{key}' is not defined in the provided dictionary.\")",
            "def _validate_condition_keys(self, field, value, error):\n    if isinstance(value, dict):\n        missing_keys = set(field.keys) - set(value.keys())\n        for key in missing_keys:\n            error(f\"The condition key '{key}' is not defined in the provided dictionary.\")",
            "def _validate_condition_keys(self, field, value, error):\n    if isinstance(value, dict):\n        for key in field.keys:\n            if key not in value:\n                error(f\"Missing key '{key}' in the provided dictionary.\")"
        ]
    },
    {
        "id": 61,
        "prompt": "def next(self):\n        \"\"\" allow us to iterate over the output of our command \"\"\"",
        "code": "def next(self):\n        \"\"\" allow us to iterate over the output of our command \"\"\"\n        if self._current_index < len(self._output):\n            result = self._output[self._current_index]\n            self._current_index += 1\n            return result\n        else:\n            return None",
        "writer": "machine",
        "rewrite": [
            "def next(self):\n        try:\n            result = self._output[self._current_index]\n            self._current_index += 1\n            return result\n        except IndexError:\n            return None",
            "def next(self):\n    if self._current_index < len(self._output):\n        return self._output[self._current_index + self._current_index.__iadd__(1) -1]\n    return None",
            "def next(self):\n    try:\n        return self._output[self._current_index++]\n    except IndexError:\n        return None"
        ]
    },
    {
        "id": 62,
        "prompt": "def close(self):\n        \"\"\"\n        Close the stream. Assumes stream has 'close' method.\n        \"\"\"",
        "code": "def close(self):\n        \"\"\"\n        Close the stream. Assumes stream has 'close' method.\n        \"\"\"\n        if hasattr(self, 'stream') and hasattr(self.stream, 'close'):\n            self.stream.close()",
        "writer": "machine",
        "rewrite": [
            "def close(self):\n    try:\n        self.stream.close()\n    except AttributeError:\n        pass",
            "def close(self):\n    if self.stream:\n        self.stream.close()",
            "def close(self):\n    try:\n        self.stream.close()\n    except AttributeError:\n        pass\n    except Exception:\n        pass"
        ]
    },
    {
        "id": 50,
        "prompt": "def to_bytes(self, transparent=True, thumbnail_size=None, resampling=None, in_range='dtype', out_range='dtype',\n                 format=\"png\"):\n        \"\"\"\n        Convert to selected format (discarding geo).\n\n        Optionally also resizes.\n        Note: for color images returns interlaced.\n        :param transparent: if True - sets alpha channel for nodata pixels\n        :param thumbnail_size: if not None - resize to thumbnail size, e.g. 512\n        :param in_range: input intensity range\n        :param out_range: output intensity range\n        :param format : str, image format, default \"png\"\n        :param resampling: one of Resampling enums\n\n        :return bytes\n        \"\"\"",
        "code": "def to_bytes(self, transparent=True, thumbnail_size=None, resampling=None, in_range='dtype', out_range='dtype',\n                 format=\"png\"):\n        \"\"\"\n        Convert to selected format (discarding geo).\n\n        Optionally also resizes.\n        Note: for color images returns interlaced.\n        :param transparent: if True - sets alpha channel for nodata pixels\n        :param thumbnail_size: if not None - resize to thumbnail size, e.g. 512\n        :param in_range: input intensity range\n        :param out_range: output intensity range\n        :param format : str, image format, default \"png\"\n        :param resampling: one of Resampling enums\n\n        :return bytes\n        \"\"\"\n        resampling = resampling if resampling is not None else Resampling.cubic\n\n        if self.num_bands < 3:\n            warnings.warn(\"Deprecation: to_png of less then three bands raster will be not be supported in next \\\nrelease, please use: .colorize('gray').to_png()\", GeoRaster2Warning)\n\n        if self.num_bands > 3:\n            warnings.warn(\"Limiting %d bands raster to first three bands to generate png\" % self.num_bands,\n                          GeoRaster2Warning)\n            three_first_bands = self.band_names[:3]\n            raster = self.limit_to_bands(three_first_bands)\n        elif self.num_bands == 2:\n            warnings.warn(\"Limiting two bands raster to use the first band to generate png\",\n                          GeoRaster2Warning)\n            first_band = self.band_names[:1]\n            raster = self.limit_to_bands(first_band)\n        else:\n            raster = self\n\n        if raster.image.dtype != np.uint8:\n            warnings.warn(\"downscaling dtype to 'uint8' to convert to png\",\n                          GeoRaster2Warning)\n            thumbnail = raster.astype(np.uint8, in_range=in_range, out_range=out_range)\n        else:\n            thumbnail = raster.copy_with()\n\n        if thumbnail_size:\n            if thumbnail.width > thumbnail.height:\n                thumbnail = thumbnail.resize(dest_width=thumbnail_size, resampling=resampling)\n            else:\n                thumbnail = thumbnail.resize(dest_height=thumbnail_size, resampling=resampling)\n\n        img, mask = thumbnail.to_pillow_image(return_mask=True)\n\n        if transparent:\n            mask = np.array(mask)[:, :, np.newaxis]\n            mask = 255 - 255 * mask  # inverse\n\n            if thumbnail.num_bands == 1:\n                img = np.stack([img, img, img], axis=2)  # make grayscale into rgb. bypass, as mode=LA isn't supported\n\n            img = np.stack(tuple(np.split(np.asarray(img), 3, axis=2) + [mask]), axis=2)  # re-arrange into RGBA\n            img = img[:, :, :, 0]\n\n        f = io.BytesIO()\n        imageio.imwrite(f, img, format)\n        image_data = f.getvalue()\n        return image_data",
        "writer": "human",
        "rewrite": [
            "import io\nimport imageio\nimport numpy as np\nimport warnings\n\ndef to_bytes(self, transparent=True, thumbnail_size=None, resampling=None, in_range='dtype', out_range='dtype', format=\"png\"):\n    resampling = resampling or Resampling.cubic\n    if self.num_bands < 3:\n        warnings.warn(\"Deprecation: to_png of less then three bands raster will be not be supported in next release, please use: .colorize('gray').to_png()\", GeoRaster2Warning)\n    if self.num_bands > 3:\n        warnings.warn(\"Limiting %d bands raster to first three bands to generate png\" % self.num_bands, GeoRaster2Warning)\n        raster = self.limit_to_bands(self.band_names[:3])\n    elif self.num_bands == 2:\n        warnings.warn(\"Limiting two bands raster to use the first band to generate png\", GeoRaster2Warning)\n        raster = self.limit_to_bands(self.band_names[:1])\n    else:\n        raster = self\n    if raster.image.dtype != np.uint8:\n        warnings.warn(\"downscaling dtype to 'uint8' to convert to png\", GeoRaster2Warning)\n        thumbnail = raster.astype(np.uint8, in_range=in_range, out_range=out_range)\n    else:\n        thumbnail = raster.copy_with()\n    if thumbnail_size:\n        thumbnail = thumbnail.resize(dest_width=thumbnail_size if thumbnail.width > thumbnail.height else None, dest_height=thumbnail_size if thumbnail.height > thumbnail.width else None, resampling=resampling)\n    img, mask = thumbnail.to_pillow_image(return_mask=True)\n    if transparent:\n        mask = 255 - 255 * np.array(mask)[:, :, np.newaxis]\n        img = np.stack([img, img, img, mask], axis=2) if thumbnail.num_bands == 1 else np.concatenate((np.asarray(img), mask), axis=2)\n    f = io.BytesIO()\n    imageio.imwrite(f, img, format)\n    return f.getvalue()",
            "import io\nimport imageio\nimport numpy as np\nimport warnings\n\ndef to_bytes(self, transparent=True, thumbnail_size=None, resampling=None, in_range='dtype', out_range='dtype', format=\"png\"):\n    resampling = resampling or Resampling.cubic\n    if self.num_bands < 3:\n        warnings.warn(\"Deprecation: to_png of less then three bands raster will be not be supported in next release, please use: .colorize('gray').to_png()\", GeoRaster2Warning)\n    raster = self.limit_to_bands(self.band_names[:3]) if self.num_bands > 3 else self.limit_to_bands(self.band_names[:1]) if self.num_bands == 2 else self\n    thumbnail = raster.astype(np.uint8, in_range=in_range, out_range=out_range) if raster.image.dtype != np.uint8 else raster.copy_with()\n    if thumbnail_size:\n        thumbnail = thumbnail.resize(dest_width=thumbnail_size if thumbnail.width > thumbnail.height else None, dest_height=thumbnail_size if thumbnail.height >= thumbnail.width else None, resampling=resampling)\n    img, mask = thumbnail.to_pillow_image(return_mask=True)\n    if transparent:\n        mask = 255 - 255 * np.array(mask)[:, :, np.newaxis]\n        img = np.concatenate((img, mask), axis=2) if img.shape[2] == 3 else np.stack((img,)*3 + [mask], axis=2)\n    f = io.BytesIO()\n    imageio.imwrite(f, img, format)\n    return f.getvalue()",
            "import io\nimport imageio\nimport numpy as np\nimport warnings\n\ndef to_bytes(self, transparent=True, thumbnail_size=None, resampling=None, in_range='dtype', out_range='dtype', format=\"png\"):\n    resampling = resampling or Resampling.cubic\n    if self.num_bands < 3: warnings.warn(\"Deprecation: to_png of less then three bands raster will be not be supported in next release, please use: .colorize('gray').to_png()\", GeoRaster2Warning)\n    raster = self.limit_to_bands(self.band_names[:min(3, self.num_bands)])\n    thumbnail = raster.astype(np.uint8, in_range=in_range, out_range=out_range) if raster.image.dtype != np.uint8 else raster.copy_with()\n    if thumbnail_size: thumbnail = thumbnail.resize(dest_width=thumbnail_size if thumbnail.width > thumbnail.height else None, dest_height=thumbnail_size if thumbnail.height >= thumbnail.width else None, resampling=resampling)\n    img, mask = thumbnail.to_pillow_image(return_mask=True)\n    if transparent:\n        mask = 255 - 255 * np.array(mask)[:,:,None]\n        img = np.concatenate((img, mask), axis=2) if img.shape[2] == 3 else np.stack((img,)*3 + [mask], axis=2)\n    f = io.BytesIO()\n    imageio.imwrite(f, img, format)\n    return f.getvalue()"
        ]
    },
    {
        "id": 62,
        "prompt": "def close(self):\n        \"\"\"\n        Close the stream. Assumes stream has 'close' method.\n        \"\"\"",
        "code": "def close(self):\n        \"\"\"\n        Close the stream. Assumes stream has 'close' method.\n        \"\"\"\n        self.out_stream.close()\n        # If we're asked to write in place, substitute the named\n        # temporary file for the current file\n        if self.in_place:\n            shutil.move(self.temp_file.name, self.out)",
        "writer": "human",
        "rewrite": [
            "def close(self):\n    self.out_stream.close()\n    if self.in_place:\n        os.replace(self.temp_file.name, self.out)",
            "def close(self):\n    try:\n        self.out_stream.close()\n    except Exception as e:\n        print(f\"Error closing stream: {e}\")\n    if self.in_place:\n        try:\n            os.replace(self.temp_file.name, self.out)\n        except Exception as e:\n            print(f\"Error replacing file: {e}\")",
            "def close(self):\n    self.out_stream.close()\n    if self.in_place:\n        try:\n            os.replace(self.temp_file.name, self.out)\n        except OSError:\n            shutil.move(self.temp_file.name, self.out)"
        ]
    },
    {
        "id": 64,
        "prompt": "def delete(self):\n        \"\"\"Remove myself from my :class:`Character`.\n\n        For symmetry with :class:`Thing` and :class`Place`.\n\n        \"\"\"",
        "code": "def delete(self):\n        \"\"\"Remove myself from my :class:`Character`.\n\n        For symmetry with :class:`Thing` and :class:`Place`.\n\n        \"\"\"\n        if self.location:\n            self.location.contents.remove(self)\n        self.location = None",
        "writer": "machine",
        "rewrite": [
            "def delete(self):\n    if self.location:\n        self.location.contents.remove(self)\n    self.location = None",
            "def delete(self):\n    try:\n        self.location.contents.remove(self)\n    except AttributeError:\n        pass\n    self.location = None",
            "def delete(self):\n    if hasattr(self, 'location') and self.location:\n        self.location.contents.remove(self)\n    self.location = None"
        ]
    },
    {
        "id": 61,
        "prompt": "def next(self):\n        \"\"\" allow us to iterate over the output of our command \"\"\"",
        "code": "def next(self):\n        \"\"\" allow us to iterate over the output of our command \"\"\"\n\n        if self._stopped_iteration:\n            raise StopIteration()\n\n        # we do this because if get blocks, we can't catch a KeyboardInterrupt\n        # so the slight timeout allows for that.\n        while True:\n            try:\n                chunk = self.process._pipe_queue.get(True, 0.001)\n            except Empty:\n                if self.call_args[\"iter_noblock\"]:\n                    return errno.EWOULDBLOCK\n            else:\n                if chunk is None:\n                    self.wait()\n                    self._stopped_iteration = True\n                    raise StopIteration()\n                try:\n                    return chunk.decode(self.call_args[\"encoding\"],\n                        self.call_args[\"decode_errors\"])\n                except UnicodeDecodeError:\n                    return chunk",
        "writer": "human",
        "rewrite": [
            "def next(self):\n    if self._stopped_iteration:\n        raise StopIteration()\n    while True:\n        try:\n            chunk = self.process._pipe_queue.get(True, 0.001)\n            if chunk is None:\n                self.wait()\n                self._stopped_iteration = True\n                raise StopIteration()\n            try:\n                return chunk.decode(self.call_args[\"encoding\"], self.call_args[\"decode_errors\"])\n            except UnicodeDecodeError:\n                return chunk\n        except queue.Empty:\n            if self.call_args[\"iter_noblock\"]:\n                return errno.EWOULDBLOCK",
            "def next(self):\n    if self._stopped_iteration: raise StopIteration()\n    while True:\n        try:\n            chunk = self.process._pipe_queue.get(timeout=0.001)\n            if chunk is None:\n                self.wait()\n                self._stopped_iteration = True\n                raise StopIteration()\n            try: return chunk.decode(self.call_args['encoding'], self.call_args['decode_errors'])\n            except UnicodeDecodeError: return chunk\n        except queue.Empty:\n            if self.call_args['iter_noblock']: return errno.EWOULDBLOCK",
            "def next(self):\n    if self._stopped_iteration:\n        raise StopIteration\n    while True:\n        try:\n            chunk = self.process._pipe_queue.get(True, 0.001)\n            if chunk is None:\n                self._stopped_iteration = True\n                self.wait()\n                raise StopIteration\n            try:\n                return chunk.decode(self.call_args[\"encoding\"], self.call_args[\"decode_errors\"])\n            except UnicodeDecodeError:\n                return chunk\n        except queue.Empty:\n            if self.call_args[\"iter_noblock\"]:\n                return errno.EWOULDBLOCK"
        ]
    },
    {
        "id": 65,
        "prompt": "def list_private_repos(profile='github'):\n    \"\"\"\n    List private repositories within the organization. Dependent upon the access\n    rights of the profile token.\n\n    .. versionadded:: 2016.11.0\n\n    profile\n        The name of the profile configuration to use. Defaults to ``github``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion github.list_private_repos\n        salt myminion github.list_private_repos profile='my-github-profile'\n    \"\"\"",
        "code": "def list_private_repos(profile='github'):\n    \"\"\"\n    List private repositories within the organization. Dependent upon the access\n    rights of the profile token.\n\n    .. versionadded:: 2016.11.0\n\n    profile\n        The name of the profile configuration to use. Defaults to ``github``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion github.list_private_repos\n        salt myminion github.list_private_repos profile='my-github-profile'\n    \"\"\"\n    repos = []\n    for repo in _get_repos(profile):\n        if repo.private is True:\n            repos.append(repo.name)\n    return repos",
        "writer": "human",
        "rewrite": [
            "def list_private_repos(profile='github'):\n    return [repo.name for repo in _get_repos(profile) if repo.private]",
            "def list_private_repos(profile='github'):\n    return [repo.name for repo in _get_repos(profile) if repo.private]",
            "def list_private_repos(profile='github'):\n    return [repo.name for repo in _get_repos(profile) if repo.private == True]"
        ]
    },
    {
        "id": 59,
        "prompt": "def _validate_condition_keys(self, field, value, error):\n        \"\"\"\n        Validates that all of the keys in one of the sets of keys are defined\n        as keys of ``value``.\n        \"\"\"",
        "code": "def _validate_condition_keys(self, field, value, error):\n        \"\"\"\n        Validates that all of the keys in one of the sets of keys are defined\n        as keys of ``value``.\n        \"\"\"\n        if 'field' in value:\n            operators = self.nonscalar_conditions + self.scalar_conditions\n            matches = sum(1 for k in operators if k in value)\n            if matches == 0:\n                error(field, 'Must contain one of {}'.format(operators))\n                return False\n            elif matches > 1:\n                error(\n                    field,\n                    'Must contain no more than one of {}'.format(operators)\n                )\n                return False\n            return True\n        elif 'and' in value:\n            for condition in value['and']:\n                self._validate_condition_keys(field, condition, error)\n        elif 'or' in value:\n            for condition in value['or']:\n                self._validate_condition_keys(field, condition, error)\n        else:\n            error(field, \"Must contain field + operator keys, 'and', or 'or'.\")\n            return False",
        "writer": "human",
        "rewrite": [
            "def _validate_condition_keys(self, field, value, error):\n    if 'field' in value:\n        operators = self.nonscalar_conditions + self.scalar_conditions\n        matches = sum(1 for k in operators if k in value)\n        if not matches:\n            error(field, f'Must contain one of {operators}')\n            return False\n        if matches > 1:\n            error(field, f'Must contain no more than one of {operators}')\n            return False\n        return True\n    if 'and' in value:\n        return all(self._validate_condition_keys(field, cond, error) for cond in value['and'])\n    if 'or' in value:\n        return any(self._validate_condition_keys(field, cond, error) for cond in value['or'])\n    error(field, \"Must contain field + operator keys, 'and', or 'or'.\")\n    return False",
            "def _validate_condition_keys(self, field, value, error):\n    if 'field' in value:\n        ops = self.nonscalar_conditions + self.scalar_conditions\n        count = sum(1 for op in ops if op in value)\n        if not count:\n            error(field, f\"Must contain one of {ops}\")\n            return False\n        if count > 1:\n            error(field, f\"Must contain no more than one of {ops}\")\n            return False\n        return True\n    if 'and' in value:\n        return all(self._validate_condition_keys(field, c, error) for c in value['and'])\n    if 'or' in value:\n        return any(self._validate_condition_keys(field, c, error) for c in value['or'])\n    error(field, \"Must contain field + operator keys, 'and', or 'or'.\")\n    return False",
            "def _validate_condition_keys(self, field, value, error):\n    if \"field\" in value:\n        operators = self.nonscalar_conditions + self.scalar_conditions\n        if not any(op in value for op in operators):\n            error(field, f\"Must contain one of {operators}\")\n            return False\n        if sum(1 for op in operators if op in value) > 1:\n            error(field, f\"Must contain no more than one of {operators}\")\n            return False\n        return True\n    if \"and\" in value:\n        return all(self._validate_condition_keys(field, cond, error) for cond in value[\"and\"])\n    if \"or\" in value:\n        return any(self._validate_condition_keys(field, cond, error) for cond in value[\"or\"])\n    error(field, \"Must contain field + operator keys, 'and', or 'or'.\")\n    return False"
        ]
    },
    {
        "id": 67,
        "prompt": "def ls_dir(path, include_hidden=False):\n    \"\"\"Finds content of folder\n\n    :param path: directory to get list of files and folders\n    :param include_hidden: True iff include hidden files in list\n    :return: List of paths in given directory\n    \"\"\"",
        "code": "def ls_dir(path, include_hidden=False):\n    \"\"\"Finds content of folder\n\n    :param path: directory to get list of files and folders\n    :param include_hidden: True iff include hidden files in list\n    :return: List of paths in given directory\n    \"\"\"\n    lst = []\n    for file in os.listdir(path):\n        hidden_file = FileSystem(file).is_hidden()\n        if (hidden_file and include_hidden) or (not hidden_file):\n            lst.append(os.path.join(path, file))\n    return list(set(lst))",
        "writer": "human",
        "rewrite": [
            "import os\n\ndef ls_dir(path, include_hidden=False):\n    return [os.path.join(path, file) for file in os.listdir(path) if not file.startswith('.') or include_hidden]",
            "import os\n\ndef ls_dir(path, include_hidden=False):\n    files = os.listdir(path)\n    paths = [os.path.join(path, file) for file in files]\n    if not include_hidden:\n        paths = [p for p in paths if not os.path.basename(p).startswith('.')]\n    return list(set(paths))",
            "import os\n\ndef ls_dir(path, include_hidden=False):\n    return list({os.path.join(path, f) for f in os.listdir(path) if not f.startswith('.') or include_hidden})"
        ]
    },
    {
        "id": 67,
        "prompt": "def ls_dir(path, include_hidden=False):\n    \"\"\"Finds content of folder\n\n    :param path: directory to get list of files and folders\n    :param include_hidden: True iff include hidden files in list\n    :return: List of paths in given directory\n    \"\"\"",
        "code": "def ls_dir(path, include_hidden=False):\n    \"\"\"Finds content of folder\n\n    :param path: directory to get list of files and folders\n    :param include_hidden: True iff include hidden files in list\n    :return: List of paths in given directory\n    \"\"\"\n    import os\n    result = []\n    for item in os.listdir(path):\n        if include_hidden or not item.startswith(\".\"):\n            result.append(os.path.join(path, item))\n    return result",
        "writer": "machine",
        "rewrite": [
            "import os\ndef ls_dir(path, include_hidden=False):\n    return [os.path.join(path, item) for item in os.listdir(path) if include_hidden or not item.startswith('.')]",
            "import os\ndef ls_dir(path, include_hidden=False):\n    return [os.path.join(path, f) for f in os.listdir(path) if include_hidden or not f.startswith('.')]",
            "import os\ndef ls_dir(p,h=False):\n    return [os.path.join(p,i) for i in os.listdir(p) if h or not i.startswith('.')]"
        ]
    },
    {
        "id": 64,
        "prompt": "def delete(self):\n        \"\"\"Remove myself from my :class:`Character`.\n\n        For symmetry with :class:`Thing` and :class`Place`.\n\n        \"\"\"",
        "code": "def delete(self):\n        \"\"\"Remove myself from my :class:`Character`.\n\n        For symmetry with :class:`Thing` and :class`Place`.\n\n        \"\"\"\n        branch, turn, tick = self.engine._nbtt()\n        self.engine._edges_cache.store(\n            self.character.name,\n            self.origin.name,\n            self.destination.name,\n            0,\n            branch,\n            turn,\n            tick,\n            None\n        )\n        self.engine.query.exist_edge(\n            self.character.name,\n            self.origin.name,\n            self.destination.name,\n            branch, turn, tick, False\n        )\n        try:\n            del self.engine._edge_objs[\n                (self.graph.name, self.orig, self.dest)\n            ]\n        except KeyError:\n            pass\n        self.character.portal[self.origin.name].send(\n            self.character.portal[self.origin.name],\n            key='dest', val=None\n        )",
        "writer": "human",
        "rewrite": [
            "def delete(self):\n    branch, turn, tick = self.engine._nbtt()\n    self.engine._edges_cache.store(self.character.name, self.origin.name, self.destination.name, 0, branch, turn, tick, None)\n    self.engine.query.exist_edge(self.character.name, self.origin.name, self.destination.name, branch, turn, tick, False)\n    self.character.portal[self.origin.name].send(self.character.portal[self.origin.name], key='dest', val=None)",
            "def delete(self):\n    b, t, ti = self.engine._nbtt()\n    self.engine._edges_cache.store(self.character.name, self.origin.name, self.destination.name, 0, b, t, ti, None)\n    self.engine.query.exist_edge(self.character.name, self.origin.name, self.destination.name, b, t, ti, False)\n    try: del self.engine._edge_objs[(self.graph.name, self.orig, self.dest)]\n    except KeyError: pass\n    self.character.portal[self.origin.name].send(self.character.portal[self.origin.name], key='dest', val=None)",
            "def delete(self):\n    b, t, tk = self.engine._nbtt()\n    self.engine._edges_cache.store(self.character.name, self.origin.name, self.destination.name, 0, b, t, tk, None)\n    self.engine.query.exist_edge(self.character.name, self.origin.name, self.destination.name, b, t, tk, False)\n    try: del self.engine._edge_objs[(self.graph.name, self.orig, self.dest)]\n    except KeyError: pass\n    self.character.portal[self.origin.name].send(self.character.portal[self.origin.name], key='dest', val=None)"
        ]
    },
    {
        "id": 69,
        "prompt": "def reload(self, schedule):\n        \"\"\"\n        Reload the schedule from saved schedule file.\n        \"\"\"",
        "code": "def reload(self, schedule):\n        \"\"\"\n        Reload the schedule from saved schedule file.\n        \"\"\"\n        # Remove all jobs from self.intervals\n        self.intervals = {}\n\n        if 'schedule' in schedule:\n            schedule = schedule['schedule']\n        self.opts.setdefault('schedule', {}).update(schedule)",
        "writer": "human",
        "rewrite": [
            "def reload(self, schedule):\n    self.intervals = {}\n    self.opts['schedule'] = schedule.get('schedule', {})",
            "def reload(self, schedule):\n    self.intervals = {}\n    try:\n        self.opts['schedule'].update(schedule['schedule'])\n    except KeyError:\n        self.opts['schedule'] = schedule",
            "def reload(self, schedule):\n    self.intervals = {}\n    self.opts['schedule'] = schedule.get('schedule', {})"
        ]
    },
    {
        "id": 65,
        "prompt": "def list_private_repos(profile='github'):\n    \"\"\"\n    List private repositories within the organization. Dependent upon the access\n    rights of the profile token.\n\n    .. versionadded:: 2016.11.0\n\n    profile\n        The name of the profile configuration to use. Defaults to ``github``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion github.list_private_repos\n        salt myminion github.list_private_repos profile='my-github-profile'\n    \"\"\"",
        "code": "def list_private_repos(profile='github'):\n    '''\n    List private repositories within the organization. Dependent upon the access\n    rights of the profile token.\n\n    .. versionadded:: 2016.11.0\n\n    profile\n        The name of the profile configuration to use. Defaults to ``github``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion github.list_private_repos\n        salt myminion github.list_private_repos profile='my-github-profile'\n    '''\n    ret = {}\n    try:\n        github = __salt__['github.get_conn'](profile)\n        repos = github.get_user().get_repos(type='private')\n        if repos:\n            ret['result'] = True\n            ret['repos'] = [repo.name for repo in repos]\n        else:\n            ret['result'] = False\n            ret['comment'] = 'No private repositories found.'\n    except Exception as e:\n        ret['result'] = False\n        ret['comment'] = str(e)\n    return ret",
        "writer": "machine",
        "rewrite": [
            "def list_private_repos(profile='github'):\n    ret = {}\n    try:\n        github = __salt__['github.get_conn'](profile)\n        repos = github.get_user().get_repos(type='private')\n        ret['result'] = bool(repos)\n        ret['repos'] = [repo.name for repo in repos] if repos else []\n        ret['comment'] = 'No private repositories found.' if not repos else ''\n\n    except Exception as e:\n        ret['result'] = False\n        ret['comment'] = str(e)\n    return ret",
            "def list_private_repos(profile='github'):\n    ret = {'result': False, 'repos': [], 'comment': ''}\n    try:\n        github = __salt__['github.get_conn'](profile)\n        repos = github.get_user().get_repos(type='private')\n        ret['repos'] = [repo.name for repo in repos]\n        ret['result'] = bool(ret['repos'])\n        ret['comment'] = 'No private repositories found.' if not ret['result'] else ''\n    except Exception as e:\n        ret['comment'] = str(e)\n    return ret",
            "def list_private_repos(profile='github'):\n    ret = {'result': False, 'comment': 'No private repositories found.', 'repos': []}\n    try:\n        conn = __salt__['github.get_conn'](profile)\n        repos = conn.get_user().get_repos(type='private')\n        if repos:\n            ret['result'] = True\n            ret['repos'] = [r.name for r in repos]\n            ret['comment'] = ''\n    except Exception as e:\n        ret['comment'] = str(e)\n    return ret"
        ]
    },
    {
        "id": 69,
        "prompt": "def reload(self, schedule):\n        \"\"\"\n        Reload the schedule from saved schedule file.\n        \"\"\"",
        "code": "def reload(self, schedule):\n        \"\"\"\n        Reload the schedule from saved schedule file.\n        \"\"\"\n        with open(self.schedule_file, 'r') as f:\n            self.schedule = json.load(f)",
        "writer": "machine",
        "rewrite": [
            "def reload(self, schedule):\n    with open(self.schedule_file, 'r') as f:\n        self.schedule = json.load(f)",
            "def reload(self):\n    try:\n        with open(self.schedule_file, 'r') as f:\n            self.schedule = json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError) as e:\n        self.schedule = {} # Or handle the error appropriately",
            "def reload(self):\n    try:\n        with open(self.schedule_file, 'r') as f:\n            self.schedule = json.load(f)\n    except Exception as e:\n        print(f\"Error reloading schedule: {e}\")"
        ]
    },
    {
        "id": 68,
        "prompt": "def get_target_url(endpoint_url, bucket_name=None, object_name=None,\n                   bucket_region='us-east-1', query=None):\n    \"\"\"\n    Construct final target url.\n\n    :param endpoint_url: Target endpoint url where request is served to.\n    :param bucket_name: Bucket component for the target url.\n    :param object_name: Object component for the target url.\n    :param bucket_region: Bucket region for the target url.\n    :param query: Query parameters as a *dict* for the target url.\n    :return: Returns final target url as *str*.\n    \"\"\"",
        "code": "def get_target_url(endpoint_url, bucket_name=None, object_name=None,\n                   bucket_region='us-east-1', query=None):\n    \"\"\"\n    Construct final target url.\n\n    :param endpoint_url: Target endpoint url where request is served to.\n    :param bucket_name: Bucket component for the target url.\n    :param object_name: Object component for the target url.\n    :param bucket_region: Bucket region for the target url.\n    :param query: Query parameters as a *dict* for the target url.\n    :return: Returns final target url as *str*.\n    \"\"\"\n    target_url = endpoint_url\n    if bucket_name:\n        target_url = f'{target_url}/{bucket_name}'\n    if object_name:\n        target_url = f'{target_url}/{object_name}'\n    if query:\n        query_string = '&'.join([f'{k}={v}' for k, v in query.items()])\n        target_url = f'{target_url}?{query_string}'\n    return target_",
        "writer": "machine",
        "rewrite": [
            "def get_target_url(endpoint_url, bucket_name=None, object_name=None, bucket_region='us-east-1', query=None):\n    target_url = endpoint_url\n    if bucket_name: target_url += f\"/{bucket_name}\"\n    if object_name: target_url += f\"/{object_name}\"\n    if query: target_url += f\"?{'&'.join([f'{k}={v}' for k,v in query.items()])}\"\n    return target_url",
            "def get_target_url(endpoint_url, bucket_name=None, object_name=None, bucket_region='us-east-1', query=None):\n    parts = [endpoint_url]\n    if bucket_name: parts.append(bucket_name)\n    if object_name: parts.append(object_name)\n    url = \"/\".join(parts)\n    if query:\n        url += \"?\" + \"&\".join(f\"{k}={v}\" for k, v in query.items())\n    return url",
            "from urllib.parse import urlencode\n\ndef get_target_url(endpoint_url, bucket_name=None, object_name=None, bucket_region='us-east-1', query=None):\n    path = '/'.join(part for part in [bucket_name, object_name] if part)\n    if path:\n        endpoint_url += '/' + path\n    if query:\n        endpoint_url += '?' + urlencode(query)\n    return endpoint_url"
        ]
    },
    {
        "id": 68,
        "prompt": "def get_target_url(endpoint_url, bucket_name=None, object_name=None,\n                   bucket_region='us-east-1', query=None):\n    \"\"\"\n    Construct final target url.\n\n    :param endpoint_url: Target endpoint url where request is served to.\n    :param bucket_name: Bucket component for the target url.\n    :param object_name: Object component for the target url.\n    :param bucket_region: Bucket region for the target url.\n    :param query: Query parameters as a *dict* for the target url.\n    :return: Returns final target url as *str*.\n    \"\"\"",
        "code": "def get_target_url(endpoint_url, bucket_name=None, object_name=None,\n                   bucket_region='us-east-1', query=None):\n    \"\"\"\n    Construct final target url.\n\n    :param endpoint_url: Target endpoint url where request is served to.\n    :param bucket_name: Bucket component for the target url.\n    :param object_name: Object component for the target url.\n    :param bucket_region: Bucket region for the target url.\n    :param query: Query parameters as a *dict* for the target url.\n    :return: Returns final target url as *str*.\n    \"\"\"\n    # New url\n    url = None\n\n    # Parse url\n    parsed_url = urlsplit(endpoint_url)\n\n    # Get new host, scheme.\n    scheme = parsed_url.scheme\n    host = parsed_url.netloc\n\n    # Strip 80/443 ports since curl & browsers do not\n    # send them in Host header.\n    if (scheme == 'http' and parsed_url.port == 80) or\\\n       (scheme == 'https' and parsed_url.port == 443):\n        host = parsed_url.hostname\n\n    if 's3.amazonaws.com' in host:\n        host = get_s3_endpoint(bucket_region)\n\n    url = scheme + '://' + host\n    if bucket_name:\n        # Save if target url will have buckets which suppport\n        # virtual host.\n        is_virtual_host_style = is_virtual_host(endpoint_url,\n                                                bucket_name)\n        if is_virtual_host_style:\n            url = (scheme + '://' + bucket_name + '.' + host)\n        else:\n            url = (scheme + '://' + host + '/' + bucket_name)\n\n    url_components = [url]\n    url_components.append('/')\n\n    if object_name:\n        object_name = encode_object_name(object_name)\n        url_components.append(object_name)\n\n    if query:\n        ordered_query = collections.OrderedDict(sorted(query.items()))\n        query_components = []\n        for component_key in ordered_query:\n            if isinstance(ordered_query[component_key], list):\n                for value in ordered_query[component_key]:\n                    query_components.append(component_key+'='+\n                                            queryencode(value))\n            else:\n                query_components.append(\n                    component_key+'='+\n                    queryencode(ordered_query.get(component_key, '')))\n\n        query_string = '&'.join(query_components)\n        if query_string:\n            url_components.append('?')\n            url_components.append(query_string)\n\n    return ''.join(url_components)",
        "writer": "human",
        "rewrite": [
            "from urllib.parse import urlsplit\nimport collections\nfrom urllib.parse import urlencode\n\ndef get_s3_endpoint(bucket_region):\n    return f's3.{bucket_region}.amazonaws.com'\n\ndef is_virtual_host(endpoint_url, bucket_name):\n    return 'amazonaws.com' in endpoint_url\n\ndef encode_object_name(object_name):\n    return object_name\n\ndef get_target_url(endpoint_url, bucket_name=None, object_name=None, bucket_region='us-east-1', query=None):\n    parsed_url = urlsplit(endpoint_url)\n    scheme = parsed_url.scheme\n    host = parsed_url.netloc\n    if (scheme == 'http' and parsed_url.port == 80) or (scheme == 'https' and parsed_url.port == 443):\n        host = parsed_url.hostname\n    if 's3.amazonaws.com' in host:\n        host = get_s3_endpoint(bucket_region)\n    url = f'{scheme}://{host}'\n    if bucket_name:\n        is_virtual_host_style = is_virtual_host(endpoint_url, bucket_name)\n        url = f'{scheme}://{bucket_name}.{host}' if is_virtual_host_style else f'{scheme}://{host}/{bucket_name}'\n    url_components = [url, '/']\n    if object_name:\n        url_components.append(encode_object_name(object_name))\n    if query:\n        ordered_query = collections.OrderedDict(sorted(query.items()))\n        query_components = []\n        for k, v in ordered_query.items():\n            if isinstance(v, list):\n                for val in v:\n                    query_components.append(f'{k}={urlencode({k:val})}')\n            else:\n                query_components.append(f'{k}={urlencode({k:v})}')\n        query_string = '&'.join(query_components)\n        url_components.extend(['?', query_string])\n    return ''.join(url_components)",
            "from urllib.parse import urlsplit, urlencode\nimport collections\n\ndef get_s3_endpoint(region):\n    return f\"s3.{region}.amazonaws.com\"\n\ndef is_virtual_host(endpoint, bucket):\n    return \"amazonaws.com\" in endpoint\n\ndef encode_object_name(name):\n    return name\n\ndef get_target_url(endpoint_url, bucket_name=None, object_name=None, bucket_region=\"us-east-1\", query=None):\n    parsed = urlsplit(endpoint_url)\n    scheme = parsed.scheme\n    host = parsed.netloc\n    if (scheme == \"http\" and parsed.port == 80) or (scheme == \"https\" and parsed.port == 443):\n        host = parsed.hostname\n    if \"s3.amazonaws.com\" in host:\n        host = get_s3_endpoint(bucket_region)\n    url = f\"{scheme}://{host}\"\n    if bucket_name:\n        url = f\"{scheme}://{bucket_name}.{host}\" if is_virtual_host(endpoint_url, bucket_name) else f\"{scheme}://{host}/{bucket_name}\"\n    url += \"/\"\n    if object_name:\n        url += encode_object_name(object_name)\n    if query:\n        q = collections.OrderedDict(sorted(query.items()))\n        params = []\n        for k, v in q.items():\n            if isinstance(v, list):\n                for val in v:\n                    params.append(f\"{k}={urlencode({k: val})}\")\n            else:\n                params.append(f\"{k}={urlencode({k: v})}\")\n        url += \"?\" + \"&\".join(params)\n    return url",
            "from urllib.parse import urlsplit, urlencode\nimport collections\n\ndef get_s3_endpoint(region):\n    return f\"s3.{region}.amazonaws.com\"\n\ndef is_virtual_host(endpoint, bucket):\n    return \"amazonaws.com\" in endpoint\n\ndef encode_object_name(name):\n  return name\n\ndef get_target_url(endpoint_url, bucket_name=None, object_name=None, bucket_region='us-east-1', query=None):\n    parsed = urlsplit(endpoint_url)\n    scheme = parsed.scheme\n    host = parsed.netloc\n    if (scheme == 'http' and parsed.port == 80) or (scheme == 'https' and parsed.port == 443):\n        host = parsed.hostname\n    if 's3.amazonaws.com' in host:\n        host = get_s3_endpoint(bucket_region)\n    url = f\"{scheme}://{host}\"\n    if bucket_name:\n        url = f\"{scheme}://{bucket_name}.{host}\" if is_virtual_host(endpoint_url, bucket_name) else f\"{scheme}://{host}/{bucket_name}\"\n    url += \"/\"\n    if object_name:\n        url += encode_object_name(object_name)\n    if query:\n        query_string = \"&\".join([f\"{k}={urlencode({k: v})}\" if not isinstance(v, list) else \"&\".join([f\"{k}={urlencode({k: val})}\" for val in v]) for k, v in sorted(query.items())])\n        url += f\"?{query_string}\"\n    return url"
        ]
    }
]